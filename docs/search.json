[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Brian Carey",
    "section": "",
    "text": "I have a background ranging from international finance and administration, programming, software translation and localization management. I’ve been a medical doctor, business owner and professor. As I look back, the common thread in all of these has been innovation. In each of my jobs, I was able to innovate the role, often introducing new technologies, frameworks and processes to stream-line current practices and address new challenges.\nAll of the posts of my experiences can be found here.\n\nFinance and AdministrationTechnologyMedical Practice and TeachingOther Interests\n\n\nI was senior consolidations accountant at the San Rafael headquarters of Autodesk, before moving to their European headquarters in Switzerland where I was senior financial analyst for the operations and software centers, and subsequently world-wide manager of vendors and translation services. You can read more here\n\n\nMy first programming job was using Clipper, an MS-DOS version of dBase, to write a scheduling module for a private education company. I switched to Linux three decades ago, and wrote my first “web app” in Perl/CGI using PostgreSQL, back when JavaScript was just getting started. I am now very proficient in Python and R, with intermediate skills in Haskell and JavaScript, as well as attendant technologies such as HTML and CSS.\nThe best way to get an idea of my programming and data science skills are to look at some of the projects on the site. You can also find more here\n\n\nI practiced for over 15 years as a Doctor of Oriental Medicine, much of the time teaching at Southwest Acupuncture College. I was most known for teaching the Chinese materia medica, a three-semester sequence covering well over a 1,000 individual medicinal substances. I re-arranged herbs from their traditional order to one more didactically-oriented, providing structure for the students, and developed rich slide presentations with \\(LaTeX\\). Other areas I taught included diagnosis and differentiation as well as basic Chinese medical theory.\nI ran an independent clinic, where I combined TCM style acupuncture, Kiiko style acupuncture, and Daoist style of Jeffrey Yuen. A large part of my practice, as is typical in American acupuncture clinics, involved acute and chronic pain syndromes, but a significant part was also devoted to digestive, emotional, gynecological and sleep disorders. Treatments generally involved a combination of acupuncture, herbal therapy and food therapy.\n\n\nI’m interested in history, languages and linguistics, gardening, cooking and hiking. Look for articles on some of these topics in the near future.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "biscotty’s Workshop",
    "section": "",
    "text": "For the love of learning"
  },
  {
    "objectID": "index.html#whats-new",
    "href": "index.html#whats-new",
    "title": "biscotty’s Workshop",
    "section": "What’s new",
    "text": "What’s new\nAn article about the medicinal properties of shiso and its use in Chinese Medicine.\nAn analysis of the Hispanic population of New Mexico using R and TidyCensus.\nEating in spring and The Magical Mulberry are about Chinese medicine.\nA new article about the drawbacks of Docker and superiority of Nix for software development and management.\nA new post comparing the use of R with Python for working with GPX data."
  },
  {
    "objectID": "index.html#whats-here",
    "href": "index.html#whats-here",
    "title": "biscotty’s Workshop",
    "section": "What’s here",
    "text": "What’s here\nIf this is your first time, you can see all the posts here, or use the search icon in the upper left. Or just start below.\n\nObsidianData ScienceLinuxChinese Medicine\n\n\nObsidian is Personal Knowledge Management PKM software. If you don’t know what Obsidian or PKM are, you could start with this article. I have many articles here about advanced usage, taking full advantage of Obsidian’s design. Freeing your thinking explains how to move beyond directory structures, taking advantage of the no-sql nature of Obsidian. There is also a series about the usage of dataviewjs, written for non-programmers, to create dashboards and interactive tables.\n\n\nMy current passion is data science, with particular interest in geospatial analyses (GIS). I enjoy working both in both R and Python. Some of articles will explicitly compare the two, as I think that data science Python programmers would find R delightful, such as showing how to do gps trail mappingin R instead of Python.\n\n\nI’ve used Linux almost exclusively for over three decades. The first 20 years or so I used Slackware, the oldest distribution still being developed. Since then I have extensively used Fedora, both mutable and immutable versions, as well as Debian and Ubuntu.\nMy current OS of choice is NixOS, which is unlike any other operating system I’m aware of, and very different conceptually. To get an idea of what I mean, read The Linux Different.\n\n\nI am a retired Doctor of Oriental Medicine, which I practiced for nearly two decades as well as teaching at Southwest Acupuncture College where I specialized in Chinese herbal medicine, medical theory, and diagnosis. I see Chinese herbs all around me when I walk, not to mention those growing in my own yard. Some of these articles will describe the health properties, from a Chinese perspective, of everyday plants.\nI also love to cook, and tried to stress food therapy to my patients as a key aspect of healing. Keeping a proper diet is the most important thing that anyone can do for their health. So I will occasionally post recipes, pointing out the health aspects of the meals."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "biscotty’s Workshop",
    "section": "About me",
    "text": "About me\nI have a background ranging from international finance and administration, programming, software translation and localization management. I’ve been a medical doctor, business owner and professor. You can find out more about me, my current work, and services here"
  },
  {
    "objectID": "index.html#old-news",
    "href": "index.html#old-news",
    "title": "biscotty’s Workshop",
    "section": "Old News",
    "text": "Old News\nI have added, and am updating, a section about myself, for those who are interested in my background and experience.\nFor those returning, this third iteration of my website is built in Quatro. I hope you like the new layout. It now allows you to comment on posts, so I look forward to your feedback and ideas. You can still email me as before, of course. Thanks to Samantha Shanny-Csik (Sam Shanny-Csik 2022; Samantha Shanny-Csik 2022) and Albert Rapp (Rapp 2022).\nI just posted a new (old really) article about NixOS. NixOS is not generally understood in the Linux community, but it solves many problems inherent in the traditional Linux system structure. It is my first stab at explaining how NixOS does what it does, and why."
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html",
    "href": "posts/chinese-medicine/magical-mulberry/index.html",
    "title": "The Magical Mulberry",
    "section": "",
    "text": "Branch, fruit, leaves of the mulberry1\nThe much maligned mulberry, banned in many places such as the USA and Brazil for its highly invasive nature, is unique in two ways: 1) it provided, directly or indirectly, a greater variety of medicinal substances than any other plant, and; 2) it changed the course of human history. This article is primarily about the former, but I’ll look at the latter later.\nWhen I taught Chinese Materia Medica, a course in which students learn all of the properties of over 1,000 medicinal substances, one of the challenges was to make the subject tangible beyond the pages of the texts. At this stage in their studies, clinical experience was focused on acupuncture, and their only exposure to herbs was through books and lectures. Many plants commonly used in Chinese Medicine (CM) grow all around us, sometimes in our own back yard. In order to help them “get in touch” with the topic, my students had a regular assignment to identify and report on such herbs which they identified around town. Making connections with plants they see every day helped them to memorize their properties, functions and indications.\nA perennial favorite was the mulberry tree, and for good reason. The mulberry tree originated in China, but spread throughout the globe, and is now considered an invasive species in many areas. It is a tree of tremendous historical as well as medical importance, being the host of the silkworm. Silk production in China was so important that the primary inter-continental trade route for 1,500 years, up until the mid-15th century was named for it: the Silk Road. More on that later.\nThis silkworm itself provides two medicinal substances, but the mulberry tree provides no less than four, and another parasite of the tree, mistletoe, provides yet another. It is not unusual for CM to use different parts of a plant for different purposes, but such a range is unique. Depending on the part of the tree used, it may work on acute conditions like colds and flu, or chronic conditions like atrophy. It may work on the most superficial levels of the body, or the deepest levels. Some are used to address psychological/emotional issues, others explicitly physical. Some are nourishing, others are “dispersing”2.\nIn this article, I will explore the variety of uses of the different parts of the tree. This does not constitute medical advice, and use of any herb should be approached with caution. Even those which are not toxic (none of these are) can produce unwanted side effects and be harmful to your health. Doctors of Chinese Medicine spend years of study before prescribing, and for good reason. As regards the fruit…eat up!",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html#the-tree",
    "href": "posts/chinese-medicine/magical-mulberry/index.html#the-tree",
    "title": "The Magical Mulberry",
    "section": "The tree",
    "text": "The tree\n\n\n\n\n\n\nsāng shù 桑樹 Mori alba\n\n\n\n\n\n\n\nMulberry tree3\n\n\n\nI should specify that I am discussing the white mulberry. Despite the wide range of functions and indications attributed to the various parts, we can, never-the-less, identify some commonalities. CM classifies pharmaceuticals in various ways including their nature4, flavor5, part(s) of the body affected, and pathogenic influences6.\nThe mulberry tree is cold in nature. All plants are placed on a spectrum from hot to cold, which is an indication of its ability to either increase or reduce body temperature, either systemically or locally. It primarily affects the Lung System7 and the Liver System8. Most parts are bitter or pungent, flavors associated with clearing and dispersing pathogenic influences, and among these, the mulberry targets wind, dampness and phlegm.",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html#the-leaves",
    "href": "posts/chinese-medicine/magical-mulberry/index.html#the-leaves",
    "title": "The Magical Mulberry",
    "section": "The leaves",
    "text": "The leaves\n\n\n\n\n\n\nsāng yè 桑葉 Mori Folium (Bensky 2004, 54) (Chen, Chen, and Crampton 2004, 74)\n\n\n\n\n\n\n\nMulberry leaves\n\n\n\nMulberry leaves are one of the most commonly used herbs to treat certain types of cough: those associated with warm weather colds and flu, and those with a lot of phlegm in the lungs. In CM, the Lung System controls the skin as well as the lungs themselves, and is responsible for temperature regulation. It is our primary defense against all airborne pathogens, including viruses and bacteria. Sāng yè is one of the main ingredients9 in the classical formula sāng yè tāng, which exemplifies the treatment of colds occurring in warm weather.\nAmong other things, the Liver System is responsible for vision and the eyes, and the leaves are important in treating red, swollen eyes. Interestingly, in both these applications, sāng yè is most often combined with chrysanthemum flowers10. In CM, herbs are (almost) always used in combinations, and certain pairings are recognized as particularly useful. (Herbal combinations11, usually pairings, constitute a separate course in the Chinese curriculum, but is sadly missing in the curriculum in the USA.)",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html#the-fruit",
    "href": "posts/chinese-medicine/magical-mulberry/index.html#the-fruit",
    "title": "The Magical Mulberry",
    "section": "The fruit",
    "text": "The fruit\n\n\n\n\n\n\nsāng shèn 桑葚 Mori Fructus (Bensky 2004, 763) (Chen, Chen, and Crampton 2004, 959)\n\n\n\n\n\n\n\nMulberries12\n\n\n\nMulberries are delicious. If you spot them around your neighborhood, pick a few and taste. If you are ambitious, pick a lot and make jam.\nWhile the leaves affect the most superficial parts of the body, the fruit affects some of the deepest levels. Unlike the other parts, the fruit has no affect on the Lungs, but does strongly affect the Liver. While the other parts of the tree are primarily used to treat pathogens, the fruit is nourishing, building the resources the body needs to heal itself. More specifically, it nourishes the blood. The Chinese concept of blood is more expansive than the western definition, and “insufficiency of blood” is often the root not only of anemia but also of insomnia, depression, and other emotional disorders. Physical manifestations include hypotension, dizziness, and tinnitus. The mulberry fruit is currently being used in China for the treatment of diabetes.",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html#the-twigs",
    "href": "posts/chinese-medicine/magical-mulberry/index.html#the-twigs",
    "title": "The Magical Mulberry",
    "section": "The twigs",
    "text": "The twigs\n\n\n\n\n\n\nsāng zhī 桑枝 Mori Ramulus (Bensky 2004, 355) (Chen, Chen, and Crampton 2004, 327)\n\n\n\n\n\n\n\nsāng zhī 桑枝 Mori Ramulus\n\n\n\nIf the pathogenic forces of wind, dampness and heat make it through the superficial levels of the body, they enter the joints, limbs, and what CM refers to as the “sinews”13. This can lead to symptoms such as swelling, pain, numbness, or even paralysis. Where the mulberry leaves focus on the superficial layers, the twig or branch penetrates deeper as well, expelling the pathogenic forces from those areas. This herb is targeted at the Liver.",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html#the-root-bark",
    "href": "posts/chinese-medicine/magical-mulberry/index.html#the-root-bark",
    "title": "The Magical Mulberry",
    "section": "The root bark",
    "text": "The root bark\n\n\n\n\n\n\nsāng bái pí 桑白皮 Mori Cortex (Bensky 2004, 450) (Chen, Chen, and Crampton 2004, 741)\n\n\n\n\n\n\n\nsāng bái pí 桑白皮 Mori Cortex\n\n\n\nOn the other hand, the root bark is focused on the Lungs, not the Liver. Sāng bái pí is commonly used to treat cough and asthma. Its ability to cool and expel phlegm makes it ideal for cough and asthma characterized by thick, viscous sputum, perhaps yellow.\nThe root bark is also a diuretic, and is used to treat edema, especially that occurring in the face or limbs. It may seem strange that a medicine which targets the Lung would have anything to do with urinary function. There is a saying that “the Lungs are the upper source of water”. While urination is primarily the responsibility of the kidneys and urinary bladder, the Lungs provide downward movement in the body, and obstruction of the Lungs can result in urinary difficulty and consequent edema.",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html#mistletoe",
    "href": "posts/chinese-medicine/magical-mulberry/index.html#mistletoe",
    "title": "The Magical Mulberry",
    "section": "Mistletoe",
    "text": "Mistletoe\n\n\n\n\n\n\nsāng jì shēng 桑寄生 Taxilli Herba(Bensky 2004, 335) (Chen, Chen, and Crampton 2004, 345)\n\n\n\n\n\n\n\nMistletoe on a mulberry tree\n\n\n\nMistletoe is, of course, an entirely different plant from the mulberry: it is a parasite which lives on the tree. Never-the-less, its uses are similarly to the uses of the mulberry itself. Sāng jì shēng combines the nourishing characteristics of the mulberry fruit with the pain and numbness relieving abilities of the root bark. It is used to treat more advanced conditions of limb disorders including pain, stiffness, numbness, and atrophy14.\nSāng jì shēng has another property which will seem odd to westerners. It is said to “calm the fetus”. It is one of about a dozen herbs that are used to treat a range of pregnancy-related issues from morning sickness to bleeding and threatened miscarriage. These herbs are even added to formulas for pregnant women when treating disorders unrelated to the pregnancy. Some, such as the mistletoe, also promote lactation post-partum. Doctors of Chinese Medicine in the USA rarely treat pregnant women due to insurance liability issues, but herbal medicine and acupuncture are very useful in treating many common problems which go along with being pregnant.",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html#the-silkworm",
    "href": "posts/chinese-medicine/magical-mulberry/index.html#the-silkworm",
    "title": "The Magical Mulberry",
    "section": "The silkworm",
    "text": "The silkworm\n\n\n\n\nA healthy, happy silkworm15\n\n\n\nFew plants can claim to have had a greater impact on human history than the mulberry. It is not for its medicinal power, rather for the fact that it is the sole host to the silkworm.\nUntil the Middle Ages, silk was only produced in Asia, mainly in China and India16. Chinese silk in particular was much desired throughout the ancient world, from Japan, whose need was insatiable, to the courts of western Europe where it was prized by the aristocracy and wealthy. It was a major factor in the relationship between China and Japan, and provided a foothold for eventual Western naval and trade dominance in the region.\nWhen the Portuguese arrived in Asia, China and Japan were in a state of conflict and had no trading relationship. Never-the-less, the silk trade was very important to both countries. The Portuguese were able to access the famously closed societies by acting as go-betweens, buying silks in China and selling them in Japan17.\nOf course, many other commodities were traded between Europe and China (and India), spices being of particular import, and the naval routes quickly surpassed in importance the overland Silk Road. The wealth accumulated by Europe as a result of this trade, and their focus on the naval and military technology needed to carry it out, ushered in the long era of western world dominance.\nAnd none of this would have happened if not for the mulberry tree.\nGetting back to medicine, it may come as a surprise, but, despite the name, Chinese Herbal Medicine includes more than just herbs. Beyond the plant kingdom, the Materia Medica includes animals and substances derived from them, fungi, and mineral substances.\n\n\n\n\n\n\nbái jiāng cán 白僵蠶 Bombyx Batryicatus (Bensky 2004, 984) (Chen, Chen, and Crampton 2004, 792)\n\n\n\nThis is an example of a medicinal substance that is both an animal and a fungus. These are silkworms which die after contracting a fungus called “Bombys bassiana*. (In CM terminology, this is considered an invasion of wind pathogen.) Similar to what we saw with the mistletoe, despite being an entirely different species, and even a different kingdom, the silkworm also works on the Lungs and Liver, where it treats relatively severed conditions of spasms, convulsions, and paralysis. (Usually animal substances have stronger effects compared to plants.)\nAs you may recall, the Lungs are responsible for the skin, and while the silkworm does not treat cough, it is commonly used to treat skin itching, rashes, and swelling of the throat, another part of the body belonging to the Lung system. The Lungs also control the voice, and this is used to treat loss of voice.\n\n\n\n\n\n\ncán shā 蠶沙 Bombycis Faeces (Bensky 2004, 334) (Chen, Chen, and Crampton 2004, 312)\n\n\n\n\n\n\n\nSilkworms with feces18\n\n\n\nIntroducing this herb always got a reaction from my students, ranging from nervous giggles to outright disgust, exacerbated when I told them cán shā has a sweet flavor. The Chinese name of this medicinal is quite pleasant, as it translates into English as “silkworm sand”.\nIn keeping with a major function of the medicinals so far discussed, cán shā expels wind and dampness, thereby treating pain and swelling in the limbs and itchy skin rashes. Unlike the other herbs however, which are cold, cán shā is warm. This is an example of how medicinals are chosen based on a range of criteria. Even though this herb and the root bark go to the same parts of the body and focus on the same pathogens, they are not interchangeable, and inappropriate choice due to misdiagnosis can lead to complications.\nCán shā is actually more important clinically to treat dampness in a different part of the body. It affects the abdomen and the digestive system (stomach and pancreas19). Symptoms include severe abdominal pain, vomiting, diarrhea and cramps. When used for this, it is always paired with another herb, Chinese flowering quince mù guā 木瓜 Chaenomeles Fructus.",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html#closing-thoughts",
    "href": "posts/chinese-medicine/magical-mulberry/index.html#closing-thoughts",
    "title": "The Magical Mulberry",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nChinese herbal medicine is a complex and voluminous topic. It has evolved continuously over two thousand years, refining methods and understanding of the properties of thousands of medicinal substances. I hope that I have given you a flavor (no pun intended) of how herbs are used by doctors of oriental medicine, a way very different from that of western herbalists.\nAlways be cautious when taking any herbal medicine. It is a common misconception that taking “nutritional supplements”, as herbs are considered in the USA, can’t really hurt you, but they can. The person stocking the shelves may not be the best resource for good information, so do your homework.\nOn the other hand, sāng shēn, those delicious purple berries, can be enjoyed by the handful, and make for a delicious jam. Look around your neighborhood, or in green spaces around you. If you find mulberries, pick some and enjoy.",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/magical-mulberry/index.html#footnotes",
    "href": "posts/chinese-medicine/magical-mulberry/index.html#footnotes",
    "title": "The Magical Mulberry",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMulberry branch image by Ирина Кудрявцева from Pixabay↩︎\ndispersing refers to eliminating pathogens↩︎\nPhoto by Christopher Eden on Unsplash↩︎\nhot, warm, neutral, cool, cold↩︎\nsour, sweet, salty, bitter, pungent↩︎\nheat, cold, damp, phlegm, wind, etc.↩︎\nThe Lung System includes much of the immune system. It controls the exterior of the body, and is responsible for resisting external pathogens which result in, eg. colds and flu.↩︎\nThe Liver System includes the muscles and joints. It plays an important role in emotional regulation, and poor function can result in irritability, anger, depression and poor sleep.↩︎\nthe other being chrysanthemum flowers↩︎\njú huā 菊花 Chrysanthemi flos↩︎\nduì yào 對藥↩︎\nMulberries image by Анастасия Белоусова from Pixabay↩︎\nThis includes the tendons and ligaments↩︎\nMistletoe also goes to the Kidneys and is commonly used for lumbar pain↩︎\nSilkworm on hand by ivabalk from Pixabay↩︎\nIndia’s traditional Ayurvedic medicine is as old as China’s medical system, and shares a number of features.↩︎\nby means of the Black Ship↩︎\nSilkworms with feces image by Alexandre Lacerda de Almeida from Pixabay↩︎\nThis organ is, unfortunately, called the Spleen in Chinese medicine.↩︎",
    "crumbs": [
      "Chinese Medicine",
      "The Magical Mulberry"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/chinese-medicine.html",
    "href": "posts/chinese-medicine/chinese-medicine.html",
    "title": "Chinese Medicine",
    "section": "",
    "text": "I practiced Chinese Medicine for over 20 years, and taught for over half that time. My specialty at the college was herbal medicine and diagnosis, and in practice I mixed acupuncture, herbal medicine, and food therapy. This is an occasional series on food, herbs, and Chinese medicine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShiso Fine\n\n\nThe Purple Revival\n\n\n\nChinese Medicine\n\nHerbs\n\nFood\n\n\n\nThe famous leaves served with sushi have myriad functions in Traditional Chinese Medicine\n\n\n\n\n\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Magical Mulberry\n\n\nHerbs around us\n\n\n\nChinese Medicine\n\nHerbs\n\nHistory\n\n\n\nThis invasive tree provides many of the more commonly used medicinals in Chinese medicine and shaped world history\n\n\n\n\n\nMay 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEating in spring\n\n\nChinese medicine food therapy\n\n\n\nChinese Medicine\n\nFood\n\n\n\nFoods and recipes appropriate for the spring season\n\n\n\n\n\nMay 15, 2025\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "Chinese Medicine",
      "Chinese Medicine"
    ]
  },
  {
    "objectID": "posts/obsidian/syncing-your-thinking-syncthing/index.html",
    "href": "posts/obsidian/syncing-your-thinking-syncthing/index.html",
    "title": "Syncing your Thinking with Syncthing",
    "section": "",
    "text": "Obsidian loses a lot of power if you can’t take notes from anywhere at anytime on any device you are using. Obsidian offers a subscription syncing service about which I’ve seen no complaints.\nBut there is an awesome free, open source project called Syncthing which is ideal for Obsidian. It’s easy to install on any OS or device, with almost instantaneous syncing, and it’s 100% private.",
    "crumbs": [
      "Obsidian",
      "Syncing your Thinking with Syncthing"
    ]
  },
  {
    "objectID": "posts/obsidian/syncing-your-thinking-syncthing/index.html#connecting-devices",
    "href": "posts/obsidian/syncing-your-thinking-syncthing/index.html#connecting-devices",
    "title": "Syncing your Thinking with Syncthing",
    "section": "Connecting Devices",
    "text": "Connecting Devices\nDevices are connected by exchanging Device IDs, which are long alphanumeric strings. The connection can be initiated from either the computer or the phone. Syncthing will display a QR code containing the Device ID, so the most convenient way is to display the computer’s code, scan it with a phone and initiate the connection from the phone.\nThe Syncthing interface can be accessed using your web browser (localhost:8384). From the browser interface, show the QR code on the computer.\n\nOn the phone, in the devices tab, click on the + to add a device, and then the QR symbol to scan the code. After that, give the device a meaningful name and save the new device. \nBack on your computer you will see an alert asking if you want to add the phone device. Accept this and give the device a name.\n\nAfter accepting the device and after a short period the devices will display connected. This may take a few minutes the first time, but don’t worry…if you scanned the QR code nothing can really go wrong.",
    "crumbs": [
      "Obsidian",
      "Syncing your Thinking with Syncthing"
    ]
  },
  {
    "objectID": "posts/obsidian/syncing-your-thinking-syncthing/index.html#sharing-directories",
    "href": "posts/obsidian/syncing-your-thinking-syncthing/index.html#sharing-directories",
    "title": "Syncing your Thinking with Syncthing",
    "section": "Sharing Directories",
    "text": "Sharing Directories\nThe first thing to do is create a directory on your phone for your Obsidian vault.\nOnce the devices are connected you can start to share directories. Since this is an Obsidian tutorial we only need to share the vault directory, but you can use the same process to share, for example, your photo or image directories on your phone so that you automatically get copies on your computer.\nWhen we add (and delete) Syncthing directories, we aren’t really adding or removing any directories or files. We are adding an identifier which points to a directory on your device or system. So deleting a Syncthing directory is an entirely safe process.\nThat said, go ahead and add a Syncthing directory on your computer. Give the directory a specific name (Obsidian, maybe) and point it to the directory on your computer which contains your vault. \nIn the sharing tab, select your phone or other device. \nAs when we added a device earlier, an alert will appear, this time on your phone. Accept to add the offered directory, and you will be prompted to give the directory a name and location. The name may or may not be the same as the one on the computer, since Syncthing is actually using the long, auto-generated string in the middle which you left alone, right?\nThen select the new directory we created earlier as the location, and that’s it. Depending on the size of the files in the directory the initial sync can take a while. Since Obsidian files are just text files even the initial sync should be rapid. If you are syncing your phone images, the initial sync can take hours. Once they are synced, though, images from your phone will appear on your computer in seconds.",
    "crumbs": [
      "Obsidian",
      "Syncing your Thinking with Syncthing"
    ]
  },
  {
    "objectID": "posts/obsidian/syncing-your-thinking-syncthing/index.html#closing-thoughts",
    "href": "posts/obsidian/syncing-your-thinking-syncthing/index.html#closing-thoughts",
    "title": "Syncing your Thinking with Syncthing",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nFrom the wonderful world of FOSS we have a synchronization tool ideal for keeping directories in sync across personal devices. Perfect for Obsidian!",
    "crumbs": [
      "Obsidian",
      "Syncing your Thinking with Syncthing"
    ]
  },
  {
    "objectID": "posts/obsidian/second-brain/index.html",
    "href": "posts/obsidian/second-brain/index.html",
    "title": "Obsidian: A Second Brain?",
    "section": "",
    "text": "Obsidian: A Second Brain?\nObsidian is sometimes called a second brain. This is because the structure of a vault, with notes connected by links, is the way a neural network is structured, with notes as nodes and links as edges. Such data structures are typically represented by graphs, and Obsidian has a built-in Graph View for this purpose.\nIn this video, I visually explore the concept of Obsidian as a second brain.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Obsidian",
      "Obsidian: A Second Brain?"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-jupyter/index.html",
    "href": "posts/obsidian/obsidian-jupyter/index.html",
    "title": "Obsidian and Jupyter Notebooks",
    "section": "",
    "text": "Personal Knowledge Management for Data Science\n\n\n\nJupyter notebooks, or more properly iPython notebooks, are fantastic tools for data exploration and modeling. You can run bits of code, interspersed with blocks of markdown, allowing you to easily work with data and present analyses and forecasts in a visual and interactive format. Notebooks can be easily shared via GitHub, or run on-line with Colab.\nThe problem with iPython notebooks, from a Personal Knowledge Management perspective, is they end up being “books on the shelf”. For Obsidian users and PKM practitioners, the whole point is to get away from keeping information in notebooks, with all the uselessness that that implies. One could, as you will see, just do data exploration directly in Obsidian instead of in a standard iPython notebook. But Obsidian is not an IDE. The ecosystem around the interactive Python notebook IDEs, be it Jupyter Lab itself, or VS Code, etc, is so useful, that working in Obsidian while exploring data and creating models would be unacceptably tedious. Using fit-for-purpose tools is very important for efficiency.\nWe need a painless way to capture all this information in our vault, making notes out of the notebooks. How can we easily make this information future-useful, without repeating/duplicating our efforts or doing a ton of copy/paste? In this article, I’ll explain the solution that works well for me, ensuring Obsidian-speed access to any bits of information I have in my notebooks, as well as making review and study activities so much more pleasant. And, of course, my canvases have all gotten richer as well.\n\n\n\nThe community plugin needed to accomplish this is called Execute Code, written by Tim Wibiral together with Jupyter and a library called nbconvert. The latter will convert the notebooks to markdown, and the plugin allows you to execute the code directly within a note. To get started, create a virtual environment for Obsidian to use. If you aren’t using virtual environments, please start now! It’s simple, and you will avoid future problems. From the command line, do:\nmkdir -p $HOME/.config/venvs && cd \"$_\"\npython -m venv obsidian_venv\nsource obsidian_venv/bin/activate\npip install --upgrade pip\npip install jupyterlab nbconvert \nYou can install other libraries like Pandas and Matplotlib as well into the virtual environment with pip install. Jupyter lab and nbconvert will be necessary to convert the notebooks to markdown. At this point, you could launch jupyter lab, but there is no need to. After installing packages, you can exit the virtual environment with deactivate. Should you need to install more packages later, you can type source $HOME/.config/venvs/obsidian_venv/bin/activate to re-enter the virtual environment and pip install other packages.\nIn Obsidian, install the Execute Code plugin. After installing the plugin you must point it to the version of Python you want to use, in this case the one we made in the virtual environment above. In the settings for the plugin, under the language-specific settings, choose Python from the drop down list. For Python Path, enter /home/directory/.config/venvs/obsidian_venv/bin/python.\nWith that done, any code block with the keyword python added directly after the opening back ticks of the block can be run in the Note. In Read view, a Run button will appear by each code block, allowing you to execute the code in the block. After execution, there will be a Clear button to clear up output that you want removed from the note. Code can also be executed from Edit view by using the keyword run-python rather than simply python. The plugin offers a command to run all the code in the note, as well as a command to view and kill any active runtimes.\n\n\n\nJupyter Lab can export an ipynb file directly to markdown! As of writing, VS Code can only export to py, pdf or html. From the file menu, select Export and choose Markdown. This will generate a zipped archive containing a markdown page, along with any image files in the notebook. The problem with this approach is that you will find all of the image files named output something, and so after exporting a few notebooks, there will be name conflicts in your vault.\nUsing the command line avoids this problem, and is in any case much more efficient. You will need to activate your virtual environment with source as described above. Then type\njupyter nbconvert --to Markdown your_notebook.ipynb\nThis will generate an md file which can be copied into your vault. If there are images from generated by the output, like graphs and other visuals, these will be put in a directory created by the above command. If you copy this directory, with all the image files into the vault directory that you use for attachments, the new md file will find them. (It is important to copy the directory itself and not just the files, since the new note will expect to find them there.)\nOnce in Obsidian you can process your Notebooks as any other file, breaking them into bite-size chunks of information. I rely heavily on “Extract this heading”, available with a right-click on any heading in a note. This replaces the section with a link to a newly-created note containing the section’s content. I find it useful to use a template that loads common libraries, since they will need to be added them at the top of the new files.\nWhen converting your notebooks to notes, be aware that different notes do not share the same runtime. Be sure to include all the variables/calculations necessary for the part of the code you extracted in the new file, as this will not be available from another file’s state. Also, any images that you link to in the markdown sections of the original notebook will need to be manually copied into the vault, as only images generated from code in the file will be exported. ## Closing Thoughts\nI first started using Obsidian for the specific purpose of studying data science. My use of Obsidian broadened considerably and quickly once I first began with it. However, after some time I realized that, because of the nature of iPython notebooks, and the necessity, or really the pleasure, of using them, I found myself many months later in the very position I was trying to avoid vis-a-vis my notes: information I needed was somewhere in my piles of notebooks, and I turned to Google more often than searching through my notebooks.\nNow I can immerse myself in the Python project or study I am focusing on, knowing that after I’m finished, generating proper atomic notes from the work I’m doing will be a breeze. I hope that you may find this information useful. Happy coding!",
    "crumbs": [
      "Obsidian",
      "Obsidian and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-jupyter/index.html#motivation",
    "href": "posts/obsidian/obsidian-jupyter/index.html#motivation",
    "title": "Obsidian and Jupyter Notebooks",
    "section": "",
    "text": "Jupyter notebooks, or more properly iPython notebooks, are fantastic tools for data exploration and modeling. You can run bits of code, interspersed with blocks of markdown, allowing you to easily work with data and present analyses and forecasts in a visual and interactive format. Notebooks can be easily shared via GitHub, or run on-line with Colab.\nThe problem with iPython notebooks, from a Personal Knowledge Management perspective, is they end up being “books on the shelf”. For Obsidian users and PKM practitioners, the whole point is to get away from keeping information in notebooks, with all the uselessness that that implies. One could, as you will see, just do data exploration directly in Obsidian instead of in a standard iPython notebook. But Obsidian is not an IDE. The ecosystem around the interactive Python notebook IDEs, be it Jupyter Lab itself, or VS Code, etc, is so useful, that working in Obsidian while exploring data and creating models would be unacceptably tedious. Using fit-for-purpose tools is very important for efficiency.\nWe need a painless way to capture all this information in our vault, making notes out of the notebooks. How can we easily make this information future-useful, without repeating/duplicating our efforts or doing a ton of copy/paste? In this article, I’ll explain the solution that works well for me, ensuring Obsidian-speed access to any bits of information I have in my notebooks, as well as making review and study activities so much more pleasant. And, of course, my canvases have all gotten richer as well.",
    "crumbs": [
      "Obsidian",
      "Obsidian and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-jupyter/index.html#setting-up-obsidian",
    "href": "posts/obsidian/obsidian-jupyter/index.html#setting-up-obsidian",
    "title": "Obsidian and Jupyter Notebooks",
    "section": "",
    "text": "The community plugin needed to accomplish this is called Execute Code, written by Tim Wibiral together with Jupyter and a library called nbconvert. The latter will convert the notebooks to markdown, and the plugin allows you to execute the code directly within a note. To get started, create a virtual environment for Obsidian to use. If you aren’t using virtual environments, please start now! It’s simple, and you will avoid future problems. From the command line, do:\nmkdir -p $HOME/.config/venvs && cd \"$_\"\npython -m venv obsidian_venv\nsource obsidian_venv/bin/activate\npip install --upgrade pip\npip install jupyterlab nbconvert \nYou can install other libraries like Pandas and Matplotlib as well into the virtual environment with pip install. Jupyter lab and nbconvert will be necessary to convert the notebooks to markdown. At this point, you could launch jupyter lab, but there is no need to. After installing packages, you can exit the virtual environment with deactivate. Should you need to install more packages later, you can type source $HOME/.config/venvs/obsidian_venv/bin/activate to re-enter the virtual environment and pip install other packages.\nIn Obsidian, install the Execute Code plugin. After installing the plugin you must point it to the version of Python you want to use, in this case the one we made in the virtual environment above. In the settings for the plugin, under the language-specific settings, choose Python from the drop down list. For Python Path, enter /home/directory/.config/venvs/obsidian_venv/bin/python.\nWith that done, any code block with the keyword python added directly after the opening back ticks of the block can be run in the Note. In Read view, a Run button will appear by each code block, allowing you to execute the code in the block. After execution, there will be a Clear button to clear up output that you want removed from the note. Code can also be executed from Edit view by using the keyword run-python rather than simply python. The plugin offers a command to run all the code in the note, as well as a command to view and kill any active runtimes.",
    "crumbs": [
      "Obsidian",
      "Obsidian and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-jupyter/index.html#processing-a-notebook",
    "href": "posts/obsidian/obsidian-jupyter/index.html#processing-a-notebook",
    "title": "Obsidian and Jupyter Notebooks",
    "section": "",
    "text": "Jupyter Lab can export an ipynb file directly to markdown! As of writing, VS Code can only export to py, pdf or html. From the file menu, select Export and choose Markdown. This will generate a zipped archive containing a markdown page, along with any image files in the notebook. The problem with this approach is that you will find all of the image files named output something, and so after exporting a few notebooks, there will be name conflicts in your vault.\nUsing the command line avoids this problem, and is in any case much more efficient. You will need to activate your virtual environment with source as described above. Then type\njupyter nbconvert --to Markdown your_notebook.ipynb\nThis will generate an md file which can be copied into your vault. If there are images from generated by the output, like graphs and other visuals, these will be put in a directory created by the above command. If you copy this directory, with all the image files into the vault directory that you use for attachments, the new md file will find them. (It is important to copy the directory itself and not just the files, since the new note will expect to find them there.)\nOnce in Obsidian you can process your Notebooks as any other file, breaking them into bite-size chunks of information. I rely heavily on “Extract this heading”, available with a right-click on any heading in a note. This replaces the section with a link to a newly-created note containing the section’s content. I find it useful to use a template that loads common libraries, since they will need to be added them at the top of the new files.\nWhen converting your notebooks to notes, be aware that different notes do not share the same runtime. Be sure to include all the variables/calculations necessary for the part of the code you extracted in the new file, as this will not be available from another file’s state. Also, any images that you link to in the markdown sections of the original notebook will need to be manually copied into the vault, as only images generated from code in the file will be exported. ## Closing Thoughts\nI first started using Obsidian for the specific purpose of studying data science. My use of Obsidian broadened considerably and quickly once I first began with it. However, after some time I realized that, because of the nature of iPython notebooks, and the necessity, or really the pleasure, of using them, I found myself many months later in the very position I was trying to avoid vis-a-vis my notes: information I needed was somewhere in my piles of notebooks, and I turned to Google more often than searching through my notebooks.\nNow I can immerse myself in the Python project or study I am focusing on, knowing that after I’m finished, generating proper atomic notes from the work I’m doing will be a breeze. I hope that you may find this information useful. Happy coding!",
    "crumbs": [
      "Obsidian",
      "Obsidian and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-1/index.html",
    "href": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-1/index.html",
    "title": "Obsidian: Stop Wasting Time With Directories and Filenames",
    "section": "",
    "text": "The Problem\nBecause we’ve all used computers, when we think of organizing our stuff we naturally think in terms of files and directories, and are aware of the value of descriptive file names. Personally, if I even need to scroll down to see all the files in a single directory, it’s probably time for a subdirectory. And I’m equally unsettled when I see files with meaningless or unclear names. Many people like me explore various systems like PARA, ACCESS, LATCH, etc. to organize our stuff.\nTo be explicit, the stuff referred to here is files. But we use Obsidian for information management, not file management.\nObsidian stores information, but it’s not a database, at least not an SQL database. It is essentially a NoSQL database (more on that in Part 2 and 3). It stores information in text files, but it doesn’t care where the files are nor what they are called. So why should we care?\nI should point out that the word “notes” can mean two different things, really: the file or the information it contains. Decoupling the two meanings, I really only care about the information. The file must exist, of course, but it is not important. What is important is the information in it.\nIf I think in terms of files, I need to spend time thinking about file names and locations, both when recording and retrieving information. I need to navigate to files and directories and/or think about and type out file names. This is all wasted time because, again, the files aren’t important, it’s the information in them that I care about.\nObsidian is powerful and allows us to work with our information in many ways without stopping our flow of thought to worry about files. Stop spending your time in the Navigation pane and start using Search and Bookmarks. That way, you can focus all your attention where it should be, on your information.\n\n\nInformation First\nOk, full disclosure, I do use two directories in my vault, one for templates and one for non-md files. Templates would be impractical to use lumped with all the other files. And attachments aren’t native Obsidian formats and raise other maintenance issues.\nObsidian offers core functionality that allows me to ignore files and focus on information. I’m referring to Search, Bookmarks and Unique Notes. The latter is effectively invisible in articles and videos, and the other two rarely come up. This is especially surprising with regard to Search because that’s the most efficient and powerful way to find information in your vault. Most presenters use Quick Switcher or the Navigator to create and access files in order to create or access information. This is an extra step, wasting time and energy and impeding the flow of thought.\nCtrl-Shft-F is all I need to do to start querying my information. I don’t need to open any files, I just start typing what I’m looking for. If I think I might look for the same or similar information later, I can bookmark the filter.\nWhen I want to write something down, I hit Ctrl-U (I’ve got an assigned hotkey), type my thought, then immediately return to my reading. Again, I don’t have to think about files and directories. It’s that simple, fast, and does not impede my thinking with irrelevant considerations and extra keystrokes.\n\n\nSearch\nSimply typing the word or phrase I’m looking for in the search field will bring up all directly relevant notes on my topic. In case of TMI, I can specify that the terms should be in the same section or the same line. With Hover Editor, I can see the contents of each file listed without opening it, and even modify content directly if I want. I don’t need to actually open a file to get the information I’m looking for, or even modify it.\nSearch filters are just as powerful as Dataview queries. I can use logical AND, OR and NOT, group them with parentheses and sort by modified date. I can include or exclude based on any criteria (WHERE) and specify source (FROM). And I can save the filters as bookmarks. This way I don’t need to have a file that contains a query. The query results don’t “live” in a file, either. I do use Dataview queries frequently to aggregate information that I want to display information in notes. I do not use it to aggregate files that I want to interact with. That’s what Search is for.\n\n\nBookmarks\nI can bookmark files, of course, but I don’t… instead I bookmark sections or blocks (information, not files). I can also bookmark searches and graphs, and organize bookmarks in directories and subdirectories. In most of the videos I see, people have the Navigator panel open “by default”. For me, it’s the bookmarks panel. The Bookmarks panel provides an information tree, the Navigator panel provides a file tree. I want the information tree.\nI can use bookmarks and metadata to mimic directories if I want. My new notes used to go in a certain directory called +, and I had an “Inbox” note with a Dataview query to display the notes. The default template I now apply to every new note contains a field status:: new. I have also bookmarked a search filter “status:: new”. To see all of my new notes, I don’t need to open a directory or go to a file containing a Dataview query. All I need to do is click on the bookmark.\nAny useful search can be bookmarked. I have a bookmark, “action”, for example, which shows all files requiring attention. I always have many bookmarked searches and graphs related to topics I’m focused on, organized in virtual directories.\n\n\nUnique Notes\nMost people are unfamiliar with Unique Notes, even though it is a core plugin. This plugin will generate a new file with a unique name in a pre-defined location and automatically apply a specified template. The idea of randomly-named files dumped in one directory does seem scary, but don’t worry. The default template is, for me at least, entirely metadata. Metadata can be either frontmatter or double-colon in-line style. My template currently looks like this:\n%%\ntopic-ex::\nrelated-ex::\ncreated-ex:: &lt;% tp.date.now(\"YYYY-MM-DD\") %&gt;\nlast edit-ex::\ntype-ex:: article\nstatus-ex:: 0\nsource-ex::\ntitle-ex::\nlink-ex::\naction-ex:: false\n%%\ntopic and related take meaningfully named links, possibly to non-existent files (Obsidian doesn’t care about existence either). type can be atomic, moc, log, etc. Everything else should be self-explanatory. I don’t have to fill any of this in when I’m making notes. I can just jot down the thought and keep reading and thinking. And I can get back to them later by opening my bookmarked “status:: new” search. When I revisit the note to make it atomic, I can fill in the rest of the fields.\nWith this plugin, as I said earlier, I just press Ctrl-U and start writing my information without ever thinking about file names and directories.\nEfficiency Tip: When I’m reading a book or article, knowing I will take several notes in succession from the same source, I temporarily edit the default template to include basic information like source, title, etc. so I don’t have to type what is essentially boilerplate when I process the note and make it atomic.\n\n\nTry it out without commitment\nSince I started with Obsidian, I’ve changed the way I work multiple times. But rarely do I need to “clean up” old notes. For example, as my logging system has evolved, I have left all my old logs and their related Dataview queries in place, adding the newly written queries above them. It just means I have multiple query blocks instead of one in my Exercise Note, for example, but the information displayed remains the same.\nFuller disclosure: I actually have more directories in my vault than just templates and attachments. I used to use the ACCESS directory structure and have many, many notes. When I switched to this information first way, I stopped caring about files, remember, and I didn’t want to start this journey by launching into a massive exercise of moving files around, and adding metadata to all my old notes. That would be unacceptably ironic.\nObsidian is truly a compassionate ally. Very little needs to be done. I had to re-write some queries, as mentioned above. But any Dataview queries which aggregated files instead of information are no longer necessary anyway. For example, my Inbox note that displayed my new notes became irrelevant, since I now have a bookmarked filter, “status:: new”. If directory locations were really important in some queries, just add “(path:old-directory) OR” to your filters.\nYou could just continue to keep your Calendar notes in the same place as you do now, for that matter, or any other plugin that already has directory locations specified in settings. There is no need to be doctrinaire and rigid, after all, since we don’t care about directories anyway. Keep it simple, flexible, and don’t waste time on things that don’t add value.\nFinally, I’ll point out that I didn’t give up on meaningful file names, just at the point of initially writing down an idea. One step in processing a note to ensure its completion and atomicity is to change the name of the file. At the processing stage, choosing a name does not interrupt the flow of thinking, but complements it, since it helps encapsulate an idea and solidify my understanding. It’s just not something to do when making a fleeting note, that’s all. And the name will be useful later.\n\n\nClosing Thoughts\nThis article covers recording and retrieval of information. The second part covers how to analyze and synthesize information with this system.\nI hope I have convinced you at least to give it a try. Obsidian offers so many ways for us to free our thinking and focus on information and flow of thought. Keep the file tree closed, instead use the information tree.\nI am aware that this topic may be controversial because it goes so clearly against most of the approaches proposed on the internet. I do not mean any disrespect to the authors and videographers who have helped and inspired my journey with Obsidian, and to whom I am grateful.\nContinue to Part 2\n\n\n\n\n Back to top",
    "crumbs": [
      "Obsidian",
      "Freeing Your Thinking",
      "Obsidian: Stop Wasting Time With Directories and Filenames"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking.html",
    "href": "posts/obsidian/freeing-your-thinking.html",
    "title": "Freeing your thinking",
    "section": "",
    "text": "Obsidian: Stop Wasting Time With Directories and Filenames\n\n\n\nObsidian\n\nPKM\n\n\n\nFreeing Your Thiking Part 1 introduces the idea of of working with Obsidian as a database. This part covers capture and retrieval of information useing Unique Notes, Search and Bookmarks.\n\n\n\n\n\nAug 24, 2023\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian: Meaningless Names, No Directories, Now What?\n\n\n\nObsidian\n\nPKM\n\nGraphs\n\n\n\nFreeing Your Thiking Part 2 discusses the use of Graph View and Bookmarks to discover and organize information in your vault\n\n\n\n\n\nAug 24, 2023\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian: Freeing Your Thinking Workflow\n\n\n\nObsidian\n\nPKM\n\n\n\nA video showing how to use the information-focused Freeing Your Thinking paradigm.\n\n\n\n\n\nSep 21, 2023\n\n\nBrian Carey\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "Freeing your thinking"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian.html",
    "href": "posts/obsidian/obsidian.html",
    "title": "Obsidian",
    "section": "",
    "text": "Obsidian is Personal Knowledge Management PKM software. If you don’t know what Obsidian or PKM are, you could start with this article. I have many articles here about advanced usage, taking full advantage of Obsidian’s design. Freeing your thinking explains how to move beyond directory structures, taking advantage of the no-sql nature of Obsidian. There is also a series about the usage of dataviewjs, written for non-programmers, to create dashboards and interactive tables.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\nObsidian and Jupyter Notebooks\n\n\nBrian Carey\n\n\n\n\n\n\nJan 20, 2024\n\n\nObsidian Canvas Work Spaces\n\n\nBrian Carey\n\n\n\n\n\n\nJan 19, 2024\n\n\nGit and GitHub for Obsidian Users\n\n\nBrian Carey\n\n\n\n\n\n\nOct 16, 2023\n\n\nSummarizing Information with DataviewJS\n\n\nBrian Carey\n\n\n\n\n\n\nOct 14, 2023\n\n\nInteractive Tables with DataviewJS\n\n\nBrian Carey\n\n\n\n\n\n\nOct 11, 2023\n\n\nDataviewJS: A Gentle Introduction\n\n\nBrian Carey\n\n\n\n\n\n\nOct 5, 2023\n\n\nObsidian: Pretty Canvas\n\n\nBrian Carey\n\n\n\n\n\n\nSep 21, 2023\n\n\nObsidian: Freeing Your Thinking Workflow\n\n\nBrian Carey\n\n\n\n\n\n\nSep 14, 2023\n\n\nObsidian Canvas Dashboards\n\n\nBrian Carey\n\n\n\n\n\n\nSep 6, 2023\n\n\nObsidian Metadata Menu Plugin\n\n\nBrian Carey\n\n\n\n\n\n\nSep 5, 2023\n\n\nObsidian Bookmarks\n\n\nBrian Carey\n\n\n\n\n\n\nAug 31, 2023\n\n\nObsidian: The mechanics of Graph View\n\n\nBrian Carey\n\n\n\n\n\n\nAug 30, 2023\n\n\nObsidian: A Second Brain?\n\n\nBrian Carey\n\n\n\n\n\n\nAug 28, 2023\n\n\nSyncing your Thinking with Syncthing\n\n\nBrian Carey\n\n\n\n\n\n\nAug 27, 2023\n\n\nObsidian Basics - Headers\n\n\nBrian Carey\n\n\n\n\n\n\nAug 24, 2023\n\n\nObsidian: Stop Wasting Time With Directories and Filenames\n\n\nBrian Carey\n\n\n\n\n\n\nAug 24, 2023\n\n\nObsidian: Meaningless Names, No Directories, Now What?\n\n\nBrian Carey\n\n\n\n\n\n\nAug 24, 2023\n\n\nWhat does it mean that is a NoSQL Database?\n\n\nBrian Carey\n\n\n\n\n\n\nAug 23, 2023\n\n\nRegular Expressions for In-Line Fields\n\n\nBrian Carey\n\n\n\n\n\n\nAug 5, 2023\n\n\nOptimal Notes with Obsidian\n\n\nBrian Carey\n\n\n\n\n\n\nFeb 28, 2023\n\n\nDataviewJS: A Gentle Introduction Part 2\n\n\nBrian Carey\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "Obsidian"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-no-sql-database/index.html",
    "href": "posts/obsidian/obsidian-no-sql-database/index.html",
    "title": "What does it mean that is a NoSQL Database?",
    "section": "",
    "text": "SQL and NoSQL\nThere are two different types of databases, relational and non-relational. Relational databases are the kind most people usually think about. Data is kept in tables, with columns for fields and rows for values. The tables have special columns which relate (link) them to other tables. A well-formed table in a spreadsheet is a database. If two are linked by a common column, that is a relational database.\nObsidian is obviously not a relational database.\nSQL stands for Structured Query Language, and it is the syntax which was designed to work with relational databases. The association of the syntax and data structure is so strong that relational databases came to be referred to as SQL databases, even though SQL, the language, can be used with any type of database.\nAn SQL query should look familiar to you if you have used Dataview:\nSELECT name FROM users WHERE location = “Earth” ORDER BY age;\nIt’s just like Dataview queries, yet Obsidian is not an “SQL database”. My point is that we can use SQL queries to access our data even if it isn’t a relational database.\nThe other type of database, a non-relational database, is unfortunately called a NoSQL database. SQL, the language, can still be used to query data in a NoSQL database. If I’ve adequately confused you, let’s proceed.\nRather than storing data in tables, NoSQL databases store data in documents. Documents are text files written in a standard format, usually JSON (JavaScript Object Notation) but frequently YAML, which is the syntax used in frontmatter in Obsidian.\nIn a NoSQL database, documents contain information and metadata in key/value pairs. The metadata can be used to make relationships (links) between data (information). NoSQL databases are flexible and new key/value pairs can be added to a document without modifying a whole table. None of the relationships need be pre-defined, nor need they be hierarchical. NoSQL databases are ideal for data structures which consist of nodes (documents) and edges (links). Does any of this sound familiar?\nObsidian is a NoSQL database.\n\n\nOn Ontologies\nCommonly used organizational systems often focus on ontological relationships. Four relationships exist: parent, child, sibling, and friend. First, note that the first three are essentially the same since when a parent is defined, child and sibling relationships are logically specified. So we really only have two categories, which could just as easily be called “directly related” and “indirectly related”. Also, notice that in file-based hierarchical, ontological data structures a note (usually) has, rather unnaturally, only one parent.\nObsidian offers core functionality that allows me to ignore files and focus on information. I’m referring to Search, Bookmarks and Unique Notes. The latter is effectively invisible in articles and videos, and the other two rarely come up. This is especially surprising with regard to Search because that’s the most efficient and powerful way to find information in your vault. Most presenters use Quick Switcher or the Navigator to create and access files in order to create or access information. This is an extra step, wasting time and energy and impeding the flow of thought.\nSQL databases rely on hierarchy. NoSQL databases don’t. By thinking in hierarchical, ontological terms, we tie ourselves to a table-based way of thinking. By thinking instead about direct and indirect relationships, a note can have multiple “parents” because it can naturally relate directly to more than one topic. (We all have at least two parents in life, and often have other people who fill the role as well.) These articles, for example, have two main topics, two direct relationships: Obsidian and Personal Knowledge Management. I don’t have to choose one or the other. I might decide to write about information processing, in which case these articles could be given another “parent”, the new topic.\n\n\nInteracting with NoSQL Databases\nThe word graph is commonly used to refer to many different types of charts, such as bar charts or line graphs. It has a more specific meaning, however, which is the specific type of graph produced by Graph View. This is the normal way that these data structures are visualized, which is why Graph View is the ideal way to visually explore our vault. This is also the way neural networks are visualized.\nGetting information from NoSQL databases is done through queries, not by opening documents. Filters in Obsidian Search have all the capabilities of Dataview queries, with a somewhat simpler syntax.\n\n\nFinal words\nI hope I have explained what it means to say that Obsidian is a NoSQL and the implications of that. Everything we write, every word, becomes part of a database, and we can use the power of databases to retrieve information. Stop using Obsidian as a file manager!\n\n\n\n\n Back to top",
    "crumbs": [
      "Obsidian",
      "What does it mean that is a NoSQL Database?"
    ]
  },
  {
    "objectID": "posts/obsidian/visual-mocs-with-canvas/index.html",
    "href": "posts/obsidian/visual-mocs-with-canvas/index.html",
    "title": "Obsidian Canvas Dashboards",
    "section": "",
    "text": "This is the last in a series of articles, including Freeing Your Thinking and Building a Knowledge Tree, where I talk about using Obsidian as a non-relational database. The main idea is to interact with information, not files, and Obsidian’s core plugins facilitate this approach. I have talked about Search, Bookmarks, Unique Notes and Graph View. This article will describe how to use Canvas to create visual maps of content which not only present information in a clear, attractive and flexible way, but also allow you to explore and interact with your information. As usual, our goal is to obviate the need to open files.\nIn data science there is a motto “visualize, visualize, visualize.” In the business world, information is commonly presented in the form of a dashboard to allow for visualizing data. A dashboard is a graphical user interface containing views of information. Aside from providing graphical elements and views, a dashboard will typically provide navigational tools and other ways to interact with information and even edit information directly from the dashboard.\nBefore discussing canvas, I want to mention Excalidraw, an excellent Community Plugin by Zsolt Viczián. It is great for creating visual maps of content with many graphical elements easily available. I highly recommend it. Nevertheless, one of the purposes of this series is to highlight Obsidian’s native tools, and Canvas is quite powerful in its own right.",
    "crumbs": [
      "Obsidian",
      "Obsidian Canvas Dashboards"
    ]
  },
  {
    "objectID": "posts/obsidian/visual-mocs-with-canvas/index.html#topical-dashboard",
    "href": "posts/obsidian/visual-mocs-with-canvas/index.html#topical-dashboard",
    "title": "Obsidian Canvas Dashboards",
    "section": "Topical Dashboard",
    "text": "Topical Dashboard\nCanvases are ideal for pulling together information on a given topic and presenting it in a visually interesting way. As a student, I can accumulate information (not files) on a topic and add it to my canvas. This makes review of the topic easy. As a teacher, such a MOC could provide the basis of a lesson plan, and could even be distributed as course material.\nAs an example, this is a canvas covering the Normal Distribution, an important topic in probability and statistics. \nAll of my information is grouped in categories. The information itself is drawn from specific sections or blocks within notes, not entire notes. I can navigate the canvas with a convenient drop down, allowing me to jump to a particular section. This is especially convenient on large canvases. \nOtherwise I can simply use the mouse to move around the canvas and zoom in and out as I review my information.",
    "crumbs": [
      "Obsidian",
      "Obsidian Canvas Dashboards"
    ]
  },
  {
    "objectID": "posts/obsidian/visual-mocs-with-canvas/index.html#interactive-dashboard",
    "href": "posts/obsidian/visual-mocs-with-canvas/index.html#interactive-dashboard",
    "title": "Obsidian Canvas Dashboards",
    "section": "Interactive Dashboard",
    "text": "Interactive Dashboard\nThis is an example of using a visual MOC to both present information and also manage information. It includes interactive tables, previews of documents in progress, and graphics to identify the stage of development of each project.\n\nThanks to Metadata Menu, I can edit the metadata for any document without opening it. \n\nUsing Hover editor, I can even edit a document without leaving the dashboard.",
    "crumbs": [
      "Obsidian",
      "Obsidian Canvas Dashboards"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-bookmarks/index.html",
    "href": "posts/obsidian/obsidian-bookmarks/index.html",
    "title": "Obsidian Bookmarks",
    "section": "",
    "text": "Bookmarks are the key to effectively using obsidian as a non-relational database. The virtues of approaching your notes this way are several and I covered some in the Freeing Your Thinking series. In this article I want to develop the idea of using Bookmarks to construct flexible, interactive Knowledge Trees to organize notes. Just as a reminder, we are organizing information, not files.\nThe Navigation tab gives at best a static arrangement of information, while the Bookmarks tab provides a dynamic way to interact with, develop and access your information. Here you can create Views of your information which you can organize by domain, subject, activity, etc. Views are saved filters and queries. Your information, unlike your files, can happily exist in multiple places in your Knowledge Tree.\nWhen you begin to build your Knowledge Tree, you might start by creating Views which map to your current directories using filters. After all, you want to preserve the information you have put into your vault by organizing your notes in a File Tree. Most people have a separate directory for fleeting notes, so you might make a view containing path: \"fleeting-note-directory\". But is such a view terribly useful? Browsing through all your fleeting notes is fun if you’ve nothing better to do, but it’s not focused or efficient use of time if you are working on a particular topic. Wouldn’t it be better to just see the fleeting notes related to my topic? So I save the filter Whatever Topic \\type:: fleeting\\ alongside my views of the topic itself and have a list of fleeting notes only on that topic.\nAs you start mapping your old directories to views, you might begin to ask yourself at some point why you would ever really want to see all your MOCs in a list? Or Daily Notes? Or Logs? If you want information from these types of notes you typically create a note containing a Dataview query. Bookmarked filters, which I’m calling views, are like virtual Dataview queries. And, since you can do this without referencing file locations, you might question how much useful information was actually embedded in your folder structure. Then you might have a cup of tea.\nOne last point to stress: the bookmarks, which I will call views, are ephemeral. They can be created, duplicated, renamed and moved around without affecting the information itself. When focusing on a topic, drag it to the top of the tree and put it back when you are done.",
    "crumbs": [
      "Obsidian",
      "Obsidian Bookmarks"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-bookmarks/index.html#commands",
    "href": "posts/obsidian/obsidian-bookmarks/index.html#commands",
    "title": "Obsidian Bookmarks",
    "section": "Commands",
    "text": "Commands\nThe following table shows the bookmarks commands with suggested Hotkeys which are most likely unassigned.\n\n\n\n\n\n\n\n\nCommand\nDescription\nHotkey\n\n\n\n\nBookmark…\nBookmark current file\nCtrl-Cmd-Alt-M\n\n\nShow Bookmarks\nOpen bookmarks panel\nCtrl-B\n\n\nBookmark Current Search\n\nCtrl-Cmd-Alt-S\n\n\nBookmark Current Block\nBookmark Block Under Cursor\nCtrl-Cmd-Alt-B\n\n\nBookmark Current Heading\nBookmark Heading Under Cursor\nCtrl-Cmd-Alt-H",
    "crumbs": [
      "Obsidian",
      "Obsidian Bookmarks"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-bookmarks/index.html#bookmarks-panel",
    "href": "posts/obsidian/obsidian-bookmarks/index.html#bookmarks-panel",
    "title": "Obsidian Bookmarks",
    "section": "Bookmarks Panel",
    "text": "Bookmarks Panel\nThe panel lists all bookmarks, which, remember, are saved searches, or views. In the panel you can create, rename and delete bookmark groups to organize your bookmarks. Bookmarks can be dragged around to rearrange them.",
    "crumbs": [
      "Obsidian",
      "Obsidian Bookmarks"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-bookmarks/index.html#living",
    "href": "posts/obsidian/obsidian-bookmarks/index.html#living",
    "title": "Obsidian Bookmarks",
    "section": "Living",
    "text": "Living\nThis is the structure of my Living section:\n- Daily Notes\n- Logs\n    - Exercise\n    - Piano\n    - etc.\n- Daily Notes\n- CV\n- People\n- Leisure\nDaily Notes is a view of type:: daily. Exercise and Piano are actual files with interactive Dataview queries. The rest are bookmark groups containing other views.",
    "crumbs": [
      "Obsidian",
      "Obsidian Bookmarks"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-bookmarks/index.html#learning",
    "href": "posts/obsidian/obsidian-bookmarks/index.html#learning",
    "title": "Obsidian Bookmarks",
    "section": "Learning",
    "text": "Learning\nI use Learning for study and research activities. This part of the tree is the most complex in terms of structure.\nBy creating multiple views on a given topic and organizing them in folders I can essentially create an outline of my information using views. This kind of outline, again, is very flexible and can be reshaped easily as my knowledge develops.\nThis is an excerpt of my section on the normal distribution. It’s “home” is in the bookmark group Learing/Data Science/Probability/.\nNormal Distribution                 # All information on the Normal Distribution\nNormal Distribution                 # The reference note on the topic\n                                      The 2 are distinguished by different icons\nCentral Limit Theorem               # A bookmark group containing other views\n68-95-99.7 Rule                     # A view of a block in a note\nR Functions for Normal Distribution # A view of a section of a note",
    "crumbs": [
      "Obsidian",
      "Obsidian Bookmarks"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-bookmarks/index.html#creating",
    "href": "posts/obsidian/obsidian-bookmarks/index.html#creating",
    "title": "Obsidian Bookmarks",
    "section": "Creating",
    "text": "Creating\nis for writing and creation of other content. Creating includes works for external and internal use.\n- Articles\n- Stories\n- Video Scripts\n- Musings",
    "crumbs": [
      "Obsidian",
      "Obsidian Bookmarks"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-bookmarks/index.html#toolbox",
    "href": "posts/obsidian/obsidian-bookmarks/index.html#toolbox",
    "title": "Obsidian Bookmarks",
    "section": "Toolbox",
    "text": "Toolbox\nIn the toolbox I have the following directories, which should be self-explanatory:\n- Templates\n- fileClasses\n    - Class Definitions\n    - Value Lists\n- Fleeting Notes\n- Other\n    - Articles\n    - MOCs\n    - References\n    - Atomics\nOther than templates and fileClasses, I rarely use the Toolbox.",
    "crumbs": [
      "Obsidian",
      "Obsidian Bookmarks"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html",
    "href": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html",
    "title": "Summarizing Information with DataviewJS",
    "section": "",
    "text": "This article follows up on earlier articles in which I showed how to create interactive tables which are automatically updated and can be edited directly in the table. This article will describe how to create summary statistics for data. The statistics on the dashboard update based on the current day.\nAs you can see, I’ve done a bit of rearranging since the last article to make things even more readable. The statistics tables have been re-sized and reformatted for a cleaner display, and I’ve added more information. There are still some tweaks I want to do, but this is a very useful dashboard.\nA quick comment on canvases: since I started working on canvases it’s hard to go back to the 80 characters wide format of a note. Sure, when I’m writing an article or focusing on a bit of code, that format feels very natural. But when doing pretty much anything else, well, it’s just nice to be able to spread out, and not have to open and close files, or page up and down through documents. Using a canvas is like putting all of your current work on your desk, with everything open to the pages you are interested in. In a future article I will show how a dashboard can be used in place of a Daily Note to manage your day.\nHere is the table I will show how to create in this article. As before, you can just skip to the bottom and grab the code if you want. This article explains everything rather thoroughly and introduces some general programming concepts. I think that, if you spend a little time with the material you will find that it is not, as they say, rocket science.\n\nThat’s 22 numbers to calculate! The process isn’t too complicated, but we need to keep track of a lot of things. To avoid a lot of repetition, we will create some custom functions, and you need to learn how to “push” lists into lists.\nLet’s start by writing down what we need to do.\n\nCreate the data sets\n\nGet the logs for current and prior periods\nSeparate by activity\n\nMake a variable and calculate each statistic for current and prior periods\n\ndistance for each activity and total (6 total)\nduration for each activity and total (6 total)\nspeed for each activity (4 total)\ncounts for each activity (6 total)\n\nGenerate the output\n\nCreate the tables’ headers\nGenerate the table rows\nCreate headers for the with workout totals\nDisplay the tables with header elements\n\n\n\n\nFirst, let’s grab the data for the current and prior seven day periods. In the last article I showed how to get the current period. Did you figure out how to get the prior period? One way is to first take the data from the last 14 days, and then remove the most recent 7 days. So we’ll do it like this:\nconst priorPages = dv.pages() \n                    .where(b =&gt; b.type == \"log\")\n                    .where(b =&gt; b.ActivityDate &gt;= moment().subtract(14, \"days\"))\n                    .where(b =&gt; b.ActivityDate &lt;= moment().subtract(7, \"days\")) \nCreate the currentPages data set as well. Then each needs to be separated by walking and biking. You can do it like this:\nconst currentWalkPages = currentPages\n                    .where(b =&gt; b.Activity == \"🚶\")\nGo ahead and create the rest of the datasets. You need currentBikePages, priorWalkPages and priorBikePages.\n\n\n\nGood programming involves avoiding repetitive tasks. Whenever you need to do the same thing more than once or twice, the standard practice is to make a function.\n\n\nHere is an example of a function we can use. It is arguably overkill to make this a function, but it will provide a good example. A function starts with the keyword function followed by the name you want to use. The name is followed by parentheses which are required, but may be empty. They are used to pass arguments to the function. The function itself is then placed in curly braces. After the function does it’s work, it needs to return some value.\nI have a bunch of numbers in minutes which I want to convert to hours. This is just a matter of dividing by 60 of course, but I also want to round off to one decimal. There is a function toFixed() which can be used on strings. It takes one argument, the number of decimal places you want to round it to. This is the function:\nfunction toHours (time) {\n return (time / 60).toFixed(1)\n}\nand I use it like this.\nconst totalCurrentTime = toHours( sumStat(\"duration\", pages) )\nOf course, I haven’t created sumStat() yet. To calculate the sum there is a JavaScript function called reduce(). It is used like this:\nreduce(\n    (sum, b) =&gt; sum + b.Distance,\n     0\n)\nThe syntax looks a little complicated, but it’s not so bad if you break it down. reduce() takes two arguments. The first argument is (sum, b) =&gt; sum + b.Distance, which is itself a function with two arguments. The second argument here is our old friend b. In this case, we also need temporary variable called an accumulator. The name of this variable is arbitrary, like b. reduce() will go through each log, adding the value of the Distance field to the accumulator. The second argument to reduce() is just the initial value for the accumulator, which is almost always 0.\nNow I can write the function like this\nfunction sumStat (stat, thePages) {\n    const pageValues = thePages.values\n    if (stat == \"duration\") {\n        return pageValues\n          .reduce((sum, b) =&gt; sum + b.Duration, 0)\n    }\n    \n    if (stat == \"distance\") {\n        return pageValues\n          .reduce((sum, b) =&gt; sum + b.Distance, 0)\n    }\n}\nI didn’t want to write two separate functions for calculating the sums, instead I use a conditional if statement, which works as you might expect. Notice a few things about this. First, pageValues just saves us adding .values to each if statement. if statements require a condition in parentheses. It must be something that evaluates to a boolean value (true or false, remember). Then, as usual, the block of code you want to execute must be in curly braces. Finally, you almost always want to return something. Once you hit a return statement, nothing else gets executed in the function.\n\n\n\nNow we can create all of our time and distance variables like this:\nconst totalCurrentTime = toHours( sumStat( \"duration\", currentPages ) )\nconst totalCurrentDistance = sumStat( \"distance\", currentPages ).toFixed(1)\nSpeed is just distance over time, so.\nconst currentWalkSpeed = (\n    totalCurrentWalkDistance / totalCurrentWalkTime\n    )\n    .toFixed(1)\nNotice that I specified to round to one decimal point. To find the total number of workouts it is necessary merely to count the number of pages in each category, since each represents one workout. pages() is a list, or array. Arrays all have an attribute/property called length. So to count all workouts in the last seven days I can just write const currentCount = currentPages.length.\nEverything else is just repetition, since we need 22 values at the end of the day. Any repetitive activity cries out for a new function to encapsulate this repetition, but I’ll leave that for another day. (This is called refactoring.)\n\n\n\n\nThe tables from the last article used a dv.pages() object for the table rows. This time I need to create each table row myself. Each table row is itself a list. So I need a list of lists. It is easy to do this with the push() command, which just adds an item to a list. The item being added can be anything, including a list. I will build the table rows like this:\nlet rows = []\nrows.push([\"Miles\", totalCurrentDistance, totalPriorDistance])\nI have to initialize the variable, because I can’t push anything onto a non-existent list. I used let instead of const. The difference is that a const variable cannot change after it has been created, while one declared with var can. (In older code you will see var instead of let. This still works but let is preferred.)\nSo all that remains is to push each row of the table into the rows variable, create the headers for the table, and use dv.table() to display it. The final code is at the bottom. I strongly suggest, having reached this point, that you try to do this on your own and don’t simply copy and paste my code.\n\n\n\nIf you read through both articles, congratulations. Give yourself a pat on the back. You are doing object-oriented programming in the most widely-used language on the internet. And you have all of the knowledge you need to create rich, useful dashboards.\nHappy coding!\n/* Create data sets  */\n\nconst currentPages = dv.pages() \n    .where(b =&gt; b.type == \"log\")\n    .where(b =&gt; b.ActivityDate &gt;= moment().subtract(7, \"days\"))\nconst priorPages = dv.pages() \n    .where(b =&gt; b.type == \"log\")\n    .where(b =&gt; b.ActivityDate &gt;= moment().subtract(14, \"days\"))\n    .where(b =&gt; b.ActivityDate &lt;= moment().subtract(7, \"days\")) \nconst currentWalkPages = currentPages\n    .where(b =&gt; b.Activity == \"🚶\")\nconst currentBikePages = currentPages\n    .where(b =&gt; b.Activity == \"🚴\")\nconst priorWalkPages = priorPages\n    .where(b =&gt; b.Activity == \"🚶\")\nconst priorBikePages = priorPages\n    .where(b =&gt; b.Activity == \"🚴\")\n\n/* Function to sum the distances, duration */\n\nfunction sumStat (stat, thePages) {\n    const pageValues = thePages.values\n    \n    if (stat == \"duration\") {\n        return pageValues\n          .reduce((sum, b) =&gt; sum + b.Duration, 0)\n    }\n    if (stat == \"distance\") {\n        return pageValues\n          .reduce((sum, b) =&gt; sum + b.Distance, 0)\n    }\n}\n\n/* Function to convert minutes to hours */\n\nfunction toHours (time) {\n return (time / 60).toFixed(1)\n}\n\n\n/* Calculate the distances (6 values) */\n\nconst totalCurrentDistance = sumStat( \"distance\", currentPages ).toFixed(1)\n\nconst totalCurrentWalkDistance = sumStat(\"distance\",currentWalkPages).toFixed(1)\nconst totalCurrentBikeDistance = sumStat(\"distance\", currentBikePages).toFixed(1)\nconst totalPriorDistance = sumStat(\"distance\",priorPages).toFixed(1)\nconst totalPriorWalkDistance = sumStat(\"distance\",priorWalkPages).toFixed(1)\nconst totalPriorBikeDistance = sumStat(\"distance\",priorBikePages).toFixed(1)\n\n/* Calculate the durations (6 values) */\n\nconst totalCurrentTime = toHours( sumStat( \"duration\", currentPages ) )\n\nconst totalCurrentWalkTime = toHours(sumStat(\"duration\",currentWalkPages))\nconst totalCurrentBikeTime = toHours(sumStat(\"duration\",currentBikePages))\nconst totalPriorTime = toHours(sumStat(\"duration\",priorPages))\nconst totalPriorWalkTime = toHours(sumStat(\"duration\",priorWalkPages))\nconst totalPriorBikeTime = toHours(sumStat(\"duration\",priorBikePages))\n\n/* Calculate the speed (4 values) */\n\nconst currentWalkSpeed = (totalCurrentWalkDistance / totalCurrentWalkTime).toFixed(1)\nconst priorWalkSpeed = (totalPriorWalkDistance / totalPriorWalkTime).toFixed(1)\nconst currentBikeSpeed = (totalCurrentBikeDistance / totalCurrentBikeTime).toFixed(1)\nconst priorBikeSpeed = (totalPriorBikeDistance / totalPriorBikeTime).toFixed(1)\n\n/* Count the workouts */\n\nconst currentCount = currentPages.length\nconst priorCount = priorPages.length\nconst currentBikeCount = currentBikePages.length\nconst priorBikeCount = priorBikePages.length\nconst currentWalkCount = currentWalkPages.length\nconst priorWalkCount = priorWalkPages.length\n\n\n\n/* Create an array (list) and add the table rows\n   The first table displays the distance and duration */\n   \nlet rows = []\nrows.push([\"Miles\", totalCurrentDistance, totalPriorDistance])\nrows.push([\"🚶\", totalCurrentWalkDistance, totalPriorWalkDistance])\nrows.push([\"🚴\", totalCurrentBikeDistance, totalPriorBikeDistance])\nrows.push([\"\",\"\",\"\"])\nrows.push([\"Hours\", totalCurrentTime, totalPriorTime])\nrows.push([\"🚶\", totalCurrentWalkTime, totalPriorWalkTime])\nrows.push([\"🚴\", totalCurrentBikeTime, totalPriorBikeTime])\nrows.push([\"\",\"\",\"\"])\nrows.push([\"Speed (m/h)\", \"\", \"\"])\nrows.push([\"🚶\", currentWalkSpeed, priorWalkSpeed])\nrows.push([\"🚴\", currentBikeSpeed, priorBikeSpeed])\nrows.push([\"\",\"\",\"\"])\nrows.push([\"Workouts\", currentCount, priorCount])\nrows.push([\"🚶\", currentWalkCount, priorWalkCount])\nrows.push([\"🚴\", currentBikeCount, priorBikeCount])\n\n/* Create the table header */\n\nconst tableHeader = [\"\",\"Current\",\"Prior\"]\n\n/* Display the table */\n\ndv.header(3, \"7 Day Stats\")\ndv.table(tableHeader, rows)",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Summarizing Information with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html#the-data",
    "href": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html#the-data",
    "title": "Summarizing Information with DataviewJS",
    "section": "",
    "text": "First, let’s grab the data for the current and prior seven day periods. In the last article I showed how to get the current period. Did you figure out how to get the prior period? One way is to first take the data from the last 14 days, and then remove the most recent 7 days. So we’ll do it like this:\nconst priorPages = dv.pages() \n                    .where(b =&gt; b.type == \"log\")\n                    .where(b =&gt; b.ActivityDate &gt;= moment().subtract(14, \"days\"))\n                    .where(b =&gt; b.ActivityDate &lt;= moment().subtract(7, \"days\")) \nCreate the currentPages data set as well. Then each needs to be separated by walking and biking. You can do it like this:\nconst currentWalkPages = currentPages\n                    .where(b =&gt; b.Activity == \"🚶\")\nGo ahead and create the rest of the datasets. You need currentBikePages, priorWalkPages and priorBikePages.",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Summarizing Information with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html#calculations",
    "href": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html#calculations",
    "title": "Summarizing Information with DataviewJS",
    "section": "",
    "text": "Good programming involves avoiding repetitive tasks. Whenever you need to do the same thing more than once or twice, the standard practice is to make a function.\n\n\nHere is an example of a function we can use. It is arguably overkill to make this a function, but it will provide a good example. A function starts with the keyword function followed by the name you want to use. The name is followed by parentheses which are required, but may be empty. They are used to pass arguments to the function. The function itself is then placed in curly braces. After the function does it’s work, it needs to return some value.\nI have a bunch of numbers in minutes which I want to convert to hours. This is just a matter of dividing by 60 of course, but I also want to round off to one decimal. There is a function toFixed() which can be used on strings. It takes one argument, the number of decimal places you want to round it to. This is the function:\nfunction toHours (time) {\n return (time / 60).toFixed(1)\n}\nand I use it like this.\nconst totalCurrentTime = toHours( sumStat(\"duration\", pages) )\nOf course, I haven’t created sumStat() yet. To calculate the sum there is a JavaScript function called reduce(). It is used like this:\nreduce(\n    (sum, b) =&gt; sum + b.Distance,\n     0\n)\nThe syntax looks a little complicated, but it’s not so bad if you break it down. reduce() takes two arguments. The first argument is (sum, b) =&gt; sum + b.Distance, which is itself a function with two arguments. The second argument here is our old friend b. In this case, we also need temporary variable called an accumulator. The name of this variable is arbitrary, like b. reduce() will go through each log, adding the value of the Distance field to the accumulator. The second argument to reduce() is just the initial value for the accumulator, which is almost always 0.\nNow I can write the function like this\nfunction sumStat (stat, thePages) {\n    const pageValues = thePages.values\n    if (stat == \"duration\") {\n        return pageValues\n          .reduce((sum, b) =&gt; sum + b.Duration, 0)\n    }\n    \n    if (stat == \"distance\") {\n        return pageValues\n          .reduce((sum, b) =&gt; sum + b.Distance, 0)\n    }\n}\nI didn’t want to write two separate functions for calculating the sums, instead I use a conditional if statement, which works as you might expect. Notice a few things about this. First, pageValues just saves us adding .values to each if statement. if statements require a condition in parentheses. It must be something that evaluates to a boolean value (true or false, remember). Then, as usual, the block of code you want to execute must be in curly braces. Finally, you almost always want to return something. Once you hit a return statement, nothing else gets executed in the function.\n\n\n\nNow we can create all of our time and distance variables like this:\nconst totalCurrentTime = toHours( sumStat( \"duration\", currentPages ) )\nconst totalCurrentDistance = sumStat( \"distance\", currentPages ).toFixed(1)\nSpeed is just distance over time, so.\nconst currentWalkSpeed = (\n    totalCurrentWalkDistance / totalCurrentWalkTime\n    )\n    .toFixed(1)\nNotice that I specified to round to one decimal point. To find the total number of workouts it is necessary merely to count the number of pages in each category, since each represents one workout. pages() is a list, or array. Arrays all have an attribute/property called length. So to count all workouts in the last seven days I can just write const currentCount = currentPages.length.\nEverything else is just repetition, since we need 22 values at the end of the day. Any repetitive activity cries out for a new function to encapsulate this repetition, but I’ll leave that for another day. (This is called refactoring.)",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Summarizing Information with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html#generate-the-output",
    "href": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html#generate-the-output",
    "title": "Summarizing Information with DataviewJS",
    "section": "",
    "text": "The tables from the last article used a dv.pages() object for the table rows. This time I need to create each table row myself. Each table row is itself a list. So I need a list of lists. It is easy to do this with the push() command, which just adds an item to a list. The item being added can be anything, including a list. I will build the table rows like this:\nlet rows = []\nrows.push([\"Miles\", totalCurrentDistance, totalPriorDistance])\nI have to initialize the variable, because I can’t push anything onto a non-existent list. I used let instead of const. The difference is that a const variable cannot change after it has been created, while one declared with var can. (In older code you will see var instead of let. This still works but let is preferred.)\nSo all that remains is to push each row of the table into the rows variable, create the headers for the table, and use dv.table() to display it. The final code is at the bottom. I strongly suggest, having reached this point, that you try to do this on your own and don’t simply copy and paste my code.",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Summarizing Information with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html#final-thoughts",
    "href": "posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/index.html#final-thoughts",
    "title": "Summarizing Information with DataviewJS",
    "section": "",
    "text": "If you read through both articles, congratulations. Give yourself a pat on the back. You are doing object-oriented programming in the most widely-used language on the internet. And you have all of the knowledge you need to create rich, useful dashboards.\nHappy coding!\n/* Create data sets  */\n\nconst currentPages = dv.pages() \n    .where(b =&gt; b.type == \"log\")\n    .where(b =&gt; b.ActivityDate &gt;= moment().subtract(7, \"days\"))\nconst priorPages = dv.pages() \n    .where(b =&gt; b.type == \"log\")\n    .where(b =&gt; b.ActivityDate &gt;= moment().subtract(14, \"days\"))\n    .where(b =&gt; b.ActivityDate &lt;= moment().subtract(7, \"days\")) \nconst currentWalkPages = currentPages\n    .where(b =&gt; b.Activity == \"🚶\")\nconst currentBikePages = currentPages\n    .where(b =&gt; b.Activity == \"🚴\")\nconst priorWalkPages = priorPages\n    .where(b =&gt; b.Activity == \"🚶\")\nconst priorBikePages = priorPages\n    .where(b =&gt; b.Activity == \"🚴\")\n\n/* Function to sum the distances, duration */\n\nfunction sumStat (stat, thePages) {\n    const pageValues = thePages.values\n    \n    if (stat == \"duration\") {\n        return pageValues\n          .reduce((sum, b) =&gt; sum + b.Duration, 0)\n    }\n    if (stat == \"distance\") {\n        return pageValues\n          .reduce((sum, b) =&gt; sum + b.Distance, 0)\n    }\n}\n\n/* Function to convert minutes to hours */\n\nfunction toHours (time) {\n return (time / 60).toFixed(1)\n}\n\n\n/* Calculate the distances (6 values) */\n\nconst totalCurrentDistance = sumStat( \"distance\", currentPages ).toFixed(1)\n\nconst totalCurrentWalkDistance = sumStat(\"distance\",currentWalkPages).toFixed(1)\nconst totalCurrentBikeDistance = sumStat(\"distance\", currentBikePages).toFixed(1)\nconst totalPriorDistance = sumStat(\"distance\",priorPages).toFixed(1)\nconst totalPriorWalkDistance = sumStat(\"distance\",priorWalkPages).toFixed(1)\nconst totalPriorBikeDistance = sumStat(\"distance\",priorBikePages).toFixed(1)\n\n/* Calculate the durations (6 values) */\n\nconst totalCurrentTime = toHours( sumStat( \"duration\", currentPages ) )\n\nconst totalCurrentWalkTime = toHours(sumStat(\"duration\",currentWalkPages))\nconst totalCurrentBikeTime = toHours(sumStat(\"duration\",currentBikePages))\nconst totalPriorTime = toHours(sumStat(\"duration\",priorPages))\nconst totalPriorWalkTime = toHours(sumStat(\"duration\",priorWalkPages))\nconst totalPriorBikeTime = toHours(sumStat(\"duration\",priorBikePages))\n\n/* Calculate the speed (4 values) */\n\nconst currentWalkSpeed = (totalCurrentWalkDistance / totalCurrentWalkTime).toFixed(1)\nconst priorWalkSpeed = (totalPriorWalkDistance / totalPriorWalkTime).toFixed(1)\nconst currentBikeSpeed = (totalCurrentBikeDistance / totalCurrentBikeTime).toFixed(1)\nconst priorBikeSpeed = (totalPriorBikeDistance / totalPriorBikeTime).toFixed(1)\n\n/* Count the workouts */\n\nconst currentCount = currentPages.length\nconst priorCount = priorPages.length\nconst currentBikeCount = currentBikePages.length\nconst priorBikeCount = priorBikePages.length\nconst currentWalkCount = currentWalkPages.length\nconst priorWalkCount = priorWalkPages.length\n\n\n\n/* Create an array (list) and add the table rows\n   The first table displays the distance and duration */\n   \nlet rows = []\nrows.push([\"Miles\", totalCurrentDistance, totalPriorDistance])\nrows.push([\"🚶\", totalCurrentWalkDistance, totalPriorWalkDistance])\nrows.push([\"🚴\", totalCurrentBikeDistance, totalPriorBikeDistance])\nrows.push([\"\",\"\",\"\"])\nrows.push([\"Hours\", totalCurrentTime, totalPriorTime])\nrows.push([\"🚶\", totalCurrentWalkTime, totalPriorWalkTime])\nrows.push([\"🚴\", totalCurrentBikeTime, totalPriorBikeTime])\nrows.push([\"\",\"\",\"\"])\nrows.push([\"Speed (m/h)\", \"\", \"\"])\nrows.push([\"🚶\", currentWalkSpeed, priorWalkSpeed])\nrows.push([\"🚴\", currentBikeSpeed, priorBikeSpeed])\nrows.push([\"\",\"\",\"\"])\nrows.push([\"Workouts\", currentCount, priorCount])\nrows.push([\"🚶\", currentWalkCount, priorWalkCount])\nrows.push([\"🚴\", currentBikeCount, priorBikeCount])\n\n/* Create the table header */\n\nconst tableHeader = [\"\",\"Current\",\"Prior\"]\n\n/* Display the table */\n\ndv.header(3, \"7 Day Stats\")\ndv.table(tableHeader, rows)",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Summarizing Information with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-files-dates/index.html",
    "href": "posts/obsidian/dataview/dataviewjs-files-dates/index.html",
    "title": "DataviewJS: A Gentle Introduction Part 2",
    "section": "",
    "text": "This article follows an earlier article introducing DataviewJS, which I strongly encourage you to read first. In that article, we learned how to display basic lists with Dataview. In this article, I will cover displaying lists of files in your vault and information in those files. I will also discuss using date ranges in queries and sorting results. Finally I will show how to make a table with multiple fields.\nAs a refresher, to show a list with a header you typed something like\nconst myList = [\n    \"First\",\n    \"Second\",\n    \"Third\"\n]\n\ndv.header(3, \"My List\")\ndv.list(myList)\n.\nThis creates a variable with a camelCase name. The variable contains a list, surrounded by square brackets, separated by commas, with quotations around strings (text). That was easy.\nNow what if I want to list all the files in my vault? I won’t do this, because the list would be crazy long, but just say I did. I would use\nconst fileNameList = dv.pages().file.name\n\ndv.header(3, \"Log files\")\ndv.list(fileNameList)\nThis would give me a bullet-point list of the names of all of the files in my vault. If you don’t have too many files in your vault, go ahead and try. Before narrowing it down to something more specific, let’s look at what is new here. What is pages().file.name? And for that matter, what’s with all the periods?\nLet me explain the periods first. I said that dv is an object. Objects can contain functions, like list(). They can also contain values, often called attributes. The functions and attributes of any object are referred to or accessed by using the name of the object, followed by a period, and then the name of the function or attribute. So the phrase above means: dv is an object that contains a pages() function (the parentheses let you know it is a function). The pages() function selects certain pages and provides attributes for each page, such as the name, links, date created, date modified, etc., in an object called file. In this case I’m choosing to show just the file name. If I wanted links instead, I would do dv.pages().file.link.\nThis dot-notation, using periods, is actually quite nice when you get used to it. Recall that “white space”, including line breaks, is not important in JavaScript, so you will see me often using line breaks before these periods, purely for readability and ease of editing.\nSo let’s see how to narrow things down a little. For the following examples I will use my exercise logs. All the log files are linked to a file called Logs. I can select files that link to this Logs file by writing pages(\"[[Logs]]\"). Note the quotations AND double square brackets. So I can write:\nconst fileNameList = dv.pages(\"[[Logs]]\").file.link\n\ndv.header(3, \"Log files\")\ndv.list(fileNameList)\nI suggest you create some log files in your vault so that you can follow along. Each file should contain, at least, the following fields, some of which we won’t use until we look at tables:\ntopic:: [[Logs]]\ntype:: log\nActivity:: 🚶\nDuration:: 60\nDistance:: 3\nActivityDate:: 2023-10-05\nI use 🚴 as the other activity type. After creating some log files, and the Logs file itself (this file can be empty for now), go ahead and try the JavaScript. You can use this technique to retrieve files on any topic in this way, assuming you have a topic field with the link (really you only need there to be a link).\n\n\n\nLet’s look at some ways we can modify this snippet to make it more useful. First, if you have tried this, and have created log files with various dates, you may notice that they aren’t necessarily displayed in any particular order. I want to make sure they are ordered by the activity date. The pages() object contains a number of functions allowing us to work with data, and sort() is one. sort() takes two arguments, first the name of the field to sort on, and a second argument specifying the sort order, either \"asc\" or \"desc\", for ascending or descending order, respectively.\nUnfortunately, we can’t just add .sort(ActivityDate, \"desc\"). When working with pages() we need to use a special JavaScript syntax. We need to type .sort(a =&gt; a.ActivityDate, \"desc\"). The letter a is arbitrary. I could also write .sort(foo =&gt; foo.ActivityDate, \"desc\") to achieve the same thing. a, or foo, basically serves as temporary variable name for the object you are working with, in this case the page, or note. Then the command says to use the ActivityDate property on each page for sorting. I know it looks a little confusing at first, but you will see this idiom so frequently you will quickly get used to it.\nSo lets sort our logs by date as so:\nconst fileNameList = dv.pages(\"[[Logs]]\")                                                                                                                \n    .sort(b =&gt; b.ActivityDate, \"desc\")                                                                                                                   \n    .ActivityDate\n\ndv.header(3, \"Activity Dates\")\ndv.list(fileNameList)\nNotice how the dot notation allows us to easily add more conditions to our query. ## Selecting by date\nNext, let’s see how we can select logs only for certain dates. To do that, we use the .where() function supplied by pages(). To see only logs since October 3, 2023, I can write\nconst fileNameList = dv.pages(\"[[Logs]]\")\n  .where(b =&gt; b.ActivityDate &gt;= dv.date(\"2023-10-03\"))\n    .sort(b =&gt; b.ActivityDate, \"desc\")\n    .ActivityDate                \ndv.header(3, \"Activity Dates since October 3, 2023\")\ndv.list(fileNameList)\nThe syntax for where() is similar to the syntax for sort(). There is an important difference though, because here we are comparing two values. &gt;= means “greater than or equal to”, &lt;= would be “less than or equal to”, etc. This evaluates to either true or false, since the date is either earlier or later than 10/3. This type of true/false value has a special name, boolean, which is good to know. dv provides its own .date() function for simple date manipulation, and I use it here.\nNow that you know how to use where(), let’s do one more thing with it. The code currently takes advantage of the fact that all the logs are linked to the Logs note. I don’t like that, because I might create some logs with a different topic. You may have noticed that all of the log notes have a type:: log. This can be used with .where(), so I can instead write:\nconst fileNameList = dv.pages()\n .where(b =&gt; b.type == \"log\")\n .where(b =&gt; b.ActivityDate &gt;= dv.date(\"2023-10-03\"))\n .sort(b =&gt; b.ActivityDate, \"desc\")\n .ActivityDate\n                                                             \ndv.header(3, \"Activity Dates since October 3, 2023\")\ndv.list(fileNameList)\nMany of my queries are based on the type of note, and this is how the note type can be used. Pay attention to the fact that “equal to” requires two equal signs, not one. For completeness, “not equal to” is written !=.\n\n\n\nLists aren’t very interesting, though. Usually you want more than just the name, or one bit of information from a file. So let’s make a table instead with dv.table(). For tables, you must supply an additional list with the column headers. Then you specify the fields that you want using a JavaScript function called map(). It works pretty much the same way as .sort and .where(), it just has a seemingly odd name. map() is used to go through each item in a list and do something with it, in this case, select certain fields from each note. This is called iteration.\nThis table shows the type of activity, distance in miles, duration in minutes and date. I’ve used emojis for column headings. You can see that the fields I want shown are supplied as a list (between square brackets) of the fields I want in the table.\nconst headers = [\"🚶/🚴\", \"🗒️\", \"⏱️\", \"📅\"]\n\nconst pages = dv.pages() \n    .where(b =&gt; b.type == \"log\")\n    .where(b =&gt; b.ActivityDate &gt;= dv.date(\"2023-10-03\"))\n    .sort(b =&gt; b.ActivityDate, \"desc\")\n    .map(b =&gt; [\n        b.Activity, \n        b.Distance, \n        b.Duration, \n        b.ActivityDate\n    ])\n                                \n\ndv.header(3, \"Activity since October 3, 2023\")\ndv.table(headers,   pages)\n\n\n\n\nIn the next article I will look at how to make this table dynamic, allowing you to change values directly in the table. I will also talk about relative dates, eg. activity over the last 10 days, or in the 10 days prior to that, or before a certain day but after another day.\nBefore moving on, I strongly encourage you to work with the material covered so far by adding some metadata to your existing notes and creating some queries. I don’t use tags (for now), but if you do, you could do pages(\"#some/tag\") instead of pages(\"[[Logs]]\"). Maybe you have a tag, #fleeting, for example. You could make a list of all your fleeting notes with pages(#fleeting).\nFinally, if you have been reading my Freeing Your Thinking series you may already be doing these examples on a canvas instead of in a note. If you aren’t I encourage you to do so. You can put the code directly into cards on a canvas. Then you can easily see all of your tables in one place, and convert them into notes if you so desire (but why?). Since the end goal will be to have dashboards to view and manage things, you might as well start now. If you have multiple tables on the same canvas, you might want to increase the Refresh Rate in the Dataview settings to prevent the canvas from being too “jumpy” while you type. The default is 2500ms (2.5 seconds). I find that 5 seconds (5000ms) works pretty well for me and the speed I type.\nHappy coding!",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction Part 2"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-files-dates/index.html#selecting-files",
    "href": "posts/obsidian/dataview/dataviewjs-files-dates/index.html#selecting-files",
    "title": "DataviewJS: A Gentle Introduction Part 2",
    "section": "",
    "text": "This article follows an earlier article introducing DataviewJS, which I strongly encourage you to read first. In that article, we learned how to display basic lists with Dataview. In this article, I will cover displaying lists of files in your vault and information in those files. I will also discuss using date ranges in queries and sorting results. Finally I will show how to make a table with multiple fields.\nAs a refresher, to show a list with a header you typed something like\nconst myList = [\n    \"First\",\n    \"Second\",\n    \"Third\"\n]\n\ndv.header(3, \"My List\")\ndv.list(myList)\n.\nThis creates a variable with a camelCase name. The variable contains a list, surrounded by square brackets, separated by commas, with quotations around strings (text). That was easy.\nNow what if I want to list all the files in my vault? I won’t do this, because the list would be crazy long, but just say I did. I would use\nconst fileNameList = dv.pages().file.name\n\ndv.header(3, \"Log files\")\ndv.list(fileNameList)\nThis would give me a bullet-point list of the names of all of the files in my vault. If you don’t have too many files in your vault, go ahead and try. Before narrowing it down to something more specific, let’s look at what is new here. What is pages().file.name? And for that matter, what’s with all the periods?\nLet me explain the periods first. I said that dv is an object. Objects can contain functions, like list(). They can also contain values, often called attributes. The functions and attributes of any object are referred to or accessed by using the name of the object, followed by a period, and then the name of the function or attribute. So the phrase above means: dv is an object that contains a pages() function (the parentheses let you know it is a function). The pages() function selects certain pages and provides attributes for each page, such as the name, links, date created, date modified, etc., in an object called file. In this case I’m choosing to show just the file name. If I wanted links instead, I would do dv.pages().file.link.\nThis dot-notation, using periods, is actually quite nice when you get used to it. Recall that “white space”, including line breaks, is not important in JavaScript, so you will see me often using line breaks before these periods, purely for readability and ease of editing.\nSo let’s see how to narrow things down a little. For the following examples I will use my exercise logs. All the log files are linked to a file called Logs. I can select files that link to this Logs file by writing pages(\"[[Logs]]\"). Note the quotations AND double square brackets. So I can write:\nconst fileNameList = dv.pages(\"[[Logs]]\").file.link\n\ndv.header(3, \"Log files\")\ndv.list(fileNameList)\nI suggest you create some log files in your vault so that you can follow along. Each file should contain, at least, the following fields, some of which we won’t use until we look at tables:\ntopic:: [[Logs]]\ntype:: log\nActivity:: 🚶\nDuration:: 60\nDistance:: 3\nActivityDate:: 2023-10-05\nI use 🚴 as the other activity type. After creating some log files, and the Logs file itself (this file can be empty for now), go ahead and try the JavaScript. You can use this technique to retrieve files on any topic in this way, assuming you have a topic field with the link (really you only need there to be a link).",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction Part 2"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-files-dates/index.html#sorting",
    "href": "posts/obsidian/dataview/dataviewjs-files-dates/index.html#sorting",
    "title": "DataviewJS: A Gentle Introduction Part 2",
    "section": "",
    "text": "Let’s look at some ways we can modify this snippet to make it more useful. First, if you have tried this, and have created log files with various dates, you may notice that they aren’t necessarily displayed in any particular order. I want to make sure they are ordered by the activity date. The pages() object contains a number of functions allowing us to work with data, and sort() is one. sort() takes two arguments, first the name of the field to sort on, and a second argument specifying the sort order, either \"asc\" or \"desc\", for ascending or descending order, respectively.\nUnfortunately, we can’t just add .sort(ActivityDate, \"desc\"). When working with pages() we need to use a special JavaScript syntax. We need to type .sort(a =&gt; a.ActivityDate, \"desc\"). The letter a is arbitrary. I could also write .sort(foo =&gt; foo.ActivityDate, \"desc\") to achieve the same thing. a, or foo, basically serves as temporary variable name for the object you are working with, in this case the page, or note. Then the command says to use the ActivityDate property on each page for sorting. I know it looks a little confusing at first, but you will see this idiom so frequently you will quickly get used to it.\nSo lets sort our logs by date as so:\nconst fileNameList = dv.pages(\"[[Logs]]\")                                                                                                                \n    .sort(b =&gt; b.ActivityDate, \"desc\")                                                                                                                   \n    .ActivityDate\n\ndv.header(3, \"Activity Dates\")\ndv.list(fileNameList)\nNotice how the dot notation allows us to easily add more conditions to our query. ## Selecting by date\nNext, let’s see how we can select logs only for certain dates. To do that, we use the .where() function supplied by pages(). To see only logs since October 3, 2023, I can write\nconst fileNameList = dv.pages(\"[[Logs]]\")\n  .where(b =&gt; b.ActivityDate &gt;= dv.date(\"2023-10-03\"))\n    .sort(b =&gt; b.ActivityDate, \"desc\")\n    .ActivityDate                \ndv.header(3, \"Activity Dates since October 3, 2023\")\ndv.list(fileNameList)\nThe syntax for where() is similar to the syntax for sort(). There is an important difference though, because here we are comparing two values. &gt;= means “greater than or equal to”, &lt;= would be “less than or equal to”, etc. This evaluates to either true or false, since the date is either earlier or later than 10/3. This type of true/false value has a special name, boolean, which is good to know. dv provides its own .date() function for simple date manipulation, and I use it here.\nNow that you know how to use where(), let’s do one more thing with it. The code currently takes advantage of the fact that all the logs are linked to the Logs note. I don’t like that, because I might create some logs with a different topic. You may have noticed that all of the log notes have a type:: log. This can be used with .where(), so I can instead write:\nconst fileNameList = dv.pages()\n .where(b =&gt; b.type == \"log\")\n .where(b =&gt; b.ActivityDate &gt;= dv.date(\"2023-10-03\"))\n .sort(b =&gt; b.ActivityDate, \"desc\")\n .ActivityDate\n                                                             \ndv.header(3, \"Activity Dates since October 3, 2023\")\ndv.list(fileNameList)\nMany of my queries are based on the type of note, and this is how the note type can be used. Pay attention to the fact that “equal to” requires two equal signs, not one. For completeness, “not equal to” is written !=.",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction Part 2"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-files-dates/index.html#a-first-table",
    "href": "posts/obsidian/dataview/dataviewjs-files-dates/index.html#a-first-table",
    "title": "DataviewJS: A Gentle Introduction Part 2",
    "section": "",
    "text": "Lists aren’t very interesting, though. Usually you want more than just the name, or one bit of information from a file. So let’s make a table instead with dv.table(). For tables, you must supply an additional list with the column headers. Then you specify the fields that you want using a JavaScript function called map(). It works pretty much the same way as .sort and .where(), it just has a seemingly odd name. map() is used to go through each item in a list and do something with it, in this case, select certain fields from each note. This is called iteration.\nThis table shows the type of activity, distance in miles, duration in minutes and date. I’ve used emojis for column headings. You can see that the fields I want shown are supplied as a list (between square brackets) of the fields I want in the table.\nconst headers = [\"🚶/🚴\", \"🗒️\", \"⏱️\", \"📅\"]\n\nconst pages = dv.pages() \n    .where(b =&gt; b.type == \"log\")\n    .where(b =&gt; b.ActivityDate &gt;= dv.date(\"2023-10-03\"))\n    .sort(b =&gt; b.ActivityDate, \"desc\")\n    .map(b =&gt; [\n        b.Activity, \n        b.Distance, \n        b.Duration, \n        b.ActivityDate\n    ])\n                                \n\ndv.header(3, \"Activity since October 3, 2023\")\ndv.table(headers,   pages)",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction Part 2"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-files-dates/index.html#next-steps",
    "href": "posts/obsidian/dataview/dataviewjs-files-dates/index.html#next-steps",
    "title": "DataviewJS: A Gentle Introduction Part 2",
    "section": "",
    "text": "In the next article I will look at how to make this table dynamic, allowing you to change values directly in the table. I will also talk about relative dates, eg. activity over the last 10 days, or in the 10 days prior to that, or before a certain day but after another day.\nBefore moving on, I strongly encourage you to work with the material covered so far by adding some metadata to your existing notes and creating some queries. I don’t use tags (for now), but if you do, you could do pages(\"#some/tag\") instead of pages(\"[[Logs]]\"). Maybe you have a tag, #fleeting, for example. You could make a list of all your fleeting notes with pages(#fleeting).\nFinally, if you have been reading my Freeing Your Thinking series you may already be doing these examples on a canvas instead of in a note. If you aren’t I encourage you to do so. You can put the code directly into cards on a canvas. Then you can easily see all of your tables in one place, and convert them into notes if you so desire (but why?). Since the end goal will be to have dashboards to view and manage things, you might as well start now. If you have multiple tables on the same canvas, you might want to increase the Refresh Rate in the Dataview settings to prevent the canvas from being too “jumpy” while you type. The default is 2500ms (2.5 seconds). I find that 5 seconds (5000ms) works pretty well for me and the speed I type.\nHappy coding!",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction Part 2"
    ]
  },
  {
    "objectID": "posts/obsidian/optimal-notes-with-obsidian/index.html",
    "href": "posts/obsidian/optimal-notes-with-obsidian/index.html",
    "title": "Optimal Notes with Obsidian",
    "section": "",
    "text": "…a tale in the spirit of Euripides",
    "crumbs": [
      "Obsidian",
      "Optimal Notes with Obsidian"
    ]
  },
  {
    "objectID": "posts/obsidian/optimal-notes-with-obsidian/index.html#fast-powerful-search",
    "href": "posts/obsidian/optimal-notes-with-obsidian/index.html#fast-powerful-search",
    "title": "Optimal Notes with Obsidian",
    "section": "Fast, Powerful Search",
    "text": "Fast, Powerful Search\nThis is where Obsidian directly addresses the Tragedy of Notes. Literally every word in every note can be immediately retrieved thanks to a blazingly fast indexing system. Start typing a word or phrase and you will see all the notes that contain those words or phrases along with their context. With a little practice you can make sophisticated searches involving multiple conditions. Usually just a few words will find the information you are looking for.\nSo in the scenario above I could have found the answer to my question in seconds.",
    "crumbs": [
      "Obsidian",
      "Optimal Notes with Obsidian"
    ]
  },
  {
    "objectID": "posts/obsidian/optimal-notes-with-obsidian/index.html#structure-emerges-naturally",
    "href": "posts/obsidian/optimal-notes-with-obsidian/index.html#structure-emerges-naturally",
    "title": "Optimal Notes with Obsidian",
    "section": "Structure Emerges Naturally",
    "text": "Structure Emerges Naturally\nWhen think of keeping notes beyond a bunch of Word documents we naturally think of spreadsheets or databases. The structure of our data can be arbitrary in the case of spreadsheets or must be pre-defined for databases. Changing the structure later involves a lot of effort. But for quick retrieval of information you need a database, right?\nWell remember we already dealt with that above. (We don’t need no stinkin’ database.) What we have instead in Obsidian are “backlinks”, which are references from one note to another. This gives us the opportunity for a structure which evolves “organically”. As you make notes and develop them you will begin to connect ideas in one note with another note. A quick search can find all occurences of similar ideas, and relevant notes can be linked to. This cross-linking of information is the neural network, the second brain referred to above.",
    "crumbs": [
      "Obsidian",
      "Optimal Notes with Obsidian"
    ]
  },
  {
    "objectID": "posts/obsidian/optimal-notes-with-obsidian/index.html#information-about-information",
    "href": "posts/obsidian/optimal-notes-with-obsidian/index.html#information-about-information",
    "title": "Optimal Notes with Obsidian",
    "section": "Information About Information",
    "text": "Information About Information\nMetadata has entered the awareness of most people these days through controversies around the Government’s access to phone records. In these cases, the FBI doesn’t (usually) have access to the actual data, the conversations. What they do have is the metadata. This information includes dates, times, who called whom, where the people were, how long the conversation lasted, etc. Investigators say that they can often learn as much from the metadata than from the actual content.\nObsidian allows for the flexible and customizable application of metadata to notes, and this metadata can serve many puposes. For example, if I were writing an article which incorporated a series of observations taken over time (in seperate notes), using metadata I can easily pull the information I need into my article. I could use metadata to easily create a list of references for my article. I could use metadata to track the status of my project (draft, in review, final, published).",
    "crumbs": [
      "Obsidian",
      "Optimal Notes with Obsidian"
    ]
  },
  {
    "objectID": "posts/obsidian/optimal-notes-with-obsidian/index.html#creativity-beyond-information-retrieval",
    "href": "posts/obsidian/optimal-notes-with-obsidian/index.html#creativity-beyond-information-retrieval",
    "title": "Optimal Notes with Obsidian",
    "section": "Creativity: Beyond Information Retrieval",
    "text": "Creativity: Beyond Information Retrieval\nSo far I’ve really focused on information retrieval. And I’ve given a practical example of the usefulness of that quick retrieval. But there is another great value of Obsidian, which is to facilitate the creation of new works.\nObsidian is ideal for a note-taking approach called Zettelkasten, which is German for a box of index cards. In a zettelkasten system notes are “atomic”, meaning they are independent, self-contained pieces of information which are then coded and correlated for easy retrieval. This is how Carl Linnaeus was able to create the biological classification system we still use today. Another oft-cited example is the sociologist Niklas Luhmann who produced around 70 books and 400 academic papers. He apparently had 90,000 notes in his Zettelkasten.\nThis approach to note-taking is widely practiced in Europe but not so in the USA. Obsidian facilitates all kinds of writing, not just academic writing. The recently deceased Joan Rivers, a comedian, author and talk show host, had 60,000 notes. Other famous comediens like Bob Hope and George Carlin did the same.\nZettelkasten through the centuries\n\n\n\n17th Century\n\n\n\n\n20th century\n\n\n\n\n\n21st century",
    "crumbs": [
      "Obsidian",
      "Optimal Notes with Obsidian"
    ]
  },
  {
    "objectID": "posts/obsidian/optimal-notes-with-obsidian/index.html#freedom-and-privacy",
    "href": "posts/obsidian/optimal-notes-with-obsidian/index.html#freedom-and-privacy",
    "title": "Optimal Notes with Obsidian",
    "section": "Freedom and Privacy",
    "text": "Freedom and Privacy\nFinally I’ll mention one more aspect of Obsidian. All of your notes are just text files stored on your own computer. They don’t get transmitted or stored anywhere else (unless you want to). Since they are text files they can be opened and read by any editor like Notepad, LibreOffice or Word. You back up your notes in the same way you back up everything else on your system (which you do, right?). Noone is tracking your notes or any other information about you.\nObsidian is about Personal Information. It’s your brain!",
    "crumbs": [
      "Obsidian",
      "Optimal Notes with Obsidian"
    ]
  },
  {
    "objectID": "posts/linux/nixos-linux-different/index.html",
    "href": "posts/linux/nixos-linux-different/index.html",
    "title": "NixOS: the Linux different",
    "section": "",
    "text": "NixOS is a Linux distribution with a radically different design from any other. The NixOS approach solves many problems in the design of other Linux systems. However, since the NixOS paradigm relies on ideas foreign to many people such as graphs and functional programming, it carries a particularly steep learning curve. Switching from Ubuntu to Fedora is painless. Switching to NixOS is not.\nI feel strongly that it is worth the learning curve, which in my experience isn’t really so bad. You can really use NixOS without understanding immutability, graphs or functional programming. But, to understand why NixOS is a cut above all the rest, some understanding of the design is helpful.\nHere is a first attempt at a general description of NixOS and why it is so different.",
    "crumbs": [
      "Linux",
      "NixOS: the Linux different"
    ]
  },
  {
    "objectID": "posts/linux/nixos-linux-different/index.html#how-different-really-different.",
    "href": "posts/linux/nixos-linux-different/index.html#how-different-really-different.",
    "title": "NixOS: the Linux different",
    "section": "How different? Really different.",
    "text": "How different? Really different.\nAfter you’ve been working with with Linux long enough, you realize that all distributions are pretty much the same. Different but similar package managers and different but similar package repositories, and different default desktop environments. Some kernel tweaks as well perhaps, but given a command line it is almost trivial to move from one distribution to another.\nNot so with NixOS. Want to see what software is installed? You won’t find anything in /usr/bin, or any other typical bin or opt directory. Want to add an alias to /etc/hosts? That file, indeed much in /etc, turns out to be read-only, even to root. Add a user with useradd? Nope. Install some software in a gui “App Store”, or even with a command like apt install? These don’t exist. In fact, many of your basic system administration skills seem useless with NixOS not adhering to FHS (file heirarchy standard) or having the ability to edit configuration files. But, don’t be sad, your Linux skills are still important, because…",
    "crumbs": [
      "Linux",
      "NixOS: the Linux different"
    ]
  },
  {
    "objectID": "posts/linux/nixos-linux-different/index.html#its-still-linux",
    "href": "posts/linux/nixos-linux-different/index.html#its-still-linux",
    "title": "NixOS: the Linux different",
    "section": "It’s still Linux",
    "text": "It’s still Linux\nNixOS is, never-the-less, Linux just as much as is Fedora or Debian. It runs a Linux kernel with all the GNU api tools (or their Rust alternatives). It has a large package repository, the largest of any, in fact. It even uses SystemD 😞. You can add any software that runs on Linux, and to an ordinary “end user”, the daily experience is the same as on any other distro. They might remark how stable and snappy it is, but their day to day workflow would not be different.\nUnder the hood, though, everything is different on a fundamental level. Given the learning curve, which make is seem like a completely new operating system rather than just another Linux distribution, the obvious question is: why bother? There are many reasons, but one is that, once the Nix way is grokked, all of these admin tasks I mentioned become much simpler and straight-forward, more robust, easier to rollback, and fully reproducible. I guess that’s more than one reason.",
    "crumbs": [
      "Linux",
      "NixOS: the Linux different"
    ]
  },
  {
    "objectID": "posts/linux/nixos-linux-different/index.html#getting-a-little-personal",
    "href": "posts/linux/nixos-linux-different/index.html#getting-a-little-personal",
    "title": "NixOS: the Linux different",
    "section": "Getting a little personal",
    "text": "Getting a little personal\nFor me, NixOS is a sort of the culmination of why I turned to Linux decades ago. When Windows first came out, compared to DOS, it was cool and all, but I felt like a veil had been drawn over the OS, which then became hidden and mysterious. Apple computers seemed even worse in this aspect. For some types of work, the graphical user interface was a great step forward. But for other things, the gui just obscured the underlying workings of the computer and made anything outside of typical usage more difficult. On top of that was the irritating instability. With every system crash, I wanted to understand what was really going on so I could fix it.\nThen I discovered Linux, and more specifically Slackware, which uses a terminal and vi (elvis at the time) for all administrative tasks. The ability to understand exactly what my computer was doing, to configure it with text files, to install, and if necessary, build, software from the command line, was just what I was looking for. I even compiled my own kernels back then. When I started using other distributions, I never took more than a quick glance at their graphical admin tools like app stores and such. Give me the command line any day, and every distribution is basically the same.",
    "crumbs": [
      "Linux",
      "NixOS: the Linux different"
    ]
  },
  {
    "objectID": "posts/linux/nixos-linux-different/index.html#aging-systems-and-other-problems",
    "href": "posts/linux/nixos-linux-different/index.html#aging-systems-and-other-problems",
    "title": "NixOS: the Linux different",
    "section": "Aging systems and other problems",
    "text": "Aging systems and other problems\nAs experienced Linux users know, there are many issues which arise over time on any Linux system, such as software upgrades which create library version conflicts, problems which are made worse on shared systems. Cruft accumulates as uninstalls are rarely clean, especially with regard to program data files and directories. If you install and uninstall many programs, after a certain amount of time, it is difficult to be sure exactly what lingers in /etc, /usr/share, ~/.local, or anywhere else, for that matter. After a year of installing, uninstalling and configuring software, how can you be sure what is actually there? And when your new computer arrives, or you finally decide to do a clean install instead of a version upgrade, how much effort will it take to set it up exactly how you like it?\nEnter NixOS, where I can declare the entire operating system with text files! I can write down what I want, and know that that will be all that there is on my system, despite what may have been there before. I can, for example, simply grep my configs to see everything installed and all configurations, without needing a special package manager tool. In fact, with NixOS, nothing is every really installed, upgraded, or uninstalled, not in the traditional sense anyway. Any of these activities generates a brand, spankin’ new system image which entirely replaces the old one.",
    "crumbs": [
      "Linux",
      "NixOS: the Linux different"
    ]
  },
  {
    "objectID": "posts/linux/nixos-linux-different/index.html#embrace-the-difference",
    "href": "posts/linux/nixos-linux-different/index.html#embrace-the-difference",
    "title": "NixOS: the Linux different",
    "section": "Embrace the difference",
    "text": "Embrace the difference\nHere be dragons, but they are auspicious Chinese dragons, not scary European ones. The singular power of NixOS lies in the differences, so they are worth exploring. They involve ideas such as declarative, functional programming vs imperative, procedural programming, graphs vs stacks, immutable systems, atomic upgrades and ephemeral packages. Also, Nix/NixOS isn’t just one thing, it is a programming language (Nix), package manager (Nix), repository (nixpkgs), and operating system (NixOS).\nExploring all these topics would be overwhelming in one article, so it is my intention to write an occasional series on each. None of these will be tutorials, rather they are intended as an introduction to the ideas useful to understanding NixOS. As a practical matter, Nix the package manager and language can be installed on any computer and be experimented with, or even used as a primary package manager for your current system. NixOS also runs nicely in a virtual machine. To learn NixOS you’ll need to get your hands dirty, and for me at least, getting used to NixOS and configuring my system from the comfort of Fedora made the eventual switch to metal completely painless, as I just needed to copy the configs I had created to the new partition and build it, not repeat my work.\nNixOS is not for everyone. The imperative paradigm has worked reasonably well for decades, and has the virtue of familiarity, so there is no strong motivation to change for most users. On the other hand, if, like me, you are a programmer or data scientist, for example, and you prize stability, clarity, and simplicity, if you find change and learning new ways to be stimulating, and if you know that the most interesting things in life are on the roads less traveled, you will be delighted by NixOS.\nTo get started, here is an article on the File Heirarchy Standard, which NixOS abandoned for a graph structure. In so doing, Nix sidesteps the need for containers such as Docker, Distrobox or Flatpaks.",
    "crumbs": [
      "Linux",
      "NixOS: the Linux different"
    ]
  },
  {
    "objectID": "posts/linux/nix-docker-fhs/index.html",
    "href": "posts/linux/nix-docker-fhs/index.html",
    "title": "The FHS Problem",
    "section": "",
    "text": "I have been using NixOS for a year and a half. It is conceptually different from other Linuxes, and is notoriously difficult to explain. Unfortunately, the strange concepts and ugly configuration language mask an underlying simplicity which I think is superior to the traditional approach. I will try, in this article, to explain a few of these concepts, and how they relate to a development problem caused by the fundamental design of traditional Linux.\n\nThe problem with the FHS\nThe Filesystem Hierarchy Standard seems to be a basic part of Linux. Indeed, it is one of the first things everyone learns about when starting to use Linux, though they are usually unaware that it is the FHS. The FHS simply defines where different types of files should be located: executable files in /bin or /usr/bin, libraries in /lib, configurations in /etc, and so on. Most people assume this to be a fundamental aspect of Linux, but it is not.\nOn a Linux system, everything is a file, and files can be located anywhere, in any directory. As long as you know where your executables are, and they know where their libraries are, everything can be in one directory, or split across multiple paths. Linux doesn’t care.\nOn the other hand, in an ecosystem in which code is widely shared among projects, the arbitrary placement of various components made life difficult, so, early on in the life of Linux, a group of people proposed a standard (firstly the FSSTND, followed by FHS) which defined locations for the various types of files, a proposal which was quickly adopted by just about everybody.\nThe problem with this approach is its hierarchical tree structure. It would appear an obvious way to organize files, but it is by nature limiting. in modern programming, multiple versions of programming languages and libraries are necessary, not all of which are compatible. With only one /lib directory, only one /usr/bin directory, etc. how do you manage multiple versions of all the packages? A program might be dependent on a specific version of Python, while another program might require another. In development, this is particularly problematic, as version bumps can break programs, so version consistency is crucial. But on any given Linux system, ensuring such consistency is effectively impossible.\nThere is an entirely different approach, an organizational structure unfamiliar to many: a graph.\n\n\nThe Docker solution: Containers\nThe most common solution to this problem is Docker containers. A variation on virtual machines, containers contain full Linux installations which are isolated from the system on which they run, with their own root file structure and all libraries. In order to be used, these containers need to be loaded as processes managed by another process.\nThis is a simple idea but a complex solution: just have multiple Linuxes all happily doing their own thing on the same piece of hardware. However, while this does solve the initial problem, and has become nearly a de facto standard in software development, this approach has definite drawbacks: it is not reproducible, there is no guarantee of library versions, most of the container’s contents are unneeded bloat, and it requires a separate mechanism to be run and maintained.\n\n\nThe Nix solution: Graphs\nInstead of working around the FHS, Nix chose to abandon it. The FHS, and Docker, are layered, imperative approaches to system structural organization. A Dockerfile is a series of commands which result in the container environment, just as a typical computer system is installed through a series of commands, layering one package on another, highly dependent on file paths. This isn’t the only way, however.\nNix is not imperative, nor is it layered. It is declarative, and uses graphs to install software. On a Nix system, the desired environment is described in a configuration file (actually itself a functional program). When an application is installed, all necessary files are placed in a common directory, and a graph is used to connect all of the relevant bits. Every file in this directory has a name which includes a hash of the program or library itself, uniquely identifying each package/version.\nPeople are comfortable with hierarchical structures. The are used in many areas of life, and can be easily visualized as an upside-down tree. Graphs, on the other hand, are not familiar. A graph consists of objects and lines connecting them. Visually, graphs can appear chaotic, given the arbitrary locations of the objects. However, when using visualization tools which draw together linked objects (force-directed graphs), structure emerges.\nFurthermore, with the Nix approach, environments are fully reproducible, unlike with Docker and it is efficient, as only required libraries are installed, not entire OSs, and a seperate sub-system need not be running to manage anything.\nIn practice, using Nix makes life simpler, and development more secure and consistent. Since environments are declared, they are easily shared, and all versions of each file are locked in a file which is also shared. In order to enter any project environment, all you need do is enter the directory, and all tools and libraries declared are just there, without the need to start a separate process and enter the container. To top it all off, should you need an OCI container for use with Kubernetes or, if absolutely necessary, Docker itself, creating one from a Nix definition is trivial.\n\n\nReflection\nFrom a technical perspective, it seems clear that the Nix approach is simpler and more efficient than the container approach. On the other hand, it relies on concepts which are unfamiliar to most programmers and system administration. Graphs, and especially functional programming, are not widely understood. People like to stay in their comfort zone, and programmers are surprisingly resistant to learning new things. The controversy around Rust and the Linux kernel is a good example of this. Rust can make the kernel safer, and has already created far superior replacements for much of GNU, but many C programmers find Rust daunting.\nDocker itself has become a sprawling enterprise with a large variety of offerings, and an economy which has developed around it. Never-the-less, I think that the future of Linux, both as a development platform and an operating system, lies in Nix.\n\n\n\n\n Back to top",
    "crumbs": [
      "Linux",
      "The FHS Problem"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html",
    "title": "Speed, Simplification and Segments",
    "section": "",
    "text": "In the first article, we took raw data from a trekking application, a GPX file in the form of standardized XML, parsed it to CSV, which we imported into a DataFrame with Pandas. Each record contains latitude, longitude, elevation and a time stamp. Using GeoPandas, we turned the latitude and longitude fields into POINT geometries and assigned an appropriate Coordinate Reference System so that we can work with the data geospatially. In the second article, we used MovingPandas to convert those POINTs into LINESTRINGs, allowing for calculations of distances not only along the route but to other locations or routes. We also learned how to auto-detect places along the trajectory where we paused.\nIn this article I will show how to incorporate the time dimension and calculate speed, duration, and other statistics, and how to split the path into segments. We already did this implicitly last time when we did stop detection, since a pause is based on time, and those pauses are natural points to split the route, and, not surprisingly, this is quite simple to do in MovingPandas. But first, we should look at simplification of the data.\n\n\n\nThe raw data contains a level of granularity that is not necessarily needed, and can have a significant computational cost. The data can be simplified in different ways such as combining short time periods into longer ones, or smoothing by combining multiple paths within a certain area into a single, straight path. In this article, I’ll look at generalizing, down-sampling the time.\nGPS trackers take frequent readings, seconds apart. Let’s look at the average time between readings in one of our treks. I’ll start by importing a GeoDataFrame with walks and bike rides in New Mexico, using the method described in the prior articles.\ndf = pd.read_csv('data/b4/combined.csv')\ngdf = gpd.GeoDataFrame(\n    df, \n    geometry=gpd.points_from_xy(x=df.Lon, y=df.Lat), \n    crs=4269\n).to_crs(32111)\nidList = list(gdf.groupby(['Id']).nunique().reset_index().Id)\nfor i, track in enumerate(idList):\n    gdf.loc[gdf.Id == track, 'trajectory_id'] = i\ngdf['Time'] = pd.to_datetime(gdf.Time)\ntc = mpd.TrajectoryCollection(\n    gdf, 'trajectory_id', \n    t='Time', x='Lon', y='Lat'\n)\nprint(tc)\nTrajectoryCollection with 130 trajectories\nax = tc.plot(\n    column='trajectory_id',\n    figsize=(10,5), \n    cmap='inferno', \n    legend=True\n)\nax.set_title('Walks and Rides in NM')\nax.set_axis_off()\nctx.add_basemap(ax, crs=gdf.crs,\n                source=ctx.providers.Esri.NatGeoWorldMap);\n\nHere are the total number of records, and the average time between readings:\ndef print_intervals_size(tc):\n    print(f'''\n    Avg sampling interval: {(round((np.mean(\n        [t.get_sampling_interval().seconds for t in tc.trajectories]\n     )), 2))} seconds\n    Total records: {np.sum([t.size() for t in tc.trajectories])}\n     ''')\nprint_intervals_size(tc)\nAvg sampling interval: 5.92 seconds\nTotal records: 84319\nThat’s way more than I need. A reading every 30 seconds is more than enough. I can use the generalize() function from MovingPandas to easily do so using the tolerance parameter:\ntc_generalized = (\n    mpd.MinTimeDeltaGeneralizer(tc)\n        .generalize(tolerance=timedelta(seconds=30))\n)\nprint_intervals_size(tc_generalized)\nAvg sampling interval: 33.28 seconds\nTotal records: 17258\nI’ll extract a single trajectory from before and after so we can see what happens with simplification. First I’ll print some basic information about the trek. Moving pandas provides functions such as get_duration and get_length to obtain information. I’ll include the print code here to demonstrate how to access the values and what functions are available. To access columns in the underlying DataFrame, you can use tc.df.SomeColumn. Python’s f-strings make it easy to generate these reports. ```python def trek_info(traj): print(f’’’ Date: {traj.get_start_time().strftime(‘%x’)} Activity: {‘Bike Ride’ if traj.df.Sport[0] == 1 else ‘Walk’} Duration: {traj.get_duration().seconds/60:.0f} minutes Distance: {traj.get_length(units=‘mi’):.1f} miles Avg Speed: {((traj.get_length(units=‘mi’)) /\n(traj.get_duration().seconds/60**2)):.1f} mph Avg Temp: {(traj.df.Temp.mean()*9/5)+32:.0f} degrees F CRS: {traj.get_crs().to_epsg()} Points: {traj.size()} Bounds: ({“,”.join( map(str, [round(traj.to_crs(4269).get_bbox()[x], 3) for x in range(4)]))})\n    Max Elevation: {traj.get_max(column='Elev'):,} meters\n    Min Elevation: {traj.get_min(column='Elev'):,} meters\n    Avg Elevation: {traj.df.Elev.mean():,.0f} meters\n    \n            Time         Location (Lon/Lat)\n    Start: {\n      traj.get_start_time().strftime('%X')\n      }  {\n      tuple([round(x,3) for x in \n      traj.to_crs(4269).get_start_location().coords[0]])\n      } degrees\n    End:   {\n      traj.get_end_time().strftime('%X')\n      }  {\n      tuple([round(x,3) for x in\n      traj.to_crs(4269).get_end_location().coords[0]])\n      } degrees\n''')\n```python\ntraj_bike = tc.trajectories[52]\ntraj_bike_gen = tc_generalized.trajectories[52]\ntrek_info(traj_bike)\n         Date:      05/30/24\n        Activity:  Bike Ride\n        Duration:  63 minutes\n        Distance:  13.4 miles\n        Avg Speed: 12.8 mph\n        Avg Temp:  76 degrees F\n        CRS:       32111\n        Points:    681\n        Bounds:    (-106.714, 35.144, -106.648, 35.183)\n\n        Max Elevation: 1,564 meters\n        Min Elevation: 1,504 meters\n        Avg Elevation: 1,528 meters\n        \n                Time         Location (Lon/Lat)\n        Start: 18:41:12  (-106.708, 35.147) degrees\n        End:   19:44:04  (-106.708, 35.148) degrees\ntrek_info(traj_bike_gen)\n# Difference from `traj_bike`\n        Points:    105\nHere is a before and after plot:\nfrom itertools import chain\nf, (ax1, ax2) = plt.subplots(2, 2)\nfor i, traj in enumerate([traj_bike, traj_bike_gen]):\n    traj.plot(ax=ax1[i])\n    ctx.add_basemap(ax1[i], crs=traj.crs, \n                    source=ctx.providers.CartoDB.Voyager)\nfor i, traj in enumerate([traj_bike, traj_bike_gen]):\n    traj.plot(ax=ax2[i])\n    ctx.add_basemap(ax2[i], crs=traj.crs, \n                    source=ctx.providers.CartoDB.Voyager)\n    ax2[i].set_xlim(460000, 462000)\n    ax2[i].set_ylim(462000, 464000)\nfor axs in chain(ax1, ax2): \n    axs.set_axis_off() \nax1[0].set_title('Raw')\nax1[1].set_title('Generalized')\nf.suptitle('Effect of Generalization to 30 Seconds')\nf.tight_layout();\n\n\n\n\nBefore moving to speed calculations, let’s make a quick plot:\nt_plot(traj_bike_gen, column='speed');\n\nI just plotted the speed column, but a quick check will show there is no speed column.\ntraj_bike_gen.df.columns\nIndex(['Id', 'Name', 'Lat', 'Lon', 'Elev', 'Temp', 'Weather', 'Sport',\n       'Sport_s', 'trajectory_id', 'geometry'],\n      dtype='object')\nLike Dorothy in Oz, you had the speed all along. MovingPandas does the calculation in the background for you. If you want an actual speed column, that’s easy enough:\ntc_generalized.add_speed()\ntc_generalized.trajectories[52].df.columns\nIndex(['Id', 'Name', 'Lat', 'Lon', 'Elev', 'Temp', 'Weather', 'Sport',\n       'Sport_s', 'trajectory_id', 'geometry', 'mi/h', 'speed'],\n      dtype='object')\nSince my CRS is in meters, so is the speed. If I’d prefer miles per hour, I can specify the units:\ntc_generalized.add_speed(overwrite=True, name='mi/h', units=('mi', 'h'))\ntc_generalized.trajectories[52].df.iloc[5]\nId                                Workout-2024-05-30-18-41-12\nName                                              Albuquerque\nLat                                                 35.150019\nLon                                                 -106.7125\nElev                                                     1536\nTemp                                                     24.2\nWeather                                                     0\nSport                                                       1\nSport_s                                                  Bike\ntrajectory_id                                            52.0\ngeometry         POINT (457860.4830821714 460313.87076766626)\nmi/h                                                13.109903\nspeed                                                5.860651\nName: 2024-05-30 18:43:54.251000, dtype: object\nIn fact, we can automagically create distance, acceleration, direction, and time delta columns as well, all with arbitrary units. Here I’ll add a number of metrics in both metric and imperial units, by supplying the units and column names as parameters.\ntc_generalized.add_distance(overwrite=True, name=\"distance (km)\", units=\"km\")\ntc_generalized.add_distance(overwrite=True, name=\"distance (mi)\", units=\"mi\")\ntc_generalized.add_distance(overwrite=True, name=\"distance (meters)\", units=\"m\")\ntc_generalized.add_distance(overwrite=True, name=\"distance (feet)\", units=\"ft\")\ntc_generalized.add_timedelta(overwrite=True)\ntc_generalized.add_speed(overwrite=True, name=\"speed (ft/min)\", units=(\"ft\", \"min\"))\ntc_generalized.add_speed(overwrite=True, name=\"speed (km/hour)\", units=(\"km\", \"h\"))\ntc_generalized.add_speed(overwrite=True, name=\"speed (mi/hour)\", units=(\"mi\", \"h\"))\ntc_generalized.add_acceleration(\n    overwrite=True, name=\"acceleration (mph/s)\", units=(\"mi\", \"h\", \"s\")\n)\ntc_generalized.add_acceleration(\n    overwrite=True, name=\"acceleration (kmph/s)\", units=(\"km\", \"h\", \"s\")\n)\ntc_generalized.add_direction(overwrite=True)\ntc_generalized.add_angular_difference(overwrite=True)\n(tc_generalized.trajectories[52]\n .df.reset_index()\n .loc[5, 'speed':'angular_difference'])\nspeed                                  5.860651\nmi/h                                  13.109903\ndistance (km)                          0.175855\ndistance (mi)                          0.109271\ndistance (meters)                     175.85469\ndistance (feet)                      576.951082\ntimedelta                0 days 00:00:30.006000\nspeed (ft/min)                      1153.671431\nspeed (km/hour)                       21.098343\nspeed (mi/hour)                       13.109903\nacceleration (mph/s)                   0.104386\nacceleration (kmph/s)                  0.167992\ndirection                            349.062474\nangular_difference                    70.594237\nName: 5, dtype: object\nAnd now, I can generate summaries somewhat easier:\ntraj_bike_gen = tc_generalized.trajectories[52]\nprint(f'''\n    Total Distance: \n        {traj_bike_gen.df['distance (meters)']\n            .sum().item()/1000:&gt;4.1f} kilometers\n        {traj_bike_gen.df['distance (mi)']\n            .sum().item():&gt;4.1f} miles\n    Average Speed:\n        {traj_bike_gen.df['speed (km/hour)']\n            .mean().item():&gt;4.1f} kilometers per hour\n        {traj_bike_gen.df['speed (mi/hour)']\n            .mean().item():&gt;4.1f} miles per hour\n''')\n    Total Distance: \n        18.5 kilometers\n        11.5 miles\n    Average Speed:\n        19.2 kilometers per hour\n        11.9 miles per hour\nI’m going to also add an elevation delta. I’d like to plot the speed next to the change in elevation. So I’ll add the column.\ntraj_bike_gen.df['ElevD'] = (\n    [traj_bike_gen.df.iloc[i].Elev - traj_bike_gen.df.iloc[i-1].Elev \n     for i in range(len(traj_bike_gen.df))]\n)\nHere is a plot of speed next to the elevation changes.\nf, ax = plt.subplots(nrows=2, figsize=(10,7))\ntraj_bike_gen.plot(ax=ax[0], column='speed', \n                   cmap='inferno', legend=True, \n                  linewidth=4)\ntraj_bike_gen.plot(ax=ax[1], column='ElevD', \n                   cmap='inferno', legend=True, \n                  linewidth=4)\nfor i in range(2): \n    ctx.add_basemap(ax[i], crs=traj_bike_gen.crs, \n                   source=ctx.providers.Esri.NatGeoWorldMap)\n    ax[i].set_axis_off()\nax[0].set_title('Speed')\nax[1].set_title('Elevation Change')\nf.tight_layout();\n\n\n\n\nThe final topic I want to cover is route segmentation. This includes splitting the entire route by logical segments such as every 15 minutes or at stop points. It can also mean extracting a specific segment based on time or geographic location. MovingPandas has a variety of Splitter classes, depending on the purpose.\nTo demonstrate the split on stops, I’ll use the NYC walk from the last article, since I actually paused occasionally. In fact, the stop detector we used in that exercise could have given us the segments directly with it’s get_stop_segments() method, but here I’ll use the StopSplitter.\ndf_ny = gpd.read_file('data/ny_trek.gpkg')\ndf_ny['Sport'] = 17\nny_walk = (\n    mpd.TrajectoryCollection(\n    df_ny, 'trajectory_id', \n    t='Time', x='Lon', y='Lat'\n    ).trajectories[0])\n\nsplit_stop = (mpd.StopSplitter(ny_walk)\n              .split(\n                min_duration = timedelta(seconds=120), \n                max_diameter = 100))\nsplit_stop\nTrajectoryCollection with 18 trajectories\nf, ax = t_plot(ny_walk)\nfor i in range(1,18,2): \n    split_stop.trajectories[i].plot(ax=ax, color='b')\n\nI can use the TemporalSplitter to divide my trek into hour-long segments:\nsplit_time = (mpd.TemporalSplitter(ny_walk)\n             .split(mode='hour'))\n\nI can also extract a single segment based on an arbitrary time range using get_segment_between. I’ll create two datatime variables to see where I was between 4:00 and 5:00 local time.\nt1 = datetime(2024, 9, 11, 19) # UTC Time\nt2 = t1 + timedelta(minutes=60)\nsegment = ny_walk.get_segment_between(t1,t2)\nt_plot(segment);\n\nFinally, I can extract the portion of the path which lies in a specific geographical area by using clip and intersecting the path with a POLYGON or bounding box. Here is an example:\nxmin, xmax, ymin, ymax = 191000, 192500, 210000, 211000\npolygon = Polygon(\n    [(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin), (xmin, ymin)]\n)\nintersections = ny_walk.clip(polygon) \n\nf, ax = t_plot(ny_walk)\ngpd.GeoSeries(polygon).plot(ax=ax, color='lightblue', \n                           alpha=0.5)\nintersections.plot(ax=ax, color='r');\n\n\n\n\nI hope you had fun with these articles, and were able to harness your own GPS data to create some lovely maps of your excursions. If you subscribe to one of the apps like Strava or AllTrails you can get get their maps to underlay with your routes. MovingPandas has many more functions such as the Smoother referred to earlier, the Cleaner to identify and remove outliers, an Aggregator which generalizes across trajectories within a collection, and it can track multiple people in a single trajectory collection.\nIf you are already comfortable in Python’s data science space, but haven’t yet worked with geospatial data, I hope you were intrigued enough to look into it more, as explicitly including geospatial dimensions in your analyses is enriching without being terribly difficult. If you are a Pythonista without any DS experience, maybe this will help you jump in. I find that Python is such a simple, expressive language, and the libraries of Pandas, GeoPandas, and MovingPandas are such a joy to work with.\nHappy Coding ## Additional Code To make the article easier to read, I have put code here which would need to be run before trying any of the code examples in the article. This includes the required libraries and the helper function I use when making the graphs. All code can be found on my GitHub repo.\nimport pandas as pd \nimport geopandas as gpd \nimport movingpandas as mpd \nimport matplotlib.pyplot as plt\nimport contextily as ctx\nimport numpy as np\nfrom shapely.geometry import Polygon\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef t_plot(traj, \n            figsize=(8,5), \n            source=ctx.providers.Esri.NatGeoWorldMap, \n            title=None, \n            column=None\n            ):\n    sport = 'Bike Ride' if traj.df.Sport[0] == 1 else 'Walk'\n    f, ax = plt.subplots(figsize=figsize)\n    if column:\n        traj.plot(ax=ax, lw=3, column=column, \n                  cmap='inferno', legend=True)\n    else:\n        traj.plot(ax=ax, lw=3, color='cyan')\n    ctx.add_basemap(ax, crs=traj.crs, \n                    source=source)\n    ax.set_axis_off()\n    if title: \n        ax.set_title(title)\n    else:\n        ax.set_title(f'{sport} in {traj.df.Name[0]} on {traj.df.index[0].strftime('%x')}')\n    return f, ax\n\nannot_props = dict(\n    xytext=(3, 1), \n    textcoords=\"offset fontsize\", \n    c='r', weight='bold', ha='center',\n    arrowprops=dict(arrowstyle='-', ec='orange')\n)\n\ndef trek_info(traj):\n    print(f'''\n        Date:      {traj.get_start_time().strftime('%x')}\n        Activity:  {'Bike Ride' if traj.df.Sport[0] == 1 else 'Walk'}\n        Duration: {traj.get_duration().seconds/60:&gt;3.0f}   minutes\n        Distance: {traj.get_length(units='mi'):&gt;5.1f} miles\n        Avg Temp:  {(traj.df.Temp.mean()*9/5)+32:.0f} degrees F\n        CRS:       {traj.get_crs().to_epsg()}\n        Points:    {traj.size()}\n        Bounds:    ({\", \".join(\n                          map(str,\n                          [round(traj.to_crs(4269).get_bbox()[x], 3) \n                           for x in range(4)]))})\n\n        Max Elevation: {traj.get_max(column='Elev'):,} meters\n        Min Elevation: {traj.get_min(column='Elev'):,} meters\n        Avg Elevation: {traj.df.Elev.mean():,.0f} meters\n        \n                Time         Location (Lon/Lat)\n        Start: {\n          traj.get_start_time().strftime('%X')\n          }  {\n          tuple([round(x,3) for x in \n          traj.to_crs(4269).get_start_location().coords[0]])\n          } degrees\n        End:   {\n          traj.get_end_time().strftime('%X')\n          }  {\n          tuple([round(x,3) for x in\n          traj.to_crs(4269).get_end_location().coords[0]])\n          } degrees\n    ''')",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Speed, Simplification and Segments"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#the-trek-so-far",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#the-trek-so-far",
    "title": "Speed, Simplification and Segments",
    "section": "",
    "text": "In the first article, we took raw data from a trekking application, a GPX file in the form of standardized XML, parsed it to CSV, which we imported into a DataFrame with Pandas. Each record contains latitude, longitude, elevation and a time stamp. Using GeoPandas, we turned the latitude and longitude fields into POINT geometries and assigned an appropriate Coordinate Reference System so that we can work with the data geospatially. In the second article, we used MovingPandas to convert those POINTs into LINESTRINGs, allowing for calculations of distances not only along the route but to other locations or routes. We also learned how to auto-detect places along the trajectory where we paused.\nIn this article I will show how to incorporate the time dimension and calculate speed, duration, and other statistics, and how to split the path into segments. We already did this implicitly last time when we did stop detection, since a pause is based on time, and those pauses are natural points to split the route, and, not surprisingly, this is quite simple to do in MovingPandas. But first, we should look at simplification of the data.",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Speed, Simplification and Segments"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#simplification",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#simplification",
    "title": "Speed, Simplification and Segments",
    "section": "",
    "text": "The raw data contains a level of granularity that is not necessarily needed, and can have a significant computational cost. The data can be simplified in different ways such as combining short time periods into longer ones, or smoothing by combining multiple paths within a certain area into a single, straight path. In this article, I’ll look at generalizing, down-sampling the time.\nGPS trackers take frequent readings, seconds apart. Let’s look at the average time between readings in one of our treks. I’ll start by importing a GeoDataFrame with walks and bike rides in New Mexico, using the method described in the prior articles.\ndf = pd.read_csv('data/b4/combined.csv')\ngdf = gpd.GeoDataFrame(\n    df, \n    geometry=gpd.points_from_xy(x=df.Lon, y=df.Lat), \n    crs=4269\n).to_crs(32111)\nidList = list(gdf.groupby(['Id']).nunique().reset_index().Id)\nfor i, track in enumerate(idList):\n    gdf.loc[gdf.Id == track, 'trajectory_id'] = i\ngdf['Time'] = pd.to_datetime(gdf.Time)\ntc = mpd.TrajectoryCollection(\n    gdf, 'trajectory_id', \n    t='Time', x='Lon', y='Lat'\n)\nprint(tc)\nTrajectoryCollection with 130 trajectories\nax = tc.plot(\n    column='trajectory_id',\n    figsize=(10,5), \n    cmap='inferno', \n    legend=True\n)\nax.set_title('Walks and Rides in NM')\nax.set_axis_off()\nctx.add_basemap(ax, crs=gdf.crs,\n                source=ctx.providers.Esri.NatGeoWorldMap);\n\nHere are the total number of records, and the average time between readings:\ndef print_intervals_size(tc):\n    print(f'''\n    Avg sampling interval: {(round((np.mean(\n        [t.get_sampling_interval().seconds for t in tc.trajectories]\n     )), 2))} seconds\n    Total records: {np.sum([t.size() for t in tc.trajectories])}\n     ''')\nprint_intervals_size(tc)\nAvg sampling interval: 5.92 seconds\nTotal records: 84319\nThat’s way more than I need. A reading every 30 seconds is more than enough. I can use the generalize() function from MovingPandas to easily do so using the tolerance parameter:\ntc_generalized = (\n    mpd.MinTimeDeltaGeneralizer(tc)\n        .generalize(tolerance=timedelta(seconds=30))\n)\nprint_intervals_size(tc_generalized)\nAvg sampling interval: 33.28 seconds\nTotal records: 17258\nI’ll extract a single trajectory from before and after so we can see what happens with simplification. First I’ll print some basic information about the trek. Moving pandas provides functions such as get_duration and get_length to obtain information. I’ll include the print code here to demonstrate how to access the values and what functions are available. To access columns in the underlying DataFrame, you can use tc.df.SomeColumn. Python’s f-strings make it easy to generate these reports. ```python def trek_info(traj): print(f’’’ Date: {traj.get_start_time().strftime(‘%x’)} Activity: {‘Bike Ride’ if traj.df.Sport[0] == 1 else ‘Walk’} Duration: {traj.get_duration().seconds/60:.0f} minutes Distance: {traj.get_length(units=‘mi’):.1f} miles Avg Speed: {((traj.get_length(units=‘mi’)) /\n(traj.get_duration().seconds/60**2)):.1f} mph Avg Temp: {(traj.df.Temp.mean()*9/5)+32:.0f} degrees F CRS: {traj.get_crs().to_epsg()} Points: {traj.size()} Bounds: ({“,”.join( map(str, [round(traj.to_crs(4269).get_bbox()[x], 3) for x in range(4)]))})\n    Max Elevation: {traj.get_max(column='Elev'):,} meters\n    Min Elevation: {traj.get_min(column='Elev'):,} meters\n    Avg Elevation: {traj.df.Elev.mean():,.0f} meters\n    \n            Time         Location (Lon/Lat)\n    Start: {\n      traj.get_start_time().strftime('%X')\n      }  {\n      tuple([round(x,3) for x in \n      traj.to_crs(4269).get_start_location().coords[0]])\n      } degrees\n    End:   {\n      traj.get_end_time().strftime('%X')\n      }  {\n      tuple([round(x,3) for x in\n      traj.to_crs(4269).get_end_location().coords[0]])\n      } degrees\n''')\n```python\ntraj_bike = tc.trajectories[52]\ntraj_bike_gen = tc_generalized.trajectories[52]\ntrek_info(traj_bike)\n         Date:      05/30/24\n        Activity:  Bike Ride\n        Duration:  63 minutes\n        Distance:  13.4 miles\n        Avg Speed: 12.8 mph\n        Avg Temp:  76 degrees F\n        CRS:       32111\n        Points:    681\n        Bounds:    (-106.714, 35.144, -106.648, 35.183)\n\n        Max Elevation: 1,564 meters\n        Min Elevation: 1,504 meters\n        Avg Elevation: 1,528 meters\n        \n                Time         Location (Lon/Lat)\n        Start: 18:41:12  (-106.708, 35.147) degrees\n        End:   19:44:04  (-106.708, 35.148) degrees\ntrek_info(traj_bike_gen)\n# Difference from `traj_bike`\n        Points:    105\nHere is a before and after plot:\nfrom itertools import chain\nf, (ax1, ax2) = plt.subplots(2, 2)\nfor i, traj in enumerate([traj_bike, traj_bike_gen]):\n    traj.plot(ax=ax1[i])\n    ctx.add_basemap(ax1[i], crs=traj.crs, \n                    source=ctx.providers.CartoDB.Voyager)\nfor i, traj in enumerate([traj_bike, traj_bike_gen]):\n    traj.plot(ax=ax2[i])\n    ctx.add_basemap(ax2[i], crs=traj.crs, \n                    source=ctx.providers.CartoDB.Voyager)\n    ax2[i].set_xlim(460000, 462000)\n    ax2[i].set_ylim(462000, 464000)\nfor axs in chain(ax1, ax2): \n    axs.set_axis_off() \nax1[0].set_title('Raw')\nax1[1].set_title('Generalized')\nf.suptitle('Effect of Generalization to 30 Seconds')\nf.tight_layout();",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Speed, Simplification and Segments"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#speed-and-sundry",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#speed-and-sundry",
    "title": "Speed, Simplification and Segments",
    "section": "",
    "text": "Before moving to speed calculations, let’s make a quick plot:\nt_plot(traj_bike_gen, column='speed');\n\nI just plotted the speed column, but a quick check will show there is no speed column.\ntraj_bike_gen.df.columns\nIndex(['Id', 'Name', 'Lat', 'Lon', 'Elev', 'Temp', 'Weather', 'Sport',\n       'Sport_s', 'trajectory_id', 'geometry'],\n      dtype='object')\nLike Dorothy in Oz, you had the speed all along. MovingPandas does the calculation in the background for you. If you want an actual speed column, that’s easy enough:\ntc_generalized.add_speed()\ntc_generalized.trajectories[52].df.columns\nIndex(['Id', 'Name', 'Lat', 'Lon', 'Elev', 'Temp', 'Weather', 'Sport',\n       'Sport_s', 'trajectory_id', 'geometry', 'mi/h', 'speed'],\n      dtype='object')\nSince my CRS is in meters, so is the speed. If I’d prefer miles per hour, I can specify the units:\ntc_generalized.add_speed(overwrite=True, name='mi/h', units=('mi', 'h'))\ntc_generalized.trajectories[52].df.iloc[5]\nId                                Workout-2024-05-30-18-41-12\nName                                              Albuquerque\nLat                                                 35.150019\nLon                                                 -106.7125\nElev                                                     1536\nTemp                                                     24.2\nWeather                                                     0\nSport                                                       1\nSport_s                                                  Bike\ntrajectory_id                                            52.0\ngeometry         POINT (457860.4830821714 460313.87076766626)\nmi/h                                                13.109903\nspeed                                                5.860651\nName: 2024-05-30 18:43:54.251000, dtype: object\nIn fact, we can automagically create distance, acceleration, direction, and time delta columns as well, all with arbitrary units. Here I’ll add a number of metrics in both metric and imperial units, by supplying the units and column names as parameters.\ntc_generalized.add_distance(overwrite=True, name=\"distance (km)\", units=\"km\")\ntc_generalized.add_distance(overwrite=True, name=\"distance (mi)\", units=\"mi\")\ntc_generalized.add_distance(overwrite=True, name=\"distance (meters)\", units=\"m\")\ntc_generalized.add_distance(overwrite=True, name=\"distance (feet)\", units=\"ft\")\ntc_generalized.add_timedelta(overwrite=True)\ntc_generalized.add_speed(overwrite=True, name=\"speed (ft/min)\", units=(\"ft\", \"min\"))\ntc_generalized.add_speed(overwrite=True, name=\"speed (km/hour)\", units=(\"km\", \"h\"))\ntc_generalized.add_speed(overwrite=True, name=\"speed (mi/hour)\", units=(\"mi\", \"h\"))\ntc_generalized.add_acceleration(\n    overwrite=True, name=\"acceleration (mph/s)\", units=(\"mi\", \"h\", \"s\")\n)\ntc_generalized.add_acceleration(\n    overwrite=True, name=\"acceleration (kmph/s)\", units=(\"km\", \"h\", \"s\")\n)\ntc_generalized.add_direction(overwrite=True)\ntc_generalized.add_angular_difference(overwrite=True)\n(tc_generalized.trajectories[52]\n .df.reset_index()\n .loc[5, 'speed':'angular_difference'])\nspeed                                  5.860651\nmi/h                                  13.109903\ndistance (km)                          0.175855\ndistance (mi)                          0.109271\ndistance (meters)                     175.85469\ndistance (feet)                      576.951082\ntimedelta                0 days 00:00:30.006000\nspeed (ft/min)                      1153.671431\nspeed (km/hour)                       21.098343\nspeed (mi/hour)                       13.109903\nacceleration (mph/s)                   0.104386\nacceleration (kmph/s)                  0.167992\ndirection                            349.062474\nangular_difference                    70.594237\nName: 5, dtype: object\nAnd now, I can generate summaries somewhat easier:\ntraj_bike_gen = tc_generalized.trajectories[52]\nprint(f'''\n    Total Distance: \n        {traj_bike_gen.df['distance (meters)']\n            .sum().item()/1000:&gt;4.1f} kilometers\n        {traj_bike_gen.df['distance (mi)']\n            .sum().item():&gt;4.1f} miles\n    Average Speed:\n        {traj_bike_gen.df['speed (km/hour)']\n            .mean().item():&gt;4.1f} kilometers per hour\n        {traj_bike_gen.df['speed (mi/hour)']\n            .mean().item():&gt;4.1f} miles per hour\n''')\n    Total Distance: \n        18.5 kilometers\n        11.5 miles\n    Average Speed:\n        19.2 kilometers per hour\n        11.9 miles per hour\nI’m going to also add an elevation delta. I’d like to plot the speed next to the change in elevation. So I’ll add the column.\ntraj_bike_gen.df['ElevD'] = (\n    [traj_bike_gen.df.iloc[i].Elev - traj_bike_gen.df.iloc[i-1].Elev \n     for i in range(len(traj_bike_gen.df))]\n)\nHere is a plot of speed next to the elevation changes.\nf, ax = plt.subplots(nrows=2, figsize=(10,7))\ntraj_bike_gen.plot(ax=ax[0], column='speed', \n                   cmap='inferno', legend=True, \n                  linewidth=4)\ntraj_bike_gen.plot(ax=ax[1], column='ElevD', \n                   cmap='inferno', legend=True, \n                  linewidth=4)\nfor i in range(2): \n    ctx.add_basemap(ax[i], crs=traj_bike_gen.crs, \n                   source=ctx.providers.Esri.NatGeoWorldMap)\n    ax[i].set_axis_off()\nax[0].set_title('Speed')\nax[1].set_title('Elevation Change')\nf.tight_layout();",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Speed, Simplification and Segments"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#segmentation",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#segmentation",
    "title": "Speed, Simplification and Segments",
    "section": "",
    "text": "The final topic I want to cover is route segmentation. This includes splitting the entire route by logical segments such as every 15 minutes or at stop points. It can also mean extracting a specific segment based on time or geographic location. MovingPandas has a variety of Splitter classes, depending on the purpose.\nTo demonstrate the split on stops, I’ll use the NYC walk from the last article, since I actually paused occasionally. In fact, the stop detector we used in that exercise could have given us the segments directly with it’s get_stop_segments() method, but here I’ll use the StopSplitter.\ndf_ny = gpd.read_file('data/ny_trek.gpkg')\ndf_ny['Sport'] = 17\nny_walk = (\n    mpd.TrajectoryCollection(\n    df_ny, 'trajectory_id', \n    t='Time', x='Lon', y='Lat'\n    ).trajectories[0])\n\nsplit_stop = (mpd.StopSplitter(ny_walk)\n              .split(\n                min_duration = timedelta(seconds=120), \n                max_diameter = 100))\nsplit_stop\nTrajectoryCollection with 18 trajectories\nf, ax = t_plot(ny_walk)\nfor i in range(1,18,2): \n    split_stop.trajectories[i].plot(ax=ax, color='b')\n\nI can use the TemporalSplitter to divide my trek into hour-long segments:\nsplit_time = (mpd.TemporalSplitter(ny_walk)\n             .split(mode='hour'))\n\nI can also extract a single segment based on an arbitrary time range using get_segment_between. I’ll create two datatime variables to see where I was between 4:00 and 5:00 local time.\nt1 = datetime(2024, 9, 11, 19) # UTC Time\nt2 = t1 + timedelta(minutes=60)\nsegment = ny_walk.get_segment_between(t1,t2)\nt_plot(segment);\n\nFinally, I can extract the portion of the path which lies in a specific geographical area by using clip and intersecting the path with a POLYGON or bounding box. Here is an example:\nxmin, xmax, ymin, ymax = 191000, 192500, 210000, 211000\npolygon = Polygon(\n    [(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin), (xmin, ymin)]\n)\nintersections = ny_walk.clip(polygon) \n\nf, ax = t_plot(ny_walk)\ngpd.GeoSeries(polygon).plot(ax=ax, color='lightblue', \n                           alpha=0.5)\nintersections.plot(ax=ax, color='r');",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Speed, Simplification and Segments"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#conclusion",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/index.html#conclusion",
    "title": "Speed, Simplification and Segments",
    "section": "",
    "text": "I hope you had fun with these articles, and were able to harness your own GPS data to create some lovely maps of your excursions. If you subscribe to one of the apps like Strava or AllTrails you can get get their maps to underlay with your routes. MovingPandas has many more functions such as the Smoother referred to earlier, the Cleaner to identify and remove outliers, an Aggregator which generalizes across trajectories within a collection, and it can track multiple people in a single trajectory collection.\nIf you are already comfortable in Python’s data science space, but haven’t yet worked with geospatial data, I hope you were intrigued enough to look into it more, as explicitly including geospatial dimensions in your analyses is enriching without being terribly difficult. If you are a Pythonista without any DS experience, maybe this will help you jump in. I find that Python is such a simple, expressive language, and the libraries of Pandas, GeoPandas, and MovingPandas are such a joy to work with.\nHappy Coding ## Additional Code To make the article easier to read, I have put code here which would need to be run before trying any of the code examples in the article. This includes the required libraries and the helper function I use when making the graphs. All code can be found on my GitHub repo.\nimport pandas as pd \nimport geopandas as gpd \nimport movingpandas as mpd \nimport matplotlib.pyplot as plt\nimport contextily as ctx\nimport numpy as np\nfrom shapely.geometry import Polygon\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef t_plot(traj, \n            figsize=(8,5), \n            source=ctx.providers.Esri.NatGeoWorldMap, \n            title=None, \n            column=None\n            ):\n    sport = 'Bike Ride' if traj.df.Sport[0] == 1 else 'Walk'\n    f, ax = plt.subplots(figsize=figsize)\n    if column:\n        traj.plot(ax=ax, lw=3, column=column, \n                  cmap='inferno', legend=True)\n    else:\n        traj.plot(ax=ax, lw=3, color='cyan')\n    ctx.add_basemap(ax, crs=traj.crs, \n                    source=source)\n    ax.set_axis_off()\n    if title: \n        ax.set_title(title)\n    else:\n        ax.set_title(f'{sport} in {traj.df.Name[0]} on {traj.df.index[0].strftime('%x')}')\n    return f, ax\n\nannot_props = dict(\n    xytext=(3, 1), \n    textcoords=\"offset fontsize\", \n    c='r', weight='bold', ha='center',\n    arrowprops=dict(arrowstyle='-', ec='orange')\n)\n\ndef trek_info(traj):\n    print(f'''\n        Date:      {traj.get_start_time().strftime('%x')}\n        Activity:  {'Bike Ride' if traj.df.Sport[0] == 1 else 'Walk'}\n        Duration: {traj.get_duration().seconds/60:&gt;3.0f}   minutes\n        Distance: {traj.get_length(units='mi'):&gt;5.1f} miles\n        Avg Temp:  {(traj.df.Temp.mean()*9/5)+32:.0f} degrees F\n        CRS:       {traj.get_crs().to_epsg()}\n        Points:    {traj.size()}\n        Bounds:    ({\", \".join(\n                          map(str,\n                          [round(traj.to_crs(4269).get_bbox()[x], 3) \n                           for x in range(4)]))})\n\n        Max Elevation: {traj.get_max(column='Elev'):,} meters\n        Min Elevation: {traj.get_min(column='Elev'):,} meters\n        Avg Elevation: {traj.df.Elev.mean():,.0f} meters\n        \n                Time         Location (Lon/Lat)\n        Start: {\n          traj.get_start_time().strftime('%X')\n          }  {\n          tuple([round(x,3) for x in \n          traj.to_crs(4269).get_start_location().coords[0]])\n          } degrees\n        End:   {\n          traj.get_end_time().strftime('%X')\n          }  {\n          tuple([round(x,3) for x in\n          traj.to_crs(4269).get_end_location().coords[0]])\n          } degrees\n    ''')",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Speed, Simplification and Segments"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html",
    "title": "Points to Paths in Python",
    "section": "",
    "text": "In the prior article, I showed how to use raw GPS data generated from a sport tracker to create maps and profile elevations along the path. The GeoDataFrames created were just a collection of point geometries with time, location and elevation data. To profile, for example, speed, requires converting the point geometries to line geometries, calculating distances and time deltas between each point, and converting between coordinate systems and units of measurement. If we were concerned with bearing, we would need to also calculate angles between the line segments we create. None of this is overly complicated, just awfully tedious. But this being Python, there is a dedicated library that makes all of this easy peasy.\nMovingPandas is a library that extends Pandas/Geopandas by taking the list of point geometries and turning them into “trajectories”. It generates directed lines between consecutive points, and provides functions for all of the calculations I mentioned above and more.\nIn this article I will show how to create and work with trajectories. I’ll then show how to use MovingPandas to find locations along the path based on time, find the distance from the path to nearby locations, and finally how to identify locations where I paused or stopped. In the next article, I will show how to simplify and smooth paths, profile speed, and segment the trajectories based on various criteria. As before, an expanded version of the code in this article is available in my repository.\n\n\n\nI’ll start by loading the necessary libraries and creating a helper function for mapping as well as some defaults for annotating points on the maps. Then i will load some data converted from raw GPS data as described in the previous article.\nimport pandas as pd \nimport geopandas as gpd \nimport movingpandas as mpd \nimport matplotlib.pyplot as plt\nimport contextily as ctx\nfrom datetime import datetime, timedelta\nfrom geopandas.tools import reverse_geocode, geocode\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef t_plot(traj, \n            figsize=(6,8), \n            source=ctx.providers.CartoDB.Voyager, \n            title=None):\n    f, ax = plt.subplots(figsize=figsize)\n    traj.plot(ax=ax, lw=4, color='chartreuse')\n    ctx.add_basemap(ax, crs=traj.crs, \n                    source=source)\n    ax.set_axis_off()\n    if title: \n        ax.set_title(title)\n    else:\n        ax.set_title(\n            f'Walk in {traj.df.Name[0]} on {traj.df.index[0].strftime('%x')}'\n        )\n    return f, ax\n\nannot_props = dict(\n    xytext=(3, 1), \n    textcoords=\"offset fontsize\", \n    c='r', weight='bold', ha='center',\n    arrowprops=dict(arrowstyle='-', ec='orange')\n)\n\ndf = pd.read_csv('data/b3/combined.csv')\ngdf = gpd.GeoDataFrame(\n    df, \n    geometry=gpd.points_from_xy(x=df.Lon, y=df.Lat), \n    crs=4269\n).to_crs(32111)\nMovingPandas requires an integer id for each walk, which I don’t have, so I’ll create one based on the string id. I’ll also convert the Time from a generic object to datetime64.\nidList = list(gdf.groupby(['Id']).nunique().reset_index().Id)\nfor i, track in enumerate(idList):\n    gdf.loc[gdf.Id == track, 'trajectory_id'] = i\ngdf['Time'] = pd.to_datetime(gdf.Time)\ngdf.info()\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 3044 entries, 0 to 3043\nData columns (total 10 columns):\n #   Column         Non-Null Count  Dtype              \n---  ------         --------------  -----              \n 0   Id             3044 non-null   object             \n 1   Name           3044 non-null   object             \n 2   Lat            3044 non-null   float64            \n 3   Lon            3044 non-null   float64            \n 4   Elev           3044 non-null   int64              \n 5   Time           3044 non-null   datetime64[ns, UTC]\n 6   Temp           3044 non-null   float64            \n 7   Weather        3044 non-null   int64              \n 8   geometry       3044 non-null   geometry           \n 9   trajectory_id  3044 non-null   float64            \ndtypes: datetime64[ns, UTC](1), float64(4), geometry(1), int64(2), object(2)\nmemory usage: 237.9+ KB\nOne of the walks in the collection was in New York City, and the rest in New Jersey. Since the locations are pretty far apart, I’ll split out the NYC walk from the others.\ngdf_nj = gdf.loc[gdf.Name != 'New York']\ngdf_ny = gdf.loc[gdf.Name == 'New York']\ngdf_nj.shape, gdf_ny.shape\n((2063, 10), (981, 10))\n\n\n\nNow I’m ready to create the trajectories. The data will be imported as a Trajectory Collection which will contain the individual Trajectories. I’ll first create one from the NJ group, which contains several walks.\ntc_nj = mpd.TrajectoryCollection(\n    gdf_nj, 'trajectory_id', \n    t='Time', x='Lon', y='Lat'\n)\nprint(tc_nj)\nTrajectoryCollection with 6 trajectories\nLet’s see what this looks like.\nax = traj_collection.plot(\n    column='trajectory_id', \n    legend=True, figsize=(9,5), cmap='Set1')\nax.set_title('Walks in NJ, September 2024')\nctx.add_basemap(ax, crs=gdf_proj.crs,\n                source=ctx.providers.Esri.WorldTopoMap);\n\nI can extract a single trajectory and its underlying data frame.\ntraj_nj1 = tc_nj.trajectories[2]\nprint(traj_nj1)\nTrajectory 2.0 (2024-09-07 21:20:23.975000 to 2024-09-07 22:03:42.022000) | Size: 267 | Length: 5173.1m\nBounds: (177912.1946985984, 243124.49067874879, 179216.17084672116, 245408.85808260852)\nLINESTRING (178912.27991325516 244815.1747420021, 178912.27991325516 244815.1747420021, 178903.67850\nThe associated data is stored in the df attribute:\nprint(traj_nj1.df.info())\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nDatetimeIndex: 267 entries, 2024-09-07 21:20:23.975000 to 2024-09-07 22:03:42.022000\nData columns (total 9 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   Id             267 non-null    object  \n 1   Name           267 non-null    object  \n 2   Lat            267 non-null    float64 \n 3   Lon            267 non-null    float64 \n 4   Elev           267 non-null    int64   \n 5   Temp           267 non-null    float64 \n 6   Weather        267 non-null    int64   \n 7   geometry       267 non-null    geometry\n 8   trajectory_id  267 non-null    float64 \ndtypes: float64(4), geometry(1), int64(2), object(2)\nmemory usage: 29.0+ KB\nNone\nt_plot(traj_nj1, source=ctx.providers.OpenStreetMap.HOT);\n\nAnother way to extract a specific trajectory is using MovingPandas’ filter function, as so:\nt_plot(\ntc_nj.filter(\"Name\", [\"Park Ridge\"])\n     .trajectories[0]\n);\n\nFor the rest of the article I’ll use the longest trek I have in this data set, the one in New York. I’ll create the Trajectory Collection, extract the single trajectory and find the start and end times and locations.\ntc_ny = mpd.TrajectoryCollection(\n    gdf_ny, 'trajectory_id', \n    t='Time', x='Lon', y='Lat'\n)\ntraj_ny = tc_ny.trajectories[0]\nst = traj_ny.get_start_time()\nsl = traj_ny.to_crs(4269).get_start_location()\net = traj_ny.get_end_time()\nel = traj_ny.to_crs(4269).get_end_location()\nprint(f'''\n            Time         Location (Lon/Lat)\n            \n    Start: {st.strftime('%X')}   {sl.coords[0]} degrees\n    End:   {et.strftime('%X')}   {el.coords[0]}  degrees\n\n    Duration:  {traj_ny.get_duration().seconds/60:.0f} minutes\n    Distance:  {traj_ny.get_length(units='mi'):.2f} miles\n    Direction: {traj_ny.get_direction():.0f} degrees\n''')\n            Time         Location (Lon/Lat)\n            \n    Start: 17:43:42   (-73.991157, 40.752025) degrees\n    End:   20:49:09   (-73.98894, 40.726903)  degrees\n\n    Duration:  185 minutes\n    Distance:  6.69 miles\n    Direction: 176 degrees\nt_plot(traj_ny);\n\n\n\n\nFirst, let me find my position at an arbitrary time, and then 30 minutes later. Where was I, for example, at 18:00, and then 45 minutes later?\nt = datetime(2024, 9, 11, 18)\nt2 = t + timedelta(minutes=45)\nprint(f'''\n    Nearest:      {traj_ny.to_crs(4269)\n                    .get_position_at(t, method='nearest')}\n    Interpolated: {traj_ny.to_crs(4269)\n                    .get_position_at(t, method='interpolated')}\n    Previous row: {traj_ny.to_crs(4269)\n                    .get_position_at(t, method='ffill')}\n    Next row:     {traj_ny.to_crs(4269)\n                    .get_position_at(t, method='bfill')}\n''')\n    Nearest:      POINT (-74.003574 40.756368)\n    Interpolated: POINT (-74.00361304734385 40.75638409846633)\n    Previous row: POINT (-74.003688 40.756415)\n    Next row:     POINT (-74.003574 40.756368)\nI can use the geocode tool provided by geopandas to get the nearest addresses at these points so I can show them on the map. Pay attention to crs, since nominatim expects lat/lon coordinates and we currently have projected coordinates.\npoint = traj_ny.get_position_at(t, method='interpolated')\npoint_up = traj_ny.to_crs(4269).get_position_at(t, method='interpolated')\npoint2 = traj_ny.get_position_at(t2, method='interpolated')\npoint2_up = traj_ny.to_crs(4269).get_position_at(t2, method='interpolated')\n\nrg = reverse_geocode(\n    [point_up, point2_up], \n    provider=\"nominatim\", \n    user_agent=\"your_project\", \n    timeout=10\n).to_crs(32111)\n\nf, ax = t_plot(traj_ny)\ngpd.GeoSeries(point).plot(ax=ax, color='red', markersize=100)\ngpd.GeoSeries(point2).plot(ax=ax, color='red', markersize=100)\nfor x, y, label in zip(rg.geometry.x, rg.geometry.y, rg.address):\n    ax.annotate(f'{label.split(',')[0]}, {label.split(',')[1]}', \n                xy=(x, y), **annot_props)\nax.set_ylim(211500, 214500)\nax.set_xlim(191400, 192500);\n\nIt might be more interesting to plot my location at regular intervals along the walk. What about plotting my location every 30 minutes?\nint_count = (et - st) // timedelta(seconds=(60*30))\nint_length = (et - st) / int_count\nintervals = [st + (i*int_length) for i in range(int_count)]\ninterval_points = [traj_ny.to_crs(4269).get_position_at(\n                    t, method='interpolated') \n                   for t in intervals]\nrg = reverse_geocode(\n    interval_points, \n    provider=\"nominatim\", \n    user_agent=\"your_project\", \n    timeout=10\n).to_crs(32111)\n\nf, ax = t_plot(traj_ny, figsize=(7,11))\nrg.plot(ax=ax, c='r', markersize=100)\nfor x, y, label in zip(rg.geometry.x, rg.geometry.y, rg.address):\n    ax.annotate(f'{label.split(',')[0]}, {label.split(',')[1]}', \n                xy=(x, y), **annot_props)\nax.set_title(ax.get_title() + f'\\nEvery 30 minutes');\n\n\n\n\nNot only can you calculate distances along the path, which we will look at more in the next article, but you can calculate the distance from points along the trajectory to other locations not on the path. It will determine the closest you came to other points, lines or polygons, including, of course, other trajectories. For example, when I’m in New York, I love to visit the Strand and the huge Barnes and Noble on Union Square. I didn’t have time this trip, sadly, but I’m curious how close I was from those stores.\naddresses = [{\n                'id': 1,\n                'name': \"The Strand\", \n                'addr': \"828 Broadway, New York, NY 10003\"\n            }, \n            {\n                'id': 2,\n                'name': \"Barnes and Noble\", \n                'addr': \"33 East 17th Street, 10003, New York\"\n            }]\nadd_df = pd.DataFrame(addresses)\ngeo = geocode(\n    add_df['addr'], \n    provider='nominatim', \n    user_agent='your_project', \n    timeout=10\n)\nbook_stores = geo.join(add_df).to_crs(32111)\nprint(book_stores)\n                        geometry  \\\n0  POINT (192997.822 211055.516)   \n1  POINT (193108.568 211461.041)   \n\n                                             address  id              name  \\\n0  Strand Bookstore, 828, Broadway, University Vi...   1        The Strand   \n1  Barnes & Noble, 33, East 17th Street, Union Sq...   2  Barnes and Noble   \n\n                                   addr  \n0      828 Broadway, New York, NY 10003  \n1  33 East 17th Street, 10003, New York\ndists_to = [traj_ny.distance(book_stores.loc[i, ['geometry']], \n                            units='mi')[0] \n            for i in range(len(book_stores))]\nstore_names = [book_stores.loc[i, 'name'] \n               for i in range(len(book_stores))]\nfor i in range(len(book_stores)):\n    print(f'Closest distance to {store_names[i]} was {dists_to[i]:.1f} miles') \nClosest distance to The Strand was 0.4 miles\nClosest distance to Barnes and Noble was 0.6 miles\nf, ax = t_plot(traj_ny, figsize=(7,11))\nbook_stores.plot(ax=ax, color='red', markersize=100)\nfor x, y, label in zip(book_stores.geometry.x, \n                       book_stores.geometry.y, \n                       book_stores.name):\n    ax.annotate(label, xy=(x,y), **annot_props)\n\n\n\n\nIn this final section I’ll look at stop detection. Here I’ll just be considering stop locations. In the next article, I’ll show how to use the same approach as one method for segmenting the trajectories. Stop detection requires instantiation of a stop detector which will provide the functions needed.\ndetector = mpd.TrajectoryStopDetector(traj_ny)\nThe detector takes parameters to determine how long a pause constitutes a stop, and how far you need to move to constitute movement.\nstop_points = detector.get_stop_points(\n    min_duration=timedelta(seconds=120), max_diameter=100\n)\nlen(stop_points)\n18\nWe can look up the addresses nearest to the stop points, and plot some of them on the map.\nstop_points = stop_points.set_crs(32111)\nrg = reverse_geocode(\n    stop_points.to_crs(4269).geometry, \n    provider=\"nominatim\", \n    user_agent=\"your_project\", timeout=10\n).to_crs(32111)\nf, ax = t_plot(traj_ny)\nstop_points.plot(ax=ax, color='none', markersize=100, ec='r')\npoint_labels = [0,3,7,11,17]\nfor (x, y), label in zip(\n                    rg.iloc[point_labels].geometry.apply(\n                        lambda p: p.coords[0]), \n                    rg.iloc[point_labels].address):\n    ax.annotate(f'{label.split(',')[0]}, {label.split(',')[1]}', \n                xy=(x, y), **annot_props)\nax.set_title(ax.get_title() + f'\\nStop Points');\n\n\n\n\nShowing interactive graphing in the context of a published article is not very useful, but I should point out that MovingPandas supports both hvplot with folium as well as GeoPandas’ explore(), both of which can be useful tools for identifying areas of interest. This is an example with explore().\nm = traj_ny.explore(\n    column=\"trajectory_id\",\n    tooltip=\"t\",\n    popup=True,\n    style_kwds={\"weight\": 4},\n    name=\"Time\",\n)\n\nstop_points.explore(\n    m=m,\n    color=\"red\",\n    tooltip=\"stop_id\",\n    popup=True,\n    marker_kwds={\"radius\": 10},\n    name=\"Stop points\",\n)\n\n\n\n\nIn the third article, I will address issues of generalization and simplification which can be important when working with large data sets. I’ll show how to generate speed, duration and distance information, and how to segment the trajectories based on geometries, stops, and other criteria. I hope that these articles are proving interesting and useful. Please visit my website and repository for more code, articles and updates.",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Points to Paths in Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#introduction",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#introduction",
    "title": "Points to Paths in Python",
    "section": "",
    "text": "In the prior article, I showed how to use raw GPS data generated from a sport tracker to create maps and profile elevations along the path. The GeoDataFrames created were just a collection of point geometries with time, location and elevation data. To profile, for example, speed, requires converting the point geometries to line geometries, calculating distances and time deltas between each point, and converting between coordinate systems and units of measurement. If we were concerned with bearing, we would need to also calculate angles between the line segments we create. None of this is overly complicated, just awfully tedious. But this being Python, there is a dedicated library that makes all of this easy peasy.\nMovingPandas is a library that extends Pandas/Geopandas by taking the list of point geometries and turning them into “trajectories”. It generates directed lines between consecutive points, and provides functions for all of the calculations I mentioned above and more.\nIn this article I will show how to create and work with trajectories. I’ll then show how to use MovingPandas to find locations along the path based on time, find the distance from the path to nearby locations, and finally how to identify locations where I paused or stopped. In the next article, I will show how to simplify and smooth paths, profile speed, and segment the trajectories based on various criteria. As before, an expanded version of the code in this article is available in my repository.",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Points to Paths in Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#getting-ready-for-movingpandas",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#getting-ready-for-movingpandas",
    "title": "Points to Paths in Python",
    "section": "",
    "text": "I’ll start by loading the necessary libraries and creating a helper function for mapping as well as some defaults for annotating points on the maps. Then i will load some data converted from raw GPS data as described in the previous article.\nimport pandas as pd \nimport geopandas as gpd \nimport movingpandas as mpd \nimport matplotlib.pyplot as plt\nimport contextily as ctx\nfrom datetime import datetime, timedelta\nfrom geopandas.tools import reverse_geocode, geocode\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef t_plot(traj, \n            figsize=(6,8), \n            source=ctx.providers.CartoDB.Voyager, \n            title=None):\n    f, ax = plt.subplots(figsize=figsize)\n    traj.plot(ax=ax, lw=4, color='chartreuse')\n    ctx.add_basemap(ax, crs=traj.crs, \n                    source=source)\n    ax.set_axis_off()\n    if title: \n        ax.set_title(title)\n    else:\n        ax.set_title(\n            f'Walk in {traj.df.Name[0]} on {traj.df.index[0].strftime('%x')}'\n        )\n    return f, ax\n\nannot_props = dict(\n    xytext=(3, 1), \n    textcoords=\"offset fontsize\", \n    c='r', weight='bold', ha='center',\n    arrowprops=dict(arrowstyle='-', ec='orange')\n)\n\ndf = pd.read_csv('data/b3/combined.csv')\ngdf = gpd.GeoDataFrame(\n    df, \n    geometry=gpd.points_from_xy(x=df.Lon, y=df.Lat), \n    crs=4269\n).to_crs(32111)\nMovingPandas requires an integer id for each walk, which I don’t have, so I’ll create one based on the string id. I’ll also convert the Time from a generic object to datetime64.\nidList = list(gdf.groupby(['Id']).nunique().reset_index().Id)\nfor i, track in enumerate(idList):\n    gdf.loc[gdf.Id == track, 'trajectory_id'] = i\ngdf['Time'] = pd.to_datetime(gdf.Time)\ngdf.info()\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 3044 entries, 0 to 3043\nData columns (total 10 columns):\n #   Column         Non-Null Count  Dtype              \n---  ------         --------------  -----              \n 0   Id             3044 non-null   object             \n 1   Name           3044 non-null   object             \n 2   Lat            3044 non-null   float64            \n 3   Lon            3044 non-null   float64            \n 4   Elev           3044 non-null   int64              \n 5   Time           3044 non-null   datetime64[ns, UTC]\n 6   Temp           3044 non-null   float64            \n 7   Weather        3044 non-null   int64              \n 8   geometry       3044 non-null   geometry           \n 9   trajectory_id  3044 non-null   float64            \ndtypes: datetime64[ns, UTC](1), float64(4), geometry(1), int64(2), object(2)\nmemory usage: 237.9+ KB\nOne of the walks in the collection was in New York City, and the rest in New Jersey. Since the locations are pretty far apart, I’ll split out the NYC walk from the others.\ngdf_nj = gdf.loc[gdf.Name != 'New York']\ngdf_ny = gdf.loc[gdf.Name == 'New York']\ngdf_nj.shape, gdf_ny.shape\n((2063, 10), (981, 10))",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Points to Paths in Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#getting-going-with-movingpandas",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#getting-going-with-movingpandas",
    "title": "Points to Paths in Python",
    "section": "",
    "text": "Now I’m ready to create the trajectories. The data will be imported as a Trajectory Collection which will contain the individual Trajectories. I’ll first create one from the NJ group, which contains several walks.\ntc_nj = mpd.TrajectoryCollection(\n    gdf_nj, 'trajectory_id', \n    t='Time', x='Lon', y='Lat'\n)\nprint(tc_nj)\nTrajectoryCollection with 6 trajectories\nLet’s see what this looks like.\nax = traj_collection.plot(\n    column='trajectory_id', \n    legend=True, figsize=(9,5), cmap='Set1')\nax.set_title('Walks in NJ, September 2024')\nctx.add_basemap(ax, crs=gdf_proj.crs,\n                source=ctx.providers.Esri.WorldTopoMap);\n\nI can extract a single trajectory and its underlying data frame.\ntraj_nj1 = tc_nj.trajectories[2]\nprint(traj_nj1)\nTrajectory 2.0 (2024-09-07 21:20:23.975000 to 2024-09-07 22:03:42.022000) | Size: 267 | Length: 5173.1m\nBounds: (177912.1946985984, 243124.49067874879, 179216.17084672116, 245408.85808260852)\nLINESTRING (178912.27991325516 244815.1747420021, 178912.27991325516 244815.1747420021, 178903.67850\nThe associated data is stored in the df attribute:\nprint(traj_nj1.df.info())\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nDatetimeIndex: 267 entries, 2024-09-07 21:20:23.975000 to 2024-09-07 22:03:42.022000\nData columns (total 9 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   Id             267 non-null    object  \n 1   Name           267 non-null    object  \n 2   Lat            267 non-null    float64 \n 3   Lon            267 non-null    float64 \n 4   Elev           267 non-null    int64   \n 5   Temp           267 non-null    float64 \n 6   Weather        267 non-null    int64   \n 7   geometry       267 non-null    geometry\n 8   trajectory_id  267 non-null    float64 \ndtypes: float64(4), geometry(1), int64(2), object(2)\nmemory usage: 29.0+ KB\nNone\nt_plot(traj_nj1, source=ctx.providers.OpenStreetMap.HOT);\n\nAnother way to extract a specific trajectory is using MovingPandas’ filter function, as so:\nt_plot(\ntc_nj.filter(\"Name\", [\"Park Ridge\"])\n     .trajectories[0]\n);\n\nFor the rest of the article I’ll use the longest trek I have in this data set, the one in New York. I’ll create the Trajectory Collection, extract the single trajectory and find the start and end times and locations.\ntc_ny = mpd.TrajectoryCollection(\n    gdf_ny, 'trajectory_id', \n    t='Time', x='Lon', y='Lat'\n)\ntraj_ny = tc_ny.trajectories[0]\nst = traj_ny.get_start_time()\nsl = traj_ny.to_crs(4269).get_start_location()\net = traj_ny.get_end_time()\nel = traj_ny.to_crs(4269).get_end_location()\nprint(f'''\n            Time         Location (Lon/Lat)\n            \n    Start: {st.strftime('%X')}   {sl.coords[0]} degrees\n    End:   {et.strftime('%X')}   {el.coords[0]}  degrees\n\n    Duration:  {traj_ny.get_duration().seconds/60:.0f} minutes\n    Distance:  {traj_ny.get_length(units='mi'):.2f} miles\n    Direction: {traj_ny.get_direction():.0f} degrees\n''')\n            Time         Location (Lon/Lat)\n            \n    Start: 17:43:42   (-73.991157, 40.752025) degrees\n    End:   20:49:09   (-73.98894, 40.726903)  degrees\n\n    Duration:  185 minutes\n    Distance:  6.69 miles\n    Direction: 176 degrees\nt_plot(traj_ny);",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Points to Paths in Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#right-place-right-time",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#right-place-right-time",
    "title": "Points to Paths in Python",
    "section": "",
    "text": "First, let me find my position at an arbitrary time, and then 30 minutes later. Where was I, for example, at 18:00, and then 45 minutes later?\nt = datetime(2024, 9, 11, 18)\nt2 = t + timedelta(minutes=45)\nprint(f'''\n    Nearest:      {traj_ny.to_crs(4269)\n                    .get_position_at(t, method='nearest')}\n    Interpolated: {traj_ny.to_crs(4269)\n                    .get_position_at(t, method='interpolated')}\n    Previous row: {traj_ny.to_crs(4269)\n                    .get_position_at(t, method='ffill')}\n    Next row:     {traj_ny.to_crs(4269)\n                    .get_position_at(t, method='bfill')}\n''')\n    Nearest:      POINT (-74.003574 40.756368)\n    Interpolated: POINT (-74.00361304734385 40.75638409846633)\n    Previous row: POINT (-74.003688 40.756415)\n    Next row:     POINT (-74.003574 40.756368)\nI can use the geocode tool provided by geopandas to get the nearest addresses at these points so I can show them on the map. Pay attention to crs, since nominatim expects lat/lon coordinates and we currently have projected coordinates.\npoint = traj_ny.get_position_at(t, method='interpolated')\npoint_up = traj_ny.to_crs(4269).get_position_at(t, method='interpolated')\npoint2 = traj_ny.get_position_at(t2, method='interpolated')\npoint2_up = traj_ny.to_crs(4269).get_position_at(t2, method='interpolated')\n\nrg = reverse_geocode(\n    [point_up, point2_up], \n    provider=\"nominatim\", \n    user_agent=\"your_project\", \n    timeout=10\n).to_crs(32111)\n\nf, ax = t_plot(traj_ny)\ngpd.GeoSeries(point).plot(ax=ax, color='red', markersize=100)\ngpd.GeoSeries(point2).plot(ax=ax, color='red', markersize=100)\nfor x, y, label in zip(rg.geometry.x, rg.geometry.y, rg.address):\n    ax.annotate(f'{label.split(',')[0]}, {label.split(',')[1]}', \n                xy=(x, y), **annot_props)\nax.set_ylim(211500, 214500)\nax.set_xlim(191400, 192500);\n\nIt might be more interesting to plot my location at regular intervals along the walk. What about plotting my location every 30 minutes?\nint_count = (et - st) // timedelta(seconds=(60*30))\nint_length = (et - st) / int_count\nintervals = [st + (i*int_length) for i in range(int_count)]\ninterval_points = [traj_ny.to_crs(4269).get_position_at(\n                    t, method='interpolated') \n                   for t in intervals]\nrg = reverse_geocode(\n    interval_points, \n    provider=\"nominatim\", \n    user_agent=\"your_project\", \n    timeout=10\n).to_crs(32111)\n\nf, ax = t_plot(traj_ny, figsize=(7,11))\nrg.plot(ax=ax, c='r', markersize=100)\nfor x, y, label in zip(rg.geometry.x, rg.geometry.y, rg.address):\n    ax.annotate(f'{label.split(',')[0]}, {label.split(',')[1]}', \n                xy=(x, y), **annot_props)\nax.set_title(ax.get_title() + f'\\nEvery 30 minutes');",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Points to Paths in Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#i-was-so-close",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#i-was-so-close",
    "title": "Points to Paths in Python",
    "section": "",
    "text": "Not only can you calculate distances along the path, which we will look at more in the next article, but you can calculate the distance from points along the trajectory to other locations not on the path. It will determine the closest you came to other points, lines or polygons, including, of course, other trajectories. For example, when I’m in New York, I love to visit the Strand and the huge Barnes and Noble on Union Square. I didn’t have time this trip, sadly, but I’m curious how close I was from those stores.\naddresses = [{\n                'id': 1,\n                'name': \"The Strand\", \n                'addr': \"828 Broadway, New York, NY 10003\"\n            }, \n            {\n                'id': 2,\n                'name': \"Barnes and Noble\", \n                'addr': \"33 East 17th Street, 10003, New York\"\n            }]\nadd_df = pd.DataFrame(addresses)\ngeo = geocode(\n    add_df['addr'], \n    provider='nominatim', \n    user_agent='your_project', \n    timeout=10\n)\nbook_stores = geo.join(add_df).to_crs(32111)\nprint(book_stores)\n                        geometry  \\\n0  POINT (192997.822 211055.516)   \n1  POINT (193108.568 211461.041)   \n\n                                             address  id              name  \\\n0  Strand Bookstore, 828, Broadway, University Vi...   1        The Strand   \n1  Barnes & Noble, 33, East 17th Street, Union Sq...   2  Barnes and Noble   \n\n                                   addr  \n0      828 Broadway, New York, NY 10003  \n1  33 East 17th Street, 10003, New York\ndists_to = [traj_ny.distance(book_stores.loc[i, ['geometry']], \n                            units='mi')[0] \n            for i in range(len(book_stores))]\nstore_names = [book_stores.loc[i, 'name'] \n               for i in range(len(book_stores))]\nfor i in range(len(book_stores)):\n    print(f'Closest distance to {store_names[i]} was {dists_to[i]:.1f} miles') \nClosest distance to The Strand was 0.4 miles\nClosest distance to Barnes and Noble was 0.6 miles\nf, ax = t_plot(traj_ny, figsize=(7,11))\nbook_stores.plot(ax=ax, color='red', markersize=100)\nfor x, y, label in zip(book_stores.geometry.x, \n                       book_stores.geometry.y, \n                       book_stores.name):\n    ax.annotate(label, xy=(x,y), **annot_props)",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Points to Paths in Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#stop-detection",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#stop-detection",
    "title": "Points to Paths in Python",
    "section": "",
    "text": "In this final section I’ll look at stop detection. Here I’ll just be considering stop locations. In the next article, I’ll show how to use the same approach as one method for segmenting the trajectories. Stop detection requires instantiation of a stop detector which will provide the functions needed.\ndetector = mpd.TrajectoryStopDetector(traj_ny)\nThe detector takes parameters to determine how long a pause constitutes a stop, and how far you need to move to constitute movement.\nstop_points = detector.get_stop_points(\n    min_duration=timedelta(seconds=120), max_diameter=100\n)\nlen(stop_points)\n18\nWe can look up the addresses nearest to the stop points, and plot some of them on the map.\nstop_points = stop_points.set_crs(32111)\nrg = reverse_geocode(\n    stop_points.to_crs(4269).geometry, \n    provider=\"nominatim\", \n    user_agent=\"your_project\", timeout=10\n).to_crs(32111)\nf, ax = t_plot(traj_ny)\nstop_points.plot(ax=ax, color='none', markersize=100, ec='r')\npoint_labels = [0,3,7,11,17]\nfor (x, y), label in zip(\n                    rg.iloc[point_labels].geometry.apply(\n                        lambda p: p.coords[0]), \n                    rg.iloc[point_labels].address):\n    ax.annotate(f'{label.split(',')[0]}, {label.split(',')[1]}', \n                xy=(x, y), **annot_props)\nax.set_title(ax.get_title() + f'\\nStop Points');",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Points to Paths in Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#interactive-graphing",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#interactive-graphing",
    "title": "Points to Paths in Python",
    "section": "",
    "text": "Showing interactive graphing in the context of a published article is not very useful, but I should point out that MovingPandas supports both hvplot with folium as well as GeoPandas’ explore(), both of which can be useful tools for identifying areas of interest. This is an example with explore().\nm = traj_ny.explore(\n    column=\"trajectory_id\",\n    tooltip=\"t\",\n    popup=True,\n    style_kwds={\"weight\": 4},\n    name=\"Time\",\n)\n\nstop_points.explore(\n    m=m,\n    color=\"red\",\n    tooltip=\"stop_id\",\n    popup=True,\n    marker_kwds={\"radius\": 10},\n    name=\"Stop points\",\n)",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Points to Paths in Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#next-steps",
    "href": "posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/index.html#next-steps",
    "title": "Points to Paths in Python",
    "section": "",
    "text": "In the third article, I will address issues of generalization and simplification which can be important when working with large data sets. I’ll show how to generate speed, duration and distance information, and how to segment the trajectories based on geometries, stops, and other criteria. I hope that these articles are proving interesting and useful. Please visit my website and repository for more code, articles and updates.",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Points to Paths in Python"
    ]
  },
  {
    "objectID": "posts/data-science/data-science.html",
    "href": "posts/data-science/data-science.html",
    "title": "Data Science",
    "section": "",
    "text": "Nix for Data Scientists\n\n\nNix your venvs and skip pip\n\n\n\nLinux\n\nData Science\n\nNix\n\nPython\n\n\n\nInstalling and maintaining multiple versions of libraries, whether in Python or R, is best managed with Nix\n\n\n\n\n\nMay 29, 2025\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nHispanics in New Mexico\n\n\nA demographic study with R and TidyCensus\n\n\n\nR\n\nData Science\n\nGIS\n\n\n\nNew Mexico has a Latino population unlike anywhere else in the US. This article shows how to use R and tidycensus to explore the data.\n\n\n\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGPS Mapping with R\n\n\n\n\n\n\nGIS\n\nR\n\nPython\n\nData Science\n\n\n\nA Comparison with Python\n\n\n\n\n\nMar 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSpeed, Simplification and Segments\n\n\n\n\n\n\nGIS\n\nPython\n\nData Science\n\n\n\nWorking with MovingPandas\n\n\n\n\n\nFeb 25, 2025\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nPoints to Paths in Python\n\n\n\n\n\n\nData Science\n\nGIS\n\nPython\n\n\n\nCreating trajectories in MovingPandas\n\n\n\n\n\nFeb 3, 2025\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nTrail Mapping with Python\n\n\n\n\n\n\nGIS\n\nPython\n\nData Science\n\n\n\nUsing your GPX data with geopandas\n\n\n\n\n\nFeb 28, 2024\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian and Jupyter Notebooks\n\n\n\n\n\n\nObsidian\n\nPython\n\nData Science\n\n\n\nPersonal Knowledge Management for Data Science\n\n\n\n\n\nMar 20, 2023\n\n\nBrian Carey\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "Data Science"
    ]
  },
  {
    "objectID": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html",
    "href": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html",
    "title": "GPS Mapping with R",
    "section": "",
    "text": "I recently published a series of articles on analyzing GPS data from personal sport tracking software using Python. I’ve started using R again lately and, while I like Python, I really like R. R is not a general-purpose language like Python, and is therefore far less commonly studied. It was mission-built for this type of work, and, as is often the case with custom-built tools, it is powerful, comfortable, and, frankly, fun to work with. I thought it might be interesting to compare the process in Python described in the previous articles with the process in R.",
    "crumbs": [
      "Data Science",
      "R Gpx Mapping",
      "GPS Mapping with R"
    ]
  },
  {
    "objectID": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#libraries",
    "href": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#libraries",
    "title": "GPS Mapping with R",
    "section": "Libraries",
    "text": "Libraries\nPython data frames are built on the pandas and numpy libraries, with matplotlib as the primary plotting tool. Data frames and vector processing are native to R. The dplyr library provides convenience functions for manipulating data. The amazing gplot2 provides the functionality of matplotlib and seaborn and more, with simple syntax. The sf package, which stands for Simple Features, provides the geometry and geospatial functionality which geopandas and shapely do in Python. I’ll use ggspatial here for basemap tiles where I used contextily in Python, and I’ll add patchwork for convenient side-by-side display.\n\nlibrary(gpx)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggspatial)\nlibrary(patchwork)",
    "crumbs": [
      "Data Science",
      "R Gpx Mapping",
      "GPS Mapping with R"
    ]
  },
  {
    "objectID": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#syntax",
    "href": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#syntax",
    "title": "GPS Mapping with R",
    "section": "Syntax",
    "text": "Syntax\nA few notes about syntax to start off: R uses the &lt;- arrow for assignment, but accepts = as well. Data frame slicing uses the same [row,column] approach with explicit or boolean values. Unlike pandas, which distinguishes between df.loc and df.iloc, in R you can slice using numeric indices or strings without distinction. On a similar topic, R uses one-indexing, but is inclusive of the end, so df[1:2] in R is equivalent to Python’s df.iloc[:2], or more explicitly df.iloc[0:2], and the second element is directly accessed with df[2] instead of df.iloc[1].\nThere’s an important twist to this which will catch you multiple times. Even though the syntax is [row,column], if you supply only one numeric index, without a comma, it will be interpreted as a column index. So df[1] gets the first column, while df[1, ] gets the first row.\nThe main syntactical difference is R’s extensive use of piping, using the pipe operator |&gt;, sometimes written as %&gt;%. This looks similar to accessing a series of an object’s methods through a chain of .s in Python, but it isn’t. The pipe in R works like the pipe in a Linux shell, simply passing the output of one function to the next function as its first argument. This is one of my personal favorite aspects of working in R, since it allows for natural expression of a series of steps which constitue a workflow. ggplot2 takes a similar syntactical approach, layering elements of the plot by chaining using the + operator.",
    "crumbs": [
      "Data Science",
      "R Gpx Mapping",
      "GPS Mapping with R"
    ]
  },
  {
    "objectID": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#loading-the-data",
    "href": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#loading-the-data",
    "title": "GPS Mapping with R",
    "section": "Loading the data",
    "text": "Loading the data\nLet’s get started. With Python, we needed to parse the raw gpx data, which is in an XML format, to a CSV formatted file, which could then be imported into a pandas data frame, and then turned that into a geopandas data frame. I used beautifulsoup to do so. R, fortunately, has a gpx library that allows us to go straight from gpx into a data frame. Let’s see what that looks like. The str() command will let us know what’s inside.\n\ntrek_data &lt;- read_gpx(\"data/b3/Workout-2024-09-06-16-29-37.gpx\")\nstr(trek_data)\n\nList of 3\n $ routes   :List of 1\n  ..$ :'data.frame':    0 obs. of  4 variables:\n  .. ..$ Elevation: logi(0) \n  .. ..$ Time     : logi(0) \n  .. ..$ Latitude : logi(0) \n  .. ..$ Longitude: logi(0) \n $ tracks   :List of 1\n  ..$ River Vale:'data.frame':  137 obs. of  6 variables:\n  .. ..$ Elevation : num [1:137] -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 ...\n  .. ..$ Time      : POSIXct[1:137], format: \"2024-09-06 16:29:37\" \"2024-09-06 16:29:37\" ...\n  .. ..$ Latitude  : num [1:137] 41 41 41 41 41 ...\n  .. ..$ Longitude : num [1:137] -74 -74 -74 -74 -74 ...\n  .. ..$ extensions: logi [1:137] NA NA NA NA NA NA ...\n  .. ..$ Segment ID: int [1:137] 1 1 1 1 1 1 1 1 1 1 ...\n $ waypoints:List of 1\n  ..$ :'data.frame':    0 obs. of  4 variables:\n  .. ..$ Elevation: logi(0) \n  .. ..$ Time     : logi(0) \n  .. ..$ Latitude : logi(0) \n  .. ..$ Longitude: logi(0) \n\n\nThe result is not a data frame, but a list of lists. The second one, called tracks, is the only one with observations, so we can start with that. Don’t forget that R does not zero-index lists, so we use 2 not 1, and extract it with double square brackets.\n\ntrek_tracks &lt;- trek_data[[2]]\nstr(trek_tracks)\n\nList of 1\n $ River Vale:'data.frame': 137 obs. of  6 variables:\n  ..$ Elevation : num [1:137] -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 ...\n  ..$ Time      : POSIXct[1:137], format: \"2024-09-06 16:29:37\" \"2024-09-06 16:29:37\" ...\n  ..$ Latitude  : num [1:137] 41 41 41 41 41 ...\n  ..$ Longitude : num [1:137] -74 -74 -74 -74 -74 ...\n  ..$ extensions: logi [1:137] NA NA NA NA NA NA ...\n  ..$ Segment ID: int [1:137] 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThis gets us closer, now we have list of one. Let’s pull that out and display the first two rows.\n\ntrek &lt;- trek_tracks[[1]]\ntrek[1:2,]\n\n  Elevation                Time Latitude Longitude extensions Segment ID\n1        -4 2024-09-06 16:29:37 41.01128  -74.0101         NA          1\n2        -4 2024-09-06 16:29:37 41.01128  -74.0101         NA          1\n\n\nNote the comma, which is very important. If only one value is supplied, it chooses columns instead of rows.\n\nhead(trek[1:2], 2)\n\n  Elevation                Time\n1        -4 2024-09-06 16:29:37\n2        -4 2024-09-06 16:29:37\n\n\nAnd the final frame looks like:\n\nstr(trek)\n\n'data.frame':   137 obs. of  6 variables:\n $ Elevation : num  -4 -4 -4 -4 -4 -4 -4 -4 -4 -4 ...\n $ Time      : POSIXct, format: \"2024-09-06 16:29:37\" \"2024-09-06 16:29:37\" ...\n $ Latitude  : num  41 41 41 41 41 ...\n $ Longitude : num  -74 -74 -74 -74 -74 ...\n $ extensions: logi  NA NA NA NA NA NA ...\n $ Segment ID: int  1 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "Data Science",
      "R Gpx Mapping",
      "GPS Mapping with R"
    ]
  },
  {
    "objectID": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#importing-a-collection-of-treks",
    "href": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#importing-a-collection-of-treks",
    "title": "GPS Mapping with R",
    "section": "Importing a collection of treks",
    "text": "Importing a collection of treks\nNow that I know “where” the information is, I can go ahead and import a series of files and combine them into a single data frame. As I did with Python, I will assign a unique identifier to each trek, and then combine them. The R equivalent of df.append in pandas is the aptly-named bind_rows() from the dplyr library.\n\nfiles &lt;- list.files(path = \"data/b3/\", \n                    pattern = \"\\\\.gpx$\",\n                    full.names = TRUE)\ndata_list &lt;- list()\ni &lt;- 1\nfor (file in files) {\n  data &lt;- read_gpx(file)[[2]][[1]]\n  data$id = i\n  i &lt;- i + 1\n  data_list[[file]] &lt;- data\n}\ntracks_df &lt;- dplyr::bind_rows(data_list)\nstr(tracks_df)\n\n'data.frame':   3044 obs. of  7 variables:\n $ Elevation : num  79 79 77 76 75 74 72 71 70 70 ...\n $ Time      : POSIXct, format: \"2024-09-05 15:25:13\" \"2024-09-05 15:25:13\" ...\n $ Latitude  : num  41 41 41 41 41 ...\n $ Longitude : num  -74.1 -74.1 -74.1 -74.1 -74.1 ...\n $ extensions: logi  NA NA NA NA NA NA ...\n $ Segment ID: int  1 1 1 1 1 1 1 1 1 1 ...\n $ id        : num  1 1 1 1 1 1 1 1 1 1 ...\n\n\nNote the use of $ to access the columns. An alternate syntax, data['id'] is available for column names with spaces. This is like pandas use of the . operator.",
    "crumbs": [
      "Data Science",
      "R Gpx Mapping",
      "GPS Mapping with R"
    ]
  },
  {
    "objectID": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#data-frame-to-geo-data",
    "href": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#data-frame-to-geo-data",
    "title": "GPS Mapping with R",
    "section": "Data frame to geo-data",
    "text": "Data frame to geo-data\nIn Python, we turned this into a geo-enabled data frame with:\n```{python}\ntrek_gdf = gpd.GeoDataFrame( \n    trek_df, \n    geometry=gpd.points_from_xy(x=trek_df.Lon, y=trek_df.Lat)\n).set_crs(4269)\ntrek_gdf.info()\n```\nR uses the sf::st_as_sf() for this purpose, with much less verbose syntax.\n\ntreks &lt;- tracks_df |&gt; \n  st_as_sf(coords = c(\"Longitude\",\"Latitude\"),\n           crs = 4326)\n\n\nstr(treks)\n\nClasses 'sf' and 'data.frame':  3044 obs. of  6 variables:\n $ Elevation : num  79 79 77 76 75 74 72 71 70 70 ...\n $ Time      : POSIXct, format: \"2024-09-05 15:25:13\" \"2024-09-05 15:25:13\" ...\n $ extensions: logi  NA NA NA NA NA NA ...\n $ Segment ID: int  1 1 1 1 1 1 1 1 1 1 ...\n $ id        : num  1 1 1 1 1 1 1 1 1 1 ...\n $ geometry  :sfc_POINT of length 3044; first list element:  'XY' num  -74.1 41\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA\n  ..- attr(*, \"names\")= chr [1:5] \"Elevation\" \"Time\" \"extensions\" \"Segment ID\" ...\n\n\nAs you can see, we now have a geometry column and the Time column was assigned the appropriate type, which we had to do as an additional step in Python. I’ll grab the first trek, then plot it with ggplot. Using ggplot entails adding elements one by one.\n\ntrek &lt;- treks[treks$id == 1,]\n\n\nggplot(data = trek) +\n  ggspatial::annotation_map_tile(      \n    type = \"osm\",\n    cachedir = \"maps/\",\n    zoomin = -1) +\n  geom_sf(aes(color = Elevation)) +\n  scale_color_viridis_c() +\n  theme_void() +\n  labs(\n    title = paste(\"Elevation Profile: Park Ridge\", as.Date(trek[1,]$Time)),\n    caption = \"Tiles © OpenStreetMap (ODBL license)\"\n  )\n\n\n\n\n\n\n\n\nIn this case, we pass the data frame (actually an sf object) to ggplot, then layer on the basemap and the actual points using geom_sf, passing it the column we want to color using the aes() funtion. “aes” is short for aesthetics, and it is where you specify columns to be used for that element. The details of this vary depending on the geometry. theme_void() removes axis ticks, and the rest should be self-explanatory.\nWe still must change the coordinate system to get appropriate distances. In R, this is\n\ntrek_proj &lt;- trek |&gt; st_transform(crs = 32113)\n\nIn Python, we did\n```{python}\ntrek_proj = trek_gdf.to_crs(32111)\n```\nWhile the syntax looks similar, Python is accessing a method of the trek object, while R is simply passing the output of one function to another.",
    "crumbs": [
      "Data Science",
      "R Gpx Mapping",
      "GPS Mapping with R"
    ]
  },
  {
    "objectID": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#elevations",
    "href": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#elevations",
    "title": "GPS Mapping with R",
    "section": "Elevations",
    "text": "Elevations\nI won’t move beyond points until the next article, but in the first Python one I did calculate the distance, so to be fair I will do so here. I’ll explain the code later, but it is a good example of piping in R.\n\ndistance &lt;- trek |&gt; \n  st_transform(crs = 32113) |&gt; \n  dplyr::summarise(do_union = F) |&gt; \n  st_cast(\"LINESTRING\") |&gt; \n  st_length()\nprint(paste(\"Total distance\", round(distance/1000, digits = 1), \"kilometers\"))\n\n[1] \"Total distance 7.7 kilometers\"\n\n\nLet’s see the other basic information. I will put it in a data frame for a nice printout.\n\ndata.frame(\n  \"Elevation\" = c(\"Maximum\",\n             \"Minimum\",\n             \"Average\",\n             \"Initial\",\n             \"Final\"),\n  \"Meters\" = c(max(trek$Elevation),\n               min(trek$Elevation),\n               round(mean(trek$Elevation)),\n               trek[1,]$Elevation,\n               trek[length(trek),]$Elevation)\n)\n\n  Elevation Meters\n1   Maximum     81\n2   Minimum     11\n3   Average     45\n4   Initial     79\n5     Final     74\n\n\nThe c() function is used to create a vector (not a list), so I specify each column name and assign it a vector with data, ensuring that each vector has the same length, of course. Note how I access the last row of trek. There are multiple ways of doing this in R, but sadly negative indexing is not one. This is the most efficient. I will say that I miss Python’s f-string, especially with literals (f\"\"\" \"\"\").\nNow we can use ggplot for a line chart of the elevation.\n\nggplot(trek, aes(x = Time, y = Elevation)) +\n  geom_line() +\n  ggtitle(paste(\"Elevation Profile for Park Ridge Walk on\", as.Date(trek[1,]$Time)))\n\n\n\n\n\n\n\n\nAnd here are the smoothed profiles using two different methods, and display them side by side with the patchwork library.\n\nlibrary(patchwork)\ngam_plot &lt;- ggplot(trek, aes(x = Time, y = Elevation)) +\n  geom_line() +\n  geom_smooth(method = \"gam\") +\n  ggtitle(paste(\"NJ (gam smoothing)\", as.Date(trek[1,]$Time)))\n\nloess_plot &lt;- ggplot(trek, aes(x = Time, y = Elevation)) +\n  geom_line() +\n  geom_smooth(method = \"loess\") +\n  ggtitle(paste(\"NJ (loess smoothing)\", as.Date(trek[1,]$Time)))\n\ngam_plot | loess_plot\n\n\n\n\n\n\n\n\nggplot2’s approach of adding layers makes code clear and easy to write, and does not have the odd fig, ax mechanism which can be confusing.",
    "crumbs": [
      "Data Science",
      "R Gpx Mapping",
      "GPS Mapping with R"
    ]
  },
  {
    "objectID": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#next-steps",
    "href": "posts/data-science/r-gpx-mapping/trail-mapping-with-r/index.html#next-steps",
    "title": "GPS Mapping with R",
    "section": "Next steps",
    "text": "Next steps\nI hope you enjoyed this exercise, no pun intended. The code for this is on my websites GitHub repository. I’ll come out with part 2 soon. Happy coding!",
    "crumbs": [
      "Data Science",
      "R Gpx Mapping",
      "GPS Mapping with R"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping.html",
    "href": "posts/data-science/python-gpx-mapping.html",
    "title": "GPX Trail Mapping in Python",
    "section": "",
    "text": "Trail Mapping with Python\n\n\n\nGIS\n\nPython\n\nData Science\n\n\n\nUsing your GPX data with geopandas\n\n\n\n\n\nFeb 28, 2024\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nPoints to Paths in Python\n\n\n\nData Science\n\nGIS\n\nPython\n\n\n\nCreating trajectories in MovingPandas\n\n\n\n\n\nFeb 3, 2025\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nSpeed, Simplification and Segments\n\n\n\nGIS\n\nPython\n\nData Science\n\n\n\nWorking with MovingPandas\n\n\n\n\n\nFeb 25, 2025\n\n\nBrian Carey\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "Data Science",
      "GPX Trail Mapping in Python"
    ]
  },
  {
    "objectID": "blogs/dataviewjs-interactive-dynamic-tables/index.html",
    "href": "blogs/dataviewjs-interactive-dynamic-tables/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/dataview/dataviewjs-interactive-dynamic-tables/.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/regex-search-in-line-metadata/index.html",
    "href": "blogs/regex-search-in-line-metadata/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/regex-search-in-line-metadata.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/pretty-canvas/index.html",
    "href": "blogs/pretty-canvas/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/pretty-canvas.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/git-github-obsidian/index.html",
    "href": "blogs/git-github-obsidian/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https://biscotty.online/posts/obsidian/git-github-obsidian.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/graph-view-deets/index.html",
    "href": "blogs/graph-view-deets/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/graph-view-deets.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/metadata-menu/index.html",
    "href": "blogs/metadata-menu/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/metadata-menu.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/obsidian-bookmarks/index.html",
    "href": "blogs/obsidian-bookmarks/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/obsidian-headers.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/gpx-gps-data/index.html",
    "href": "blogs/gpx-gps-data/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https://biscotty.online/posts/data-science/python-gpx-mapping/gpx-gps-data.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/optimal-notes-with-obsidian/index.html",
    "href": "blogs/optimal-notes-with-obsidian/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/optimal-notes-with-obsidian.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cv/brian-carey.html",
    "href": "cv/brian-carey.html",
    "title": "Brian Carey",
    "section": "",
    "text": "I have an unusually diverse range of skills, knowledge and experience. From my first job as a stock boy at Kmart, where I was asked to join the Point of Sale implementation team at the store, later traveling to other stores as part of the corporate team, most of my experience revolves around the themes of new technology, process development and teaching.\nI am project-oriented. In every professional role I’ve held, I have had to address challenges which required the development and introduction of new processes and technologies. Once things are running smoothly, or goals have been met, I’m ready to move on.\nWith the exception of Chinese Medicine and foreign languages, I have had no formal education in any of these topics, but over this time, I’ve acquired various knowledge and skills, including.\nHere are some significant achievements from my professional life.",
    "crumbs": [
      "Home",
      "Brian Carey"
    ]
  },
  {
    "objectID": "cv/brian-carey.html#footnotes",
    "href": "cv/brian-carey.html#footnotes",
    "title": "Brian Carey",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPython, R, JavaScript, Haskell, Perl↩︎\nIncluding international complex corporate structures↩︎\nCollege professor, writer, conducted internal trainings, workshops↩︎\nI speak English, French, Spanish and Italian. I have studied Russian, Chinese, Yoruba and Egyptian hieroglyphics↩︎\ncorporate, academic, non-profit↩︎",
    "crumbs": [
      "Home",
      "Brian Carey"
    ]
  },
  {
    "objectID": "cv/vignettes/win-win-negotiation/index.html",
    "href": "cv/vignettes/win-win-negotiation/index.html",
    "title": "High stakes negotiation",
    "section": "",
    "text": "As the Worldwide Vendor Manager for localization services at Autodesk, negotiation was a regular part of my job. My primary goals in that role were to control costs and cultivate vendors qualified to provide the services we require. By having an adequately large pool, competition made pricing negotiations pretty easy. On the other hand, when things go wrong on a project, negotiation becomes much more complex.\nHere I will briefly describe a high-stakes negotiation I led, and then describe how I approached this difficult situation.",
    "crumbs": [
      "CV",
      "Vignettes",
      "High stakes negotiation"
    ]
  },
  {
    "objectID": "cv/vignettes/win-win-negotiation/index.html#background",
    "href": "cv/vignettes/win-win-negotiation/index.html#background",
    "title": "High stakes negotiation",
    "section": "Background",
    "text": "Background\nAutodesk was flying high after the twelfth release of its flagship product AutoCAD, which had reinforced its dominant position in the computer aided design space. I was working in Europe during the release of the subsequent R13. It was a pretty disastrous release cycle in many ways, beginning upstream, and upstream chaos multiplies downstream as the product is localized into multiple languages. Most of the localization work for Europe was done by a single vendor, which was one of the few large-scale vendors, operating out of London at the time.\nAfter the project was over, the internal team wanted compensation, almost retribution. Clearly comependsation was due, although, on the facts and history, there was certainly an aspect of shared responsibility. I had only recently been instituted in the newly-created role of vendor manager, and was responsible for the negotiations. I had previously been the finance and administration partner to the software center, so I was already familiar with the financial aspects.",
    "crumbs": [
      "CV",
      "Vignettes",
      "High stakes negotiation"
    ]
  },
  {
    "objectID": "cv/vignettes/win-win-negotiation/index.html#be-prepared",
    "href": "cv/vignettes/win-win-negotiation/index.html#be-prepared",
    "title": "High stakes negotiation",
    "section": "Be prepared",
    "text": "Be prepared\nThere’s no better tactic than having facts at your fingertips and being knowledgeable about the details of a situation, information which, often, those sitting across the table do not possess.\nPreparation is the most important part of any negotiation, especially when emotions are running high. The internal situation must be understood, and the impact of internal chaos on the vendor must be recognized. The vendor’s positions must be clarified in so far as possible, and an understanding of the internal dynamics of the vendor’s company should be explored as well to predict their arguments and broader interests.",
    "crumbs": [
      "CV",
      "Vignettes",
      "High stakes negotiation"
    ]
  },
  {
    "objectID": "cv/vignettes/win-win-negotiation/index.html#relative-value",
    "href": "cv/vignettes/win-win-negotiation/index.html#relative-value",
    "title": "High stakes negotiation",
    "section": "Relative value",
    "text": "Relative value\nNegotiation is not a zero-sum game. There are aspects which can be clearly quantified, of course, which can only be awarded to one side, but there are other aspects to whose value is not so easily assigned a dollar amount. It is the latter which should preoccupy the negotiator, since it is there that a zero sum game can be turned into a win-win situation.\nWhere a value is not objective, it is subjective, and each party assigns a different value to each point in contention. Effort should be made to estimate the value that the other party places on each point. Whenever a point is conceded, there is an exchange of value. Things that are of low value to me but high value to my opponent are tactically very useful, they are the low-hanging fruit. Furthermore, one should consider bringing new issues to the table which are of low value to oneself but high value to the opponent, because these are powerful bargaining chips. Alternatively, if a point is of high value to me, yet I recognize it as low value for the opponent, I will not give over-large concessions.",
    "crumbs": [
      "CV",
      "Vignettes",
      "High stakes negotiation"
    ]
  },
  {
    "objectID": "cv/vignettes/win-win-negotiation/index.html#internal-homework",
    "href": "cv/vignettes/win-win-negotiation/index.html#internal-homework",
    "title": "High stakes negotiation",
    "section": "Internal homework",
    "text": "Internal homework\nWhen a project goes poorly, it is natural to assign blame and holler for retribution. There are two parts to compensation: actual damages and punishment. In my opinion, punitive damages are normally not in the best interests of either party. As the saying goes “it’s not personal, it’s business.”\nI interviewed the department manager, project leads, engineers and translators involved in particularly problematic aspects of the project. This gave me the data I needed to establish a strong case, while sifting out the more emotional and CYA aspects of their complaints.\nI considered my “internal customers” responsibility to be the director of the software center, who was my direct report, and the director of finance and administration for Europe. Of course, I also answered to the project teams who relied on me for the services they needed, and had my own interests, namely keeping an important resource available in the future.\nAt a management level, expectations for the outcome needed to be clearly laid out, both financial and non-tangible issues. In practice, this means agreeing on three outcomes: the ideal case, where you get everything you want, the walk-away point where the relationship is severed and lawyers get involved, and a reasonable and fair resolution that doesn’t a bad taste in anyone’s mouth.",
    "crumbs": [
      "CV",
      "Vignettes",
      "High stakes negotiation"
    ]
  },
  {
    "objectID": "cv/vignettes/win-win-negotiation/index.html#external-homework",
    "href": "cv/vignettes/win-win-negotiation/index.html#external-homework",
    "title": "High stakes negotiation",
    "section": "External homework",
    "text": "External homework\nThe better you understand the internal dynamics of your opponent, the better you can estimate the relative value of the issues on the table, and predict there positions and soft spots. Here it is useful to take a broader, industry-wide perspective. Our opponent was in fierce competition with a couple other large companies, with new, smaller, and less expensive vendors popping up all the time. We needed both types, and I did not want to lose one of the major suppliers, reducing competition for our contracts. Information can be gleaned from internal team members who developed strong relationships with their contacts at the vendors.",
    "crumbs": [
      "CV",
      "Vignettes",
      "High stakes negotiation"
    ]
  },
  {
    "objectID": "cv/vignettes/win-win-negotiation/index.html#in-the-room",
    "href": "cv/vignettes/win-win-negotiation/index.html#in-the-room",
    "title": "High stakes negotiation",
    "section": "In the room",
    "text": "In the room\nIf one is to quickly conclude a negotiation, of course, the principles must be in the room, which in my case were the two directors. I did not include the responsible department manager for most of the sessions, as I was well versed in the specific points and wanted to minimize personal, emotional expressions from our side. They brought their president, the manager of our account, and project manager. They needed the project manager because the account manager was not adequately conversant in the issues discussed, but his defensiveness impeded the negotiation.\nMy general approach was to emphasize the value of retaining the partnership between ourselves and the vendor, projecting the relationship as more valuable to them than us. We were a regular source of contracts, some quite large, in a highly competitive space. Successful negotiations meant future business.\nOn the other hand, they were one of the few full-service (engineers and translators), multi-lingual shops around. I needed to convince them that we had numerous other options, and that the on-going relationship was desirable, but not critical, to us. This was not entirely true, of course.",
    "crumbs": [
      "CV",
      "Vignettes",
      "High stakes negotiation"
    ]
  },
  {
    "objectID": "cv/vignettes/win-win-negotiation/index.html#the-outcome",
    "href": "cv/vignettes/win-win-negotiation/index.html#the-outcome",
    "title": "High stakes negotiation",
    "section": "The outcome",
    "text": "The outcome\nAfter a number of sessions, we got a refund of around a fourth of the contract value, nearer to our ideal goal than to our “expected” outcome. The other company received a commitment that they would be asked to bid for future projects and given fair consideration, as long as they accepted my new contract and bidding structure. In fact, they ended up being selected by the project team (and myself) as the primary vendor for the subsequent major release as well as some other, smaller projects.",
    "crumbs": [
      "CV",
      "Vignettes",
      "High stakes negotiation"
    ]
  },
  {
    "objectID": "cv/vignettes/vendor-management/index.html",
    "href": "cv/vignettes/vendor-management/index.html",
    "title": "Managing External Partnerships",
    "section": "",
    "text": "When I assumed responsibility for all localization vendors at Autodesk, it was the first time the role had been centralized. Prior to this time, vendors were selected by project teams through an ad hoc process. Some groups approached the selection methodically, but most were chosen for vague reasons and, frankly, familiarity. Even when things go badly, there is a strong natural tendency to stick with the devil you know.\nOf course, there were numerous problems with this approach. First, there were significant pricing inconsistencies, due both to lack of real competition and variations in terms for pricing across proposals. Additionally, expectations of vendor responsibilities varied from team to team, which led to confusion when one vendor worked on multiple projects. Project budgets and forecasts were difficult to prepare, and vendor costs varied significantly from project to project.\nThere are many providers of translation services in Europe, ranging from small two or three person in-country operations to international companies like Berlitz operating centrally out of London. Obviously, the larger the shop, the more expensive the services. Smaller operations often did not offer engineering services, so if that was part of the requirements, the larger companies were necessary, of which there were only a few. I set out to expand our pool of vendors in order to leverage less expensive suppliers (who were usually the most responsive), while keeping check on the big partners on whom we were dependent. I also worked with some of the single-language companies to help them develop engineering resources, allowing them to compete for more projects.\nI imposed two hard requirements for all vendors in order to submit project proposals. First, I provided them with a template which they needed use for their bid, ensuring that all comparisons between vendors be “apples to apples” and that costs, pricing and expectations were standardized. Additionally, I developed a standard contract, which vendors needed to accept without modification. This meant no legal issues would arise after vendor selection, and that, should problems arise during the contract, our legal position in all cases was clear. The companies with legal departments fussed a bit, but the contract was not unreasonable and so all agreed.\nInternally, my task was to regularize the vendor selection process. Initially, project teams assumed I was going to impose vendors on them, or that I would take away their preferred partners. What I did impose was a transparent decision-making process, where project requirements were clearly specified in advance. I then provided proposals from several vendors (including the team favoriate) and all were then evaluated against the pre-defined requirements. Since all proposals were in the same format, and the team did not need to concern themselves with contractual issues, the final selection were generally quick and highly collaborative meetings.\n\n\n\n Back to top",
    "crumbs": [
      "CV",
      "Vignettes",
      "Managing External Partnerships"
    ]
  },
  {
    "objectID": "cv/vignettes/motivating-change/index.html",
    "href": "cv/vignettes/motivating-change/index.html",
    "title": "Motivating through change",
    "section": "",
    "text": "It’s a truism that most people don’t like change. Yet change, often internally disruptive, is constant in business. Individuals can feel that they have lost their identity as they assume new roles, in new teams, with new ways of doing things. The old ways, no matter how they were criticized before, become the golden age, lost forever. The management challenge is to forge effective teams who are motivated and empowered, and take ownership of their job. In my experience, straight-forwardness and respect are the keys to doing this.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Motivating through change"
    ]
  },
  {
    "objectID": "cv/vignettes/motivating-change/index.html#context",
    "href": "cv/vignettes/motivating-change/index.html#context",
    "title": "Motivating through change",
    "section": "Context",
    "text": "Context\nI was Sr. Financial Analyst in charge of budgeting and forecasting for the European Software and Operations Centers in Neuchâtel, Switzerland during a major reorganization, for which I provided the finance support. Subsequently, I took a management role in the newly organized group, responsible for world-wide translations and the vendors who provided localization and translation services.\nWithout going into detail, engineering and QA were prioritized, and translation entirely outsourced. Five linguists were retained to cover the primary European languages, although their role was unclear. They had been translators embedded in project teams, and now they were not to be translators, yet to have responsibility across product lines.\nNot only was their job to change in nature, but on a personal level, these were people who had had a “place” on assigned teams, and were now grouped together in a separate, bare-bones department with an ill-defined mission. Of course the laying off of a number of their colleagues only exacerbated their emotional distress. They had lost their identity in the organization, and risked either becoming zombie workers, or worse, subversive, or just wait to be fired. In any case, not energized and productive employees.\nIn their eyes, I was naturally perceived as responsible for their predicament, at least at some level. I was the outsider, which gave me an over-sized share of the blame. I had no experience in translation and localization, and they knew it, yet they would be working for me. The first staff meeting was to be challenging, to say the least.\nMy plan was to make the translators into vendor managers for their languages. But my team was composed of translators, not vendor managers. The skills necessary for translation work are not the same as managing a group of translators, and the team needed to learn these skills and, just as importantly, they needed to want to learn the skills.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Motivating through change"
    ]
  },
  {
    "objectID": "cv/vignettes/motivating-change/index.html#first-meeting",
    "href": "cv/vignettes/motivating-change/index.html#first-meeting",
    "title": "Motivating through change",
    "section": "First meeting",
    "text": "First meeting\nMy first priority was to call attention to the elephant in the room, namely me. Obviously I didn’t know, beyond a general sense, the translation and localization process, I was the classic “clueless” manager. I appealed to their knowledge and experience, reassuring them that they would make decisions on all the technical aspects of their work. Aside from being true, it was the first step in letting them know that they were valued and important to the organization.\nI discussed the ambiguity of their jobs with respect to the organization as a whole, and our role and identity as a department, and that we would need to develop together. I told them they could no longer be translators, but that we would need to re-envision their jobs, using my financial experience and their technical knowledge. I pointed out that this meant they would have roles on all major projects, not just one, with much greater visibility in the Center.\nThroughout this, expressions gradually changed from hostility to skepticism, with questions of an increasingly constructive nature, and even ideas about moving forward. By the end there was buy-in all around, ranging from clearly tentative but sincere to frankly enthusiastic.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Motivating through change"
    ]
  },
  {
    "objectID": "cv/vignettes/motivating-change/index.html#follow-up",
    "href": "cv/vignettes/motivating-change/index.html#follow-up",
    "title": "Motivating through change",
    "section": "Follow up",
    "text": "Follow up\nOver the next week or so, through many one-on-one and group meetings, some held without me, details were worked out. Working with the management team, I ensured that they would be welcomed in their new roles on the project teams, and their interactions there built self-confidence. I arranged for them to visit key in-country vendors on their own to discuss past issues and future requirements. This gave them a strong sense of ownership and independence.\nWe did hold the obligatory off-site team-building exercises, which were pleasant but motivation came much more from the day-to-day work and the empowerment they felt as they developed their roles and identities.\nBy the end of the first month, the attitude change was clear. They were working hard, and they began coming to me less with problems and more frequently with ideas for improvements to our developing processes. By the end of the first year, all of my team was intact and respected internally and externally. Later, one enjoyed the new kind of work that she transitioned to project management.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Motivating through change"
    ]
  },
  {
    "objectID": "cv/vignettes/swiss-business-plan/index.html",
    "href": "cv/vignettes/swiss-business-plan/index.html",
    "title": "Business Plan for Neuchâtel Government",
    "section": "",
    "text": "Autodesk chose to base its European center for research and development and for operations in Neuchâtel, Switzerland, in large part due to tax incentives provided by the cantonal government. During my time in Neuchâtel, the agreement was expiring, and a new proposal was required to extend the credits.\nThe original agreement included production and shipping to all European customers, in addition to research and development activities related to translation and localization. All other European subsidiaries were commissionaires of the Neuchâtel subsidiary, so most European revenue was able to be recognized in Switzerland thereby obtaining favorable tax treatment along with R&D credits..\nWe were aggressively outsourcing all operations to Dublin, which did not make the government happy, and they were not convinced that the localization activities with regard to product upgrades, which were carried out in Neuchâtel, actually constituted research and development.\nThe Swiss officials had a lot of experience with traditional production, which is to say, production of tangible things, and research and development activities were understood to be aimed at creating a new physical product, whose novelty was readily apparent. In software, we don’t sell boxes, we sell intangible goods, software licenses. The localization activities were always focused on the upcoming upgrade releases, and the tax authorities were skeptical that these upgrades constituted truly new R&D or simply maintenance. Put simply, was a new release of a product a new product?\nThe task was then to explain the software development process to the tax authorities, and convince them that each relase contained not only significant new and innovative features, but large amounts of new code. In the end, we secured a new tax agreement with the cantonal government.\n\n\n\n Back to top",
    "crumbs": [
      "CV",
      "Vignettes",
      "Business Plan for Neuchâtel Government"
    ]
  },
  {
    "objectID": "cv/vignettes/collective-forecasting/index.html",
    "href": "cv/vignettes/collective-forecasting/index.html",
    "title": "Collaborative Forecasts",
    "section": "",
    "text": "It was important to me that the department managers at Autodesk’s European Operations and Software Center were closely involved in the budgeting and monthly forecasting, for which I was responsible. I had a number of goals, one being, of course, to always be on target. But beyond that, I wanted to make the entire process tangible to the managers, some of whom were savvy, but most of whom saw the process initially as a painful exercise distracting them from their “real work”.\nTo that end, I provided an Excel workbook to each department head, and we created evebt-based budgets from the bottom up, listing all of the staffing changes and other specific expenditures that the manager intended for the year, ensuring of course that overall targets were met.\nEach month, I was able to give each manager an updated version of their workbook, containing actuals with comparisons to both the budget and the most recent forecast. It was easy, then, to re-forecast each month, since the listed events either happened or were cancelled or delayed.\nThis way, I could also ensure that managers were aware of opportunities generated by the forecasts. As a simple example, a hire budgeted for in March, but not engaged until a month or so later, can free up significant funds for other expenditures which had been prohibited by budget constraints.\nOf course, it was my job also to ensure that money wasn’t spent just because it was there, but that was rarely if ever a problem. This also allowed the directors to have a clear financial picture of all the departments, enabling them to make inter-departmental allocation decisions with regard to the “available” funds.\n\n\n\n Back to top",
    "crumbs": [
      "CV",
      "Vignettes",
      "Collaborative Forecasts"
    ]
  },
  {
    "objectID": "cv/vignettes/closing-the-books/index.html",
    "href": "cv/vignettes/closing-the-books/index.html",
    "title": "Closing the books",
    "section": "",
    "text": "I first joined Autodesk as Sr. Consolidations Account, responsible for the corporate financial statements. At the time, the company had a couple dozen foreign subsidiaries, each with their own currencies, statutory requirements, and accounting software. Closing the month end was slow, frustrating the corporate leadership. The situation resulted from a combination of dealing with reports in multiple formats and poor communication around timing and expectations, all within a relatively young company adjusting to corporate governance. My task was to reliably reduce the time to close the corporate books and have consolidated financial statements within a few days of month end.\nFortunately for me, very soon after I was hired, the finance managers from all the subsidiaries gathered in California for a conference, so timing was perfect for immediately implementing this. The face-to-face interactions, socially as well as in the context of meetings, was particularly important to me because I had no formal training in accounting, and needed to quickly build their confidence.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Closing the books"
    ]
  },
  {
    "objectID": "cv/vignettes/closing-the-books/index.html#context",
    "href": "cv/vignettes/closing-the-books/index.html#context",
    "title": "Closing the books",
    "section": "",
    "text": "I first joined Autodesk as Sr. Consolidations Account, responsible for the corporate financial statements. At the time, the company had a couple dozen foreign subsidiaries, each with their own currencies, statutory requirements, and accounting software. Closing the month end was slow, frustrating the corporate leadership. The situation resulted from a combination of dealing with reports in multiple formats and poor communication around timing and expectations, all within a relatively young company adjusting to corporate governance. My task was to reliably reduce the time to close the corporate books and have consolidated financial statements within a few days of month end.\nFortunately for me, very soon after I was hired, the finance managers from all the subsidiaries gathered in California for a conference, so timing was perfect for immediately implementing this. The face-to-face interactions, socially as well as in the context of meetings, was particularly important to me because I had no formal training in accounting, and needed to quickly build their confidence.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Closing the books"
    ]
  },
  {
    "objectID": "cv/vignettes/closing-the-books/index.html#the-people",
    "href": "cv/vignettes/closing-the-books/index.html#the-people",
    "title": "Closing the books",
    "section": "The People",
    "text": "The People\nThere seemed to be two different challenges here, one being tools and technology, the other being communication, particularly around expectations. The first would be straight-forward, the second required more consideration. In the first day or two, I was able to interact with all of the managers to both gauge their frustrations and establish myself in their eyes. I was also able to get a good sense of their own thinking.\nI believe that people commit themselves most actively to self-established goals. When it came time for me to introduce the new schedule and requirements for month end reporting, I first passed a hat around the room, asking each manager to write down how many days they found reasonable for both subsidiaries and corporate to close at the end of the month. The results of the poll were either equal to or shorter than the schedule I was to impose, so I was able to introduce the schedule as a mutually-agreed one rather than a corporate imposition, and the meeting was able to quickly move to other topics.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Closing the books"
    ]
  },
  {
    "objectID": "cv/vignettes/closing-the-books/index.html#the-tools",
    "href": "cv/vignettes/closing-the-books/index.html#the-tools",
    "title": "Closing the books",
    "section": "The Tools",
    "text": "The Tools\nAlong with their commitment to the schedule, I made a commitment to providing easy reporting tools, both for them to prepare and for me to process on receipt. The solution was rather simple, looking back from today. While it is hard to imagine these days, Excel was not ubiquitous as it is now. In fact, many were still using Lotus 1-2-3 for spreadsheets. As accountants, of course, the finance managers all had basic spreadsheet skills, but were unaware of more advanced (for the era) features.\nExcel had recently introduced workbooks, allowing for linking between spreadsheets. I provided each subsidiary with a workbook with a couple sheets for reporting in their local currency, and locked all the cells except those for input. I had a master workbook containing the corporate financial statements, linked to each of the subsidiaries’ reports. When I received the reports, I simply had to copy the submission into my workbook, do a sanity check, and that’s it.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Closing the books"
    ]
  },
  {
    "objectID": "cv/vignettes/closing-the-books/index.html#the-result",
    "href": "cv/vignettes/closing-the-books/index.html#the-result",
    "title": "Closing the books",
    "section": "The Result",
    "text": "The Result\nThe new process worked well, and deadlines were consistently met or exceeded. In fact, the subsidiaries’ efficiency put pressure on corporate accounting to improve their end of the close processes.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Closing the books"
    ]
  },
  {
    "objectID": "cv/vignettes/paradox/index.html",
    "href": "cv/vignettes/paradox/index.html",
    "title": "Sales Reporting",
    "section": "",
    "text": "The second major responsibility I had as Sr. Consolidations Accountant at Autodesk’s corporate headquarters was the monthly production of the Sales Book, a publication internally referred to as “the bible”. This listed unit sales for all geographies, with summary tables and the critical installed base for all products. The software used by operations had no built-in reporting capabilities, they could only dump shipment information to a text file, which the consolidations accountant would import into Excel. Once in Excel, the consolidations accountant would embark on a long, laborious task of sorting, totaling, subtotaling, calculating installed base and upgrades, etc. I remember watching my predecessor do this when I was being trained, immediately deciding I would not do this.\nI had familiarity with databases, having programmed applications in Clipper, a version of dBase, which was itself a major database platform at the time, and it was evident that this was a database job, not a spreadsheet one. For this task, I decided on a database system called Paradox, and started explaining to my boss what databases were, why they could be useful, and why he should get me a license. I was fortunate to have an open-minded boss, and he agreed.\nUsing Paradox, I was able to define the queries necessary for preparing the Sales Book and produce the reports immediately on receipt of the monthly shipping information. I could use the same method to produce interim reports, and was able to respond quickly to ad hoc questions from people about unit sales, questions which came frequently around quarter end.\nChoosing the appropriate tool for a task is very important. It can reduce work and increase efficiency by orders of magnitude.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Sales Reporting"
    ]
  },
  {
    "objectID": "cv/vignettes/paradox/index.html#context",
    "href": "cv/vignettes/paradox/index.html#context",
    "title": "Sales Reporting",
    "section": "",
    "text": "The second major responsibility I had as Sr. Consolidations Accountant at Autodesk’s corporate headquarters was the monthly production of the Sales Book, a publication internally referred to as “the bible”. This listed unit sales for all geographies, with summary tables and the critical installed base for all products. The software used by operations had no built-in reporting capabilities, they could only dump shipment information to a text file, which the consolidations accountant would import into Excel. Once in Excel, the consolidations accountant would embark on a long, laborious task of sorting, totaling, subtotaling, calculating installed base and upgrades, etc. I remember watching my predecessor do this when I was being trained, immediately deciding I would not do this.\nI had familiarity with databases, having programmed applications in Clipper, a version of dBase, which was itself a major database platform at the time, and it was evident that this was a database job, not a spreadsheet one. For this task, I decided on a database system called Paradox, and started explaining to my boss what databases were, why they could be useful, and why he should get me a license. I was fortunate to have an open-minded boss, and he agreed.\nUsing Paradox, I was able to define the queries necessary for preparing the Sales Book and produce the reports immediately on receipt of the monthly shipping information. I could use the same method to produce interim reports, and was able to respond quickly to ad hoc questions from people about unit sales, questions which came frequently around quarter end.\nChoosing the appropriate tool for a task is very important. It can reduce work and increase efficiency by orders of magnitude.",
    "crumbs": [
      "CV",
      "Vignettes",
      "Sales Reporting"
    ]
  },
  {
    "objectID": "cv/vignettes/chinese-herb-class/index.html",
    "href": "cv/vignettes/chinese-herb-class/index.html",
    "title": "Teaching Chinese Herbs",
    "section": "",
    "text": "The herbal part of a Chinese Medicine program begins with three consecutive semesters of Materia Medica, in which over 1,000 herbs and other medicinal substances are studied in detail. Students are expected to memorize the Chinese name, pharmaceutical name, functions, indications, taste, nature, channels entered, and contraindications for each medicinal. American, and other non-Chinese speakers, have the additional hurdle of learning names in a foreign language.\nAs you might imagine, this is a daunting task, and is the main cause of student withdrawals from the program. Even after having passed the program, not all are able to pass the board examinations for herbology.\nA big part of the difficulty in learning herbal medicine is the way in which it is taught. It is important to realize that teaching Chinese medicine in a college or university setting is a modern practice. Traditionally, Chinese doctors learned through apprenticeships. (In China, even in university programs, students spend a large part of their time at the side of their teachers, observing and assisting.) The institutionalization and standardization of instruction came with an instructional sequence which is absurd on its face.\nChinese formulas are full sentences, often composed of multiple clauses, both dependent and independent. The basic unit, the word, is the herb. The standard approach to teaching in the collegiate setting is essentially to have students memorize the dictionary for a year, and then learn how to make sentences. I was not in a position to change the system, obviously, so I could only work within it. Here are some things I did to help my students.\n\nConsiderable time was spent early on discussing and sharing study and memorization strategies. There is a considerable amount of rote learning required, made more difficult since the names that need to be memorized are in foreign languages (Chinese Pinyin and Latin).\nI provided consistent, graphically enriched course materials with slide presentations for all herbs covered, a matching pdf with notes, and quizzes and tests in consistent format. Color coding was used for key words. I used \\(LaTeX\\) for this, a typesetting system commonly used for textbook and journal publications.\nI was clear on the distinctions between required material and supplementary material to reduce student stress. (I remember one of my Chinese professors of formulas being asked by a student what we should study for the final, and he replied only “You know everything, you do perfect.”)\nChunked down and re-organized the material into smaller, logical groupings and progressions or spectra where possible.\nTaught herbal combinations, which is not required but very useful for associative learning and repetition of more important herbs. This is actually a separate course in Chinese universities\nStudents identified and reported on common, local plants which are also Chinese herbs.\nHeld potlucks where students brought in food with a description of the health properties of the dish using both Chinese herbal theory and also Chinese food therapy, a different branch of medicine.\n\n\n\n\n Back to top",
    "crumbs": [
      "CV",
      "Vignettes",
      "Teaching Chinese Herbs"
    ]
  },
  {
    "objectID": "cv/vignettes/clinic/index.html",
    "href": "cv/vignettes/clinic/index.html",
    "title": "TCM Clinic",
    "section": "",
    "text": "Introduction\nFor most of the 20 years I practiced Chinese Medicine, I owned and operated a private clinic which served around 40 clients per week. As a Doctor of Oriental Medicine, I administered both acupuncture and herbal therapy. Therapeutic massage was also offered.\nIn New Mexico, acupuncture services are covered by many insurance plans. I was a provider for the largest insurance companies in the state, including Blue Cross, United and Presbyterian, and processed insurance claims for the majority of my clients.\n\n\nConditions treated\nAcupuncture is best known for pain syndromes, both acute and chronic. Unsurprisingly, a large percentage of my patients came for treatment of neck, shoulder and back pain, as well as headaches and menstrual pain. In most cases, these were chronic conditions associated with, for example, arthritis, migraines or pre-menstrual syndrome.\nOther health concerns I treated regularly included emotional issues such as insomnia, depression and anxiety, and digestive issues such as indigestion, reflux, diarrhea and constipation. Occasionally people would come for cold and flu treatment.\n\n\nServices\nI practiced several styles of acupuncture. My formal training was in the style currently practiced in China referred to as Traditional Chinese Medicine (TCM), and I often used it for simple cases because it is efficient. I also trained in two Japanese styles, in particular that of Kiiko Matsumoto, which I used when TCM didn’t produce adequate results, and the Taoist based style of Jeffrey Yuen, which I used primarily for emotional conditions.\nMy specialty was herbal medicine, which I taught for over 10 years at the local acupuncture college (SWAC). Designing precise formulas for complex conditions is difficult, and little practiced in the US among non-Chinese practitioners. I stocked over 100 herbs, many in powder form, which I used to prepare custom formulas for patients. In simple cases, I dispensed prepared pills and tablets, and many patients with body pain received topical medication as well.\nI incorporated some massage in my treatments, but therapeutic massage services were primarily offered by Licensed Massage Therapists working at the clinic.\n\n\n\n\n Back to top",
    "crumbs": [
      "CV",
      "Vignettes",
      "TCM Clinic"
    ]
  },
  {
    "objectID": "blogs/obsidian-headers/index.html",
    "href": "blogs/obsidian-headers/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/obsidian-headers.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/python-movingpandas-points-paths/index.html",
    "href": "blogs/python-movingpandas-points-paths/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online//posts/data-science/python-gpx-mapping/python-movingpandas-points-paths/.\n\n\n\nhttp://localhost:4326\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/obsidian-whiteboards/index.html",
    "href": "blogs/obsidian-whiteboards/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/obsidian-whiteboards.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/python-movingpandas-speed-segments/index.html",
    "href": "blogs/python-movingpandas-speed-segments/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online//posts/data-science/python-gpx-mapping/python-movingpandas-speed-segments/.\n\n\n\nhttp://localhost:4326\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/visual-mocs-with-canvas/index.html",
    "href": "blogs/visual-mocs-with-canvas/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/visual-mocs-with-canvas.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/obsidian-no-sql-database/index.html",
    "href": "blogs/obsidian-no-sql-database/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/obsidian-no-sql-database.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/dataviewjs-files-dates/index.html",
    "href": "blogs/dataviewjs-files-dates/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/dataview/dataviewjs-files-dates.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/obsidian-jupyter/index.html",
    "href": "blogs/obsidian-jupyter/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/obsidian-jupyter.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/second-brain/index.html",
    "href": "blogs/second-brain/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/second-brain.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/syncing-your-thinking-syncthing/index.html",
    "href": "blogs/syncing-your-thinking-syncthing/index.html",
    "title": "biscotty's Workshop",
    "section": "",
    "text": "GPS\n\n\n\n\n\nThis page has moved to  https:/biscotty.online/posts/obsidian/syncing-your-thinking-syncthing.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/posts.html",
    "href": "posts/posts.html",
    "title": "All Posts",
    "section": "",
    "text": "Shiso Fine\n\n\nThe Purple Revival\n\n10 min\n\n\nChinese Medicine\n\nHerbs\n\nFood\n\n\n\nThe famous leaves served with sushi have myriad functions in Traditional Chinese Medicine\n\n\n\n\n\n\n\n\n\n\n\n\nNix for Data Scientists\n\n\nNix your venvs and skip pip\n\n9 min\n\n\nLinux\n\nData Science\n\nNix\n\nPython\n\n\n\nInstalling and maintaining multiple versions of libraries, whether in Python or R, is best managed with Nix\n\n\n\n\n\n\n\n\n\n\nHispanics in New Mexico\n\n\nA demographic study with R and TidyCensus\n\n27 min\n\n\nR\n\nData Science\n\nGIS\n\n\n\nNew Mexico has a Latino population unlike anywhere else in the US. This article shows how to use R and tidycensus to explore the data.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Magical Mulberry\n\n\nHerbs around us\n\n13 min\n\n\nChinese Medicine\n\nHerbs\n\nHistory\n\n\n\nThis invasive tree provides many of the more commonly used medicinals in Chinese medicine and shaped world history\n\n\n\n\n\n\n\n\n\n\n\n\nEating in spring\n\n\nChinese medicine food therapy\n\n15 min\n\n\nChinese Medicine\n\nFood\n\n\n\nFoods and recipes appropriate for the spring season\n\n\n\n\n\n\n\n\n\n\n\n\nThe FHS Problem\n\n\nDocker vs Nix\n\n6 min\n\n\nLinux\n\nDistributions\n\nNix\n\n\n\nTwo different approaches to the problem of the Filesystem Heirarchy Standard\n\n\n\n\n\n\n\n\n\n\nGPS Mapping with R\n\n8 min\n\n\nGIS\n\nR\n\nPython\n\nData Science\n\n\n\nA Comparison with Python\n\n\n\n\n\n\n\n\n\n\n\n\nSpeed, Simplification and Segments\n\n20 min\n\n\nGIS\n\nPython\n\nData Science\n\n\n\nWorking with MovingPandas\n\n\n\n\n\n\n\n\n\n\n\n\nPoints to Paths in Python\n\n16 min\n\n\nData Science\n\nGIS\n\nPython\n\n\n\nCreating trajectories in MovingPandas\n\n\n\n\n\n\n\n\n\n\n\n\nNixOS: the Linux different\n\n\nGrokking NixOS\n\n7 min\n\n\nLinux\n\nDistributions\n\n\n\nExploring the concepts behind the unique Linux distribution\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian and Jupyter Notebooks\n\n6 min\n\n\nObsidian\n\nPython\n\nData Science\n\n\n\nPersonal Knowledge Management for Data Science\n\n\n\n\n\n\n\n\n\n\n\n\nTrail Mapping with Python\n\n11 min\n\n\nGIS\n\nPython\n\nData Science\n\n\n\nUsing your GPX data with geopandas\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian Canvas Work Spaces\n\n10 min\n\n\nObsidian\n\n\n\nVisual MOCery\n\n\n\n\n\n\n\n\n\n\n\n\nGit and GitHub for Obsidian Users\n\n11 min\n\n\nObsidian\n\nLinux\n\n\n\nAnd why you probably shouldn’t be using it\n\n\n\n\n\n\n\n\n\n\n\n\nSummarizing Information with DataviewJS\n\n9 min\n\n\nJavascript\n\nObsidian\n\nPKM\n\nDataviewJS\n\n\n\nPresenting and analyzing information in Obsidian\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Tables with DataviewJS\n\n8 min\n\n\nJavascript\n\nObsidian\n\nDataviewJS\n\n\n\nSelecting Files, Working with Dates\n\n\n\n\n\n\n\n\n\n\n\n\nDataviewJS: A Gentle Introduction\n\n8 min\n\n\nJavascript\n\nObsidian\n\nDataviewJS\n\n\n\nFor Obsidian users who are not programmers\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian: Pretty Canvas\n\n6 min\n\n\nObsidian\n\nCSS\n\n\n\nUsing CSS to make dashboards cleaner and more effective.\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian: Freeing Your Thinking Workflow\n\n1 min\n\n\nObsidian\n\nPKM\n\n\n\nA video showing how to use the information-focused Freeing Your Thinking paradigm.\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian Canvas Dashboards\n\n5 min\n\n\nObsidian\n\n\n\nCanvas allows you to build dashboards to organize and interact with your information.\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian Metadata Menu Plugin\n\n2 min\n\n\nObsidian\n\n\n\nSupercharge your management of properties and in-line Dataview fields\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian Bookmarks\n\n6 min\n\n\nObsidian\n\n\n\nBookmarks are the key to effectively using obsidian as a non-relational database.\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian: The mechanics of Graph View\n\n1 min\n\n\nObsidian\n\nGraphs\n\n\n\nVideo - Working with Graph View to explore your information network.\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian: A Second Brain?\n\n1 min\n\n\nObsidian\n\nGraphs\n\n\n\nExploring the concepts of graph data and neural networks.\n\n\n\n\n\n\n\n\n\n\n\n\nSyncing your Thinking with Syncthing\n\n5 min\n\n\nObsidian\n\nLinux\n\n\n\nFast, secure, private and free way to synchronize your Obsidian vault (and any other direcotries) across multiple devices.\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian Basics - Headers\n\n5 min\n\n\nObsidian\n\n\n\nHeaders add more than visual appeal. They are important for structuring your document and enabling atomic embeds.\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian: Stop Wasting Time With Directories and Filenames\n\n9 min\n\n\nObsidian\n\nPKM\n\n\n\nFreeing Your Thiking Part 1 introduces the idea of of working with Obsidian as a database. This part covers capture and retrieval of information useing Unique Notes, Search…\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian: Meaningless Names, No Directories, Now What?\n\n7 min\n\n\nObsidian\n\nPKM\n\nGraphs\n\n\n\nFreeing Your Thiking Part 2 discusses the use of Graph View and Bookmarks to discover and organize information in your vault\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does it mean that is a NoSQL Database?\n\n4 min\n\n\nObsidian\n\n\n\nObsidian is a non-relational database. This article explains this and the significance of the fact.\n\n\n\n\n\n\n\n\n\n\n\n\nRegular Expressions for In-Line Fields\n\n2 min\n\n\nObsidian\n\n\n\nObsidian doesn’t support search filters for in-line Dataview fields, but regex makes it easy\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Notes with Obsidian\n\n9 min\n\n\nObsidian\n\nPKM\n\n\n\n…a tale in the spirit of Euripides\n\n\n\n\n\n\n\n\n\n\n\n\nObsidian and Jupyter Notebooks\n\n6 min\n\n\nObsidian\n\nPython\n\nData Science\n\n\n\nPersonal Knowledge Management for Data Science\n\n\n\n\n\n\n\n\n\n\n\n\nDataviewJS: A Gentle Introduction Part 2\n\n8 min\n\n\nJavascript\n\nObsidian\n\nDataviewJS\n\n\n\nSelecting Files, Working with Dates\n\n\n\n\n\nNo matching items\n\n  \n\n Back to top"
  },
  {
    "objectID": "posts/data-science/r-census/nm-hispanic-demographics/index.html",
    "href": "posts/data-science/r-census/nm-hispanic-demographics/index.html",
    "title": "Hispanics in New Mexico",
    "section": "",
    "text": "This article will explore some interesting aspects of New Mexico’s racial demographics using R and the TidyCensus package. The US Census Bureau makes an amazing amount of data available to the public on https://census.gov. The site allows you to easily search for and download demographic information with myriad variables and geographic subsets. The site also provides an API for programmatic access, and R is fortunate to have the TidyCensus package to easily grab data of interest to the programmer.\nNew Mexico has relatively large Native American and Hispanic populations. Nearly half of New Mexico residents are Hispanic, compared to a national average of around 20% (we’ll calculate it later). The profile of Latinos in New Mexico, however, is very different from the rest of the country, as we shall see.\nNot only is the Hispanic population of New Mexico uniquely large in percentage terms, it is also unusual in terms of national identity of the Hispanics. Unlike Latinos in most of the rest of the country, many Hispanics in New Mexico do not identify at all with Latin America, but directly with their European ancestry.\nThe rest of this article will explore the Hispanic population in New Mexico and the historical context which explains this particular situation. Along the way, I will show how to use the R package tidycensus to access the wealth of data on https://census.gov.",
    "crumbs": [
      "Data Science",
      "R Census",
      "Hispanics in New Mexico"
    ]
  },
  {
    "objectID": "posts/data-science/r-census/nm-hispanic-demographics/index.html#getting-acs-data",
    "href": "posts/data-science/r-census/nm-hispanic-demographics/index.html#getting-acs-data",
    "title": "Hispanics in New Mexico",
    "section": "Getting ACS data",
    "text": "Getting ACS data\nBefore looking at Latinos specifically, I would like to take a quick look at immigration to New Mexico, since many Hispanics are immigrants. Using get_acs() I can get data from the American Community Survey 5-year tables. The most time-consuming part is sifting through the thousands of variables to find the ones you want. Fortunately, tidycensus provides a function to download the variables, which can then be Viewed and searched.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(patchwork)\nlibrary(sf)\noptions(tigris_use_cache = TRUE)\n\n\nv23 &lt;- load_variables(2023, \"acs5\", cache = TRUE)\nView(v23)\n\n\npob_vars = c(\n  Native = \"B05002_002\",\n  ForeignBorn = \"B05002_013\"\n)\n\nNow I can use get_acs() to download the data. There are many possibilities for geography, which is required. Some are obvious like “us”, “state” and “county”. I specify which state I want. The summary_var argument adds an additional column to the data frame containing totals, allowing for easy calculations of percentages.\nget_acs() has an optional argument which can be passed, geometry = TRUE. This handy method adds geometries from tigris and returns the sf object necessary for cartographic plotting. Unfortunately, I get errors using this for county and state data, so I need to manually perform the process.\nFirst, I’ll grab shapes for the states and for the counties in New Mexico. The shift_geometry() function will put Alaska and Hawaii in a convenient place on the maps. GEOID is standard across all datasets, and encodes the state, county, and census tract, depending on the level of data requested. There are many columns in the tigris datasets, but I only want the name and geometry.\n\nlibrary(tigris)\nstates &lt;- states() |&gt; \n  select(GEOID, state = NAME, geometry) |&gt; \n  shift_geometry()\n\nRetrieving data for the year 2021\n\nnm_counties &lt;- counties(\"NM\") |&gt; \n  select(GEOID, county = NAME, geometry)\n\nRetrieving data for the year 2022\n\n\nNow, with standard dplyr syntax, I can create the sf data frame. I want to do comparisons to the US in general, so I will also get data for all 50 states. left_join allows me to add the geometry from nm_counties based on the invaluable GEOID. Once joined, though, I no longer need it, nor a number of other columns, so I select the ones I want.\n\nimmigration_23 &lt;- get_acs(\n  geography = \"county\",\n  state = \"NM\",\n  variables = pob_vars,\n  summary_var = \"B05002_001\",\n  year = 2023,\n  cache_table = TRUE,\n) |&gt; \n  left_join(nm_counties,  by = \"GEOID\") |&gt; \n  select(variable, estimate, moe,\n         county, summary_est, geometry) |&gt; \n  st_as_sf()\n\nGetting data from the 2019-2023 5-year ACS\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nimmigration_us &lt;- get_acs(\n  geography = \"state\",\n  variables = pob_vars,\n  summary_var = \"B05002_001\",\n  year = 2023,\n  cache_table = TRUE,\n) |&gt; \n  left_join(states,  by = \"GEOID\") |&gt; \n  select(variable, estimate, moe,\n         state, summary_est, geometry) |&gt; \n  st_as_sf()\n\nGetting data from the 2019-2023 5-year ACS\n\n\n\n\n\nPercentages can be more interesting than raw numbers, so I will calculate them. R makes it so easy to create new columns based on existing ones.\n\nimmigration_23$pct_foreign = immigration_23$estimate / immigration_23$summary_est\nimmigration_us$pct_foreign = immigration_us$estimate / immigration_us$summary_est\n\nNow we can see some basic stats:\n\npct_fb_nm &lt;- with(\n  immigration_23, \n  mean(pct_foreign[variable == \"ForeignBorn\"]))\npct_fb_us &lt;- with(\n  immigration_us, \n  mean(pct_foreign[variable == \"ForeignBorn\"]))\ndata.frame(\n  Area = c(\"New Mexico\", \"US\"),\n  Average = c(pct_fb_nm, pct_fb_us)\n)\n\n        Area    Average\n1 New Mexico 0.06774040\n2         US 0.09565981\n\n\nDespite having a the largest Hispanic population in the country, New Mexico has a much smaller percentage of immigrants than the overall average.",
    "crumbs": [
      "Data Science",
      "R Census",
      "Hispanics in New Mexico"
    ]
  },
  {
    "objectID": "posts/data-science/r-census/nm-hispanic-demographics/index.html#visualizing-the-data",
    "href": "posts/data-science/r-census/nm-hispanic-demographics/index.html#visualizing-the-data",
    "title": "Hispanics in New Mexico",
    "section": "Visualizing the data",
    "text": "Visualizing the data\nI would like to visualize this data. Explaining ggplot is beyond the scope of this article, unfortunately. I highly recommend the R Graphics Cookbook. I use the + operator from the patchwork library to put the plots side by side.\n\np1 &lt;- ggplot(immigration_23, \n             aes(x = variable, y = estimate)) +\n  geom_col(fill = \"navy\") +\n  labs(x = element_blank(),\n       y = \"Population\",\n       fill = \"County\",\n       title = \"New Mexico\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_bw()\n\np2 &lt;- ggplot(immigration_us, aes(x = variable, y = estimate)) +\n  geom_col(fill = \"navy\") +\n  labs(x = element_blank(),\n       y = \"Population\",\n       title = \"US\",\n       caption = \"acs5 2023\") +\n  scale_y_continuous(labels = scales::comma) +\n  labs(y = element_blank(),\n       caption = \"Source: census.gov acs5 2023\") +\n  theme_bw()\n\np1 + p2\n\n\n\n\n\n\n\nFigure 1: Foreign vs. Native Populations\n\n\n\n\n\nWe can see on a map where the largest concentration of immigrant groups are.\n\nimmigration_us$percent &lt;- immigration_us$estimate / immigration_us$summary_est\n\nimmigration_us |&gt; \n  filter(variable == \"ForeignBorn\") |&gt; \n  ggplot(aes(fill = percent)) +\n  geom_sf(aes(geometry = geometry)) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(title = \"Immigrant population by state\",\n       caption = \"Source: census.gov acs5 2023\") +\n  theme_void() +\n  theme(plot.margin = margin(.3,.3,.3,.3, \"cm\"))\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nI’d like to put some cities on the New Mexico map for orientation purposes. As of this writing, I am unable to use the standard tigris::places(\"NM\", year = 23) command, as it says that the data is unavailable. So I needed to download the file from https://www2.census.gov/geo/tiger/TIGER2023/PLACE/. I’ll extract a few cities in different parts of the state. The shapefiles are POLYGONs, but I need a single point, so I’ll use st_centroid().\n\nplaces &lt;- read_sf(\"data/places/tl_2023_35_place.shp\")\ncities &lt;- c(\"Albuquerque\", \"Santa Fe\", \n            \"Las Cruces\", \"Raton\", \"Roswell\",\n            \"Gallup\", \"Santa Rosa\", \"Farmington\")\ncity_pts &lt;- places |&gt; \n  filter(NAME %in% cities) |&gt; \n  select(City = NAME, geometry) |&gt; \n  st_as_sf() |&gt; \n  st_centroid()\n\n\nfb &lt;- immigration_23 |&gt; \n  filter(variable == \"ForeignBorn\") \n\nggplot() +\n  geom_sf(data = fb, aes(fill = pct_foreign)) +\n  geom_sf(data = city_pts, size = .5, color = \"red\") +\n  geom_sf_text(data = city_pts, aes(label = City),\n               size = 3, color = \"gold\",\n               vjust = \"top\",hjust = \"left\",\n               nudge_y = -.1) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(title = \"NM Immigrant population by county\",\n       caption = \"Source: census.gov acs5 2023\", \n       fill = \"percent\") +\n  theme_void() +\n  theme(plot.margin = margin(.3,.3,.3,.3, \"cm\"))\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nNot surprisingly, the highest percentage of immigrants is near the Mexican border.",
    "crumbs": [
      "Data Science",
      "R Census",
      "Hispanics in New Mexico"
    ]
  },
  {
    "objectID": "posts/data-science/obsidian-jupyter/index.html",
    "href": "posts/data-science/obsidian-jupyter/index.html",
    "title": "Obsidian and Jupyter Notebooks",
    "section": "",
    "text": "Personal Knowledge Management for Data Science\n\n\n\nJupyter notebooks, or more properly iPython notebooks, are fantastic tools for data exploration and modeling. You can run bits of code, interspersed with blocks of markdown, allowing you to easily work with data and present analyses and forecasts in a visual and interactive format. Notebooks can be easily shared via GitHub, or run on-line with Colab.\nThe problem with iPython notebooks, from a Personal Knowledge Management perspective, is they end up being “books on the shelf”. For Obsidian users and PKM practitioners, the whole point is to get away from keeping information in notebooks, with all the uselessness that that implies. One could, as you will see, just do data exploration directly in Obsidian instead of in a standard iPython notebook. But Obsidian is not an IDE. The ecosystem around the interactive Python notebook IDEs, be it Jupyter Lab itself, or VS Code, etc, is so useful, that working in Obsidian while exploring data and creating models would be unacceptably tedious. Using fit-for-purpose tools is very important for efficiency.\nWe need a painless way to capture all this information in our vault, making notes out of the notebooks. How can we easily make this information future-useful, without repeating/duplicating our efforts or doing a ton of copy/paste? In this article, I’ll explain the solution that works well for me, ensuring Obsidian-speed access to any bits of information I have in my notebooks, as well as making review and study activities so much more pleasant. And, of course, my canvases have all gotten richer as well.\n\n\n\nThe community plugin needed to accomplish this is called Execute Code, written by Tim Wibiral together with Jupyter and a library called nbconvert. The latter will convert the notebooks to markdown, and the plugin allows you to execute the code directly within a note. To get started, create a virtual environment for Obsidian to use. If you aren’t using virtual environments, please start now! It’s simple, and you will avoid future problems. From the command line, do:\nmkdir -p $HOME/.config/venvs && cd \"$_\"\npython -m venv obsidian_venv\nsource obsidian_venv/bin/activate\npip install --upgrade pip\npip install jupyterlab nbconvert \nYou can install other libraries like Pandas and Matplotlib as well into the virtual environment with pip install. Jupyter lab and nbconvert will be necessary to convert the notebooks to markdown. At this point, you could launch jupyter lab, but there is no need to. After installing packages, you can exit the virtual environment with deactivate. Should you need to install more packages later, you can type source $HOME/.config/venvs/obsidian_venv/bin/activate to re-enter the virtual environment and pip install other packages.\nIn Obsidian, install the Execute Code plugin. After installing the plugin you must point it to the version of Python you want to use, in this case the one we made in the virtual environment above. In the settings for the plugin, under the language-specific settings, choose Python from the drop down list. For Python Path, enter /home/directory/.config/venvs/obsidian_venv/bin/python.\nWith that done, any code block with the keyword python added directly after the opening back ticks of the block can be run in the Note. In Read view, a Run button will appear by each code block, allowing you to execute the code in the block. After execution, there will be a Clear button to clear up output that you want removed from the note. Code can also be executed from Edit view by using the keyword run-python rather than simply python. The plugin offers a command to run all the code in the note, as well as a command to view and kill any active runtimes.\n\n\n\nJupyter Lab can export an ipynb file directly to markdown! As of writing, VS Code can only export to py, pdf or html. From the file menu, select Export and choose Markdown. This will generate a zipped archive containing a markdown page, along with any image files in the notebook. The problem with this approach is that you will find all of the image files named output something, and so after exporting a few notebooks, there will be name conflicts in your vault.\nUsing the command line avoids this problem, and is in any case much more efficient. You will need to activate your virtual environment with source as described above. Then type\njupyter nbconvert --to Markdown your_notebook.ipynb\nThis will generate an md file which can be copied into your vault. If there are images from generated by the output, like graphs and other visuals, these will be put in a directory created by the above command. If you copy this directory, with all the image files into the vault directory that you use for attachments, the new md file will find them. (It is important to copy the directory itself and not just the files, since the new note will expect to find them there.)\nOnce in Obsidian you can process your Notebooks as any other file, breaking them into bite-size chunks of information. I rely heavily on “Extract this heading”, available with a right-click on any heading in a note. This replaces the section with a link to a newly-created note containing the section’s content. I find it useful to use a template that loads common libraries, since they will need to be added them at the top of the new files.\nWhen converting your notebooks to notes, be aware that different notes do not share the same runtime. Be sure to include all the variables/calculations necessary for the part of the code you extracted in the new file, as this will not be available from another file’s state. Also, any images that you link to in the markdown sections of the original notebook will need to be manually copied into the vault, as only images generated from code in the file will be exported. ## Closing Thoughts\nI first started using Obsidian for the specific purpose of studying data science. My use of Obsidian broadened considerably and quickly once I first began with it. However, after some time I realized that, because of the nature of iPython notebooks, and the necessity, or really the pleasure, of using them, I found myself many months later in the very position I was trying to avoid vis-a-vis my notes: information I needed was somewhere in my piles of notebooks, and I turned to Google more often than searching through my notebooks.\nNow I can immerse myself in the Python project or study I am focusing on, knowing that after I’m finished, generating proper atomic notes from the work I’m doing will be a breeze. I hope that you may find this information useful. Happy coding!",
    "crumbs": [
      "Data Science",
      "Obsidian and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "posts/data-science/obsidian-jupyter/index.html#motivation",
    "href": "posts/data-science/obsidian-jupyter/index.html#motivation",
    "title": "Obsidian and Jupyter Notebooks",
    "section": "",
    "text": "Jupyter notebooks, or more properly iPython notebooks, are fantastic tools for data exploration and modeling. You can run bits of code, interspersed with blocks of markdown, allowing you to easily work with data and present analyses and forecasts in a visual and interactive format. Notebooks can be easily shared via GitHub, or run on-line with Colab.\nThe problem with iPython notebooks, from a Personal Knowledge Management perspective, is they end up being “books on the shelf”. For Obsidian users and PKM practitioners, the whole point is to get away from keeping information in notebooks, with all the uselessness that that implies. One could, as you will see, just do data exploration directly in Obsidian instead of in a standard iPython notebook. But Obsidian is not an IDE. The ecosystem around the interactive Python notebook IDEs, be it Jupyter Lab itself, or VS Code, etc, is so useful, that working in Obsidian while exploring data and creating models would be unacceptably tedious. Using fit-for-purpose tools is very important for efficiency.\nWe need a painless way to capture all this information in our vault, making notes out of the notebooks. How can we easily make this information future-useful, without repeating/duplicating our efforts or doing a ton of copy/paste? In this article, I’ll explain the solution that works well for me, ensuring Obsidian-speed access to any bits of information I have in my notebooks, as well as making review and study activities so much more pleasant. And, of course, my canvases have all gotten richer as well.",
    "crumbs": [
      "Data Science",
      "Obsidian and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "posts/data-science/obsidian-jupyter/index.html#setting-up-obsidian",
    "href": "posts/data-science/obsidian-jupyter/index.html#setting-up-obsidian",
    "title": "Obsidian and Jupyter Notebooks",
    "section": "",
    "text": "The community plugin needed to accomplish this is called Execute Code, written by Tim Wibiral together with Jupyter and a library called nbconvert. The latter will convert the notebooks to markdown, and the plugin allows you to execute the code directly within a note. To get started, create a virtual environment for Obsidian to use. If you aren’t using virtual environments, please start now! It’s simple, and you will avoid future problems. From the command line, do:\nmkdir -p $HOME/.config/venvs && cd \"$_\"\npython -m venv obsidian_venv\nsource obsidian_venv/bin/activate\npip install --upgrade pip\npip install jupyterlab nbconvert \nYou can install other libraries like Pandas and Matplotlib as well into the virtual environment with pip install. Jupyter lab and nbconvert will be necessary to convert the notebooks to markdown. At this point, you could launch jupyter lab, but there is no need to. After installing packages, you can exit the virtual environment with deactivate. Should you need to install more packages later, you can type source $HOME/.config/venvs/obsidian_venv/bin/activate to re-enter the virtual environment and pip install other packages.\nIn Obsidian, install the Execute Code plugin. After installing the plugin you must point it to the version of Python you want to use, in this case the one we made in the virtual environment above. In the settings for the plugin, under the language-specific settings, choose Python from the drop down list. For Python Path, enter /home/directory/.config/venvs/obsidian_venv/bin/python.\nWith that done, any code block with the keyword python added directly after the opening back ticks of the block can be run in the Note. In Read view, a Run button will appear by each code block, allowing you to execute the code in the block. After execution, there will be a Clear button to clear up output that you want removed from the note. Code can also be executed from Edit view by using the keyword run-python rather than simply python. The plugin offers a command to run all the code in the note, as well as a command to view and kill any active runtimes.",
    "crumbs": [
      "Data Science",
      "Obsidian and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "posts/data-science/obsidian-jupyter/index.html#processing-a-notebook",
    "href": "posts/data-science/obsidian-jupyter/index.html#processing-a-notebook",
    "title": "Obsidian and Jupyter Notebooks",
    "section": "",
    "text": "Jupyter Lab can export an ipynb file directly to markdown! As of writing, VS Code can only export to py, pdf or html. From the file menu, select Export and choose Markdown. This will generate a zipped archive containing a markdown page, along with any image files in the notebook. The problem with this approach is that you will find all of the image files named output something, and so after exporting a few notebooks, there will be name conflicts in your vault.\nUsing the command line avoids this problem, and is in any case much more efficient. You will need to activate your virtual environment with source as described above. Then type\njupyter nbconvert --to Markdown your_notebook.ipynb\nThis will generate an md file which can be copied into your vault. If there are images from generated by the output, like graphs and other visuals, these will be put in a directory created by the above command. If you copy this directory, with all the image files into the vault directory that you use for attachments, the new md file will find them. (It is important to copy the directory itself and not just the files, since the new note will expect to find them there.)\nOnce in Obsidian you can process your Notebooks as any other file, breaking them into bite-size chunks of information. I rely heavily on “Extract this heading”, available with a right-click on any heading in a note. This replaces the section with a link to a newly-created note containing the section’s content. I find it useful to use a template that loads common libraries, since they will need to be added them at the top of the new files.\nWhen converting your notebooks to notes, be aware that different notes do not share the same runtime. Be sure to include all the variables/calculations necessary for the part of the code you extracted in the new file, as this will not be available from another file’s state. Also, any images that you link to in the markdown sections of the original notebook will need to be manually copied into the vault, as only images generated from code in the file will be exported. ## Closing Thoughts\nI first started using Obsidian for the specific purpose of studying data science. My use of Obsidian broadened considerably and quickly once I first began with it. However, after some time I realized that, because of the nature of iPython notebooks, and the necessity, or really the pleasure, of using them, I found myself many months later in the very position I was trying to avoid vis-a-vis my notes: information I needed was somewhere in my piles of notebooks, and I turned to Google more often than searching through my notebooks.\nNow I can immerse myself in the Python project or study I am focusing on, knowing that after I’m finished, generating proper atomic notes from the work I’m doing will be a breeze. I hope that you may find this information useful. Happy coding!",
    "crumbs": [
      "Data Science",
      "Obsidian and Jupyter Notebooks"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/gpx-gps-data/index.html",
    "href": "posts/data-science/python-gpx-mapping/gpx-gps-data/index.html",
    "title": "Trail Mapping with Python",
    "section": "",
    "text": "In the world of Data Science, I’m most strongly drawn to geographically-linked data. Choropleth maps, for example, are about the most powerful way to convey statistical information and get a point across, if you will forgive the intolerable pun. Attaching data to geography somehow makes information more relatable, more personal, and more easily absorbable. It gives people a reference point, a “you are here”, if you will, or maybe “There, but for the grace of God…”.\nI am fortunate to live in New Mexico where I can take beautiful and varied walks, hikes and bike rides nearly every day, which I do. I record most of my excursions with an app on my phone, and I realized recently that I must have lots of data that I can play with to do mapping and analysis. I was pleased to discover that the simple app I use on my phone to track my excursions can easily export the data to GPX format. I had no idea what GPX format was then, but assumed it was some standard, so things looked promising, and off I went.\nThe data I use comes from an app called SportActive. Apps like AllTrails and Strava are wonderful, especially when exploring new places, but they are overkill IMO for simple tracking of daily, and largely repetitive, activities. SportActive simply records my walks and rides, without asking if I want to share my walk with my “friends” like a meal on Facebook. (Disclaimer: I have no financial relationship with SportActive, although if you could arrange such a thing I’d happily change this disclaimer.)\nIn this and two following articles, I will show how the data can be used with Python to map and analyze the GPS information. I will show how to do analyses such as profiling elevations, calculating speeds and durations, identifying pauses, and segmenting paths based on various criteria. This type of analysis, which I’m doing for fun, is the very same as could be used to, for example, study bird migrations or the movement of container ships.\nPython provides many libraries based around the pandas ecosystem which make working with geospatial data easy. GeoPandas extends Pandas to incorporate geometries and coordinate reference systems. GPX data is a series of geolocated points, which is easily handled by geopandas. MovingPandas facilitates turning the point geometries into “trajectories”, allowing for calculations of speed, duration and direction.\nThis article will cover parsing the raw GPS data to a csv file, which I then import into a geopandas GeoDataFrame. From that I will create maps and generate some basic statistical information about the trek such as distance and elevation profiles. These articles assume basic familiarity with Python, experience with Pandas being helpful. An expanded version of the code in the articles can be obtained from my GitHub repository at https://github.com/bisotty666/GPX.",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Trail Mapping with Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/gpx-gps-data/index.html#introduction",
    "href": "posts/data-science/python-gpx-mapping/gpx-gps-data/index.html#introduction",
    "title": "Trail Mapping with Python",
    "section": "",
    "text": "In the world of Data Science, I’m most strongly drawn to geographically-linked data. Choropleth maps, for example, are about the most powerful way to convey statistical information and get a point across, if you will forgive the intolerable pun. Attaching data to geography somehow makes information more relatable, more personal, and more easily absorbable. It gives people a reference point, a “you are here”, if you will, or maybe “There, but for the grace of God…”.\nI am fortunate to live in New Mexico where I can take beautiful and varied walks, hikes and bike rides nearly every day, which I do. I record most of my excursions with an app on my phone, and I realized recently that I must have lots of data that I can play with to do mapping and analysis. I was pleased to discover that the simple app I use on my phone to track my excursions can easily export the data to GPX format. I had no idea what GPX format was then, but assumed it was some standard, so things looked promising, and off I went.\nThe data I use comes from an app called SportActive. Apps like AllTrails and Strava are wonderful, especially when exploring new places, but they are overkill IMO for simple tracking of daily, and largely repetitive, activities. SportActive simply records my walks and rides, without asking if I want to share my walk with my “friends” like a meal on Facebook. (Disclaimer: I have no financial relationship with SportActive, although if you could arrange such a thing I’d happily change this disclaimer.)\nIn this and two following articles, I will show how the data can be used with Python to map and analyze the GPS information. I will show how to do analyses such as profiling elevations, calculating speeds and durations, identifying pauses, and segmenting paths based on various criteria. This type of analysis, which I’m doing for fun, is the very same as could be used to, for example, study bird migrations or the movement of container ships.\nPython provides many libraries based around the pandas ecosystem which make working with geospatial data easy. GeoPandas extends Pandas to incorporate geometries and coordinate reference systems. GPX data is a series of geolocated points, which is easily handled by geopandas. MovingPandas facilitates turning the point geometries into “trajectories”, allowing for calculations of speed, duration and direction.\nThis article will cover parsing the raw GPS data to a csv file, which I then import into a geopandas GeoDataFrame. From that I will create maps and generate some basic statistical information about the trek such as distance and elevation profiles. These articles assume basic familiarity with Python, experience with Pandas being helpful. An expanded version of the code in the articles can be obtained from my GitHub repository at https://github.com/bisotty666/GPX.",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Trail Mapping with Python"
    ]
  },
  {
    "objectID": "posts/data-science/python-gpx-mapping/gpx-gps-data/index.html#next-steps",
    "href": "posts/data-science/python-gpx-mapping/gpx-gps-data/index.html#next-steps",
    "title": "Trail Mapping with Python",
    "section": "Next steps",
    "text": "Next steps\nI naturally want to be able to make calculations of speed and distance, identify pauses, and do other exploration. Starting from discrete points, the steps to do so manually would be simple but exceedingly tedious. Fortunately there is a wonderful library called movingpandas which makes these things all very simple. I’ll explore that in the next articles.\nI’ll go ahead and save the GeoDataFrames for future use:\ntrek_gdf.to_file('data/trek_gdf.gpkg', driver='GPKG')\ntrek_projected.to_file('data/trek_projected.gpkg', driver='GPKG')",
    "crumbs": [
      "Data Science",
      "Python Gpx Mapping",
      "Trail Mapping with Python"
    ]
  },
  {
    "objectID": "posts/linux/linux.html",
    "href": "posts/linux/linux.html",
    "title": "Linux",
    "section": "",
    "text": "I’ve used Linux almost exclusively for over three decades. The first 20 years or so I used Slackware, the oldest distribution still being developed. Since then I have extensively used Fedora, both mutable and immutable versions, as well as Debian and Ubuntu. My current OS of choice is NixOS, which is unlike any other operating system I’m aware of, and very different conceptually. To get an idea of what I mean, read The Linux Different.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNix for Data Scientists\n\n\nNix your venvs and skip pip\n\n\n\nLinux\n\nData Science\n\nNix\n\nPython\n\n\n\nInstalling and maintaining multiple versions of libraries, whether in Python or R, is best managed with Nix\n\n\n\n\n\nMay 29, 2025\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nThe FHS Problem\n\n\nDocker vs Nix\n\n\n\nLinux\n\nDistributions\n\nNix\n\n\n\nTwo different approaches to the problem of the Filesystem Heirarchy Standard\n\n\n\n\n\nMay 4, 2025\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nNixOS: the Linux different\n\n\nGrokking NixOS\n\n\n\nLinux\n\nDistributions\n\n\n\nExploring the concepts behind the unique Linux distribution\n\n\n\n\n\nJun 10, 2024\n\n\nBrian Carey\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "Linux"
    ]
  },
  {
    "objectID": "posts/linux/nix-for-data-science/index.html",
    "href": "posts/linux/nix-for-data-science/index.html",
    "title": "Nix for Data Scientists",
    "section": "",
    "text": "Imagine having a fully provisioned data science environment with all desired libraries and even an IDE with useful extensions, without having to install any packages packages on your system, just by simply placing a single file in a directory and running a single command. What if, by sharing two files, two people can have identical setups, regardless of operating system. Imagine you could add, remove, and upgrade libraries without saying a little prayer not to get the dreaded message that a compatible set of dependencies could not be found. The Nix package manager provides all this, and more, and can be itself installed on any operating system.\nMy computer is very clean. I don’t have any IDEs installed, nor any data science libraries. Well, that’s not really true. Actually I have many versions of each of these installed on my system. But they are only accessible in the directories to which they pertain, although they are not installed in those directories like they are when using virtual environments. And even though Python itself is installed system-wide on my computer, each development directory has its own version of Python.\n\nTraditional Package Management\nPackage management for data scientists working in Python has been notoriously difficult. Pip, Conda, Miniconda, Mamba, ux, poetry, venvwrapper, Docker … the list goes on of the different package managers and solutions are available. They all work, mostly. But they all break, occasionally. None are truly reproducible, and they are all prone to the eventual library conflicts, when one package upgrades its own dependencies, and the upgraded dependencies aren’t compatible with another package. Returning to older projects after some time can be hazardous.\nTo be fair, package management is a complex problem when dozens of packages with shared dependencies need to be installed and maintained. The problem is worse when considering portability to other computers and other operating systems. I should say, it is a complex problem under the traditional FHS paradigm. I have written about the problems caused by the FHS, and the gymnastics required to work around its limitations. The basic problem is that the FHS does not easily accommodate multiple versions of the same package, and the traditional way of installing packages relies heavily on shared libraries.\nVirtual environments go some way to resolve the package management problems. Instead of installing packages system-wide, they are installed in a subdirectory in the project folder. When the virtual environment is active (and you need to remember to manually do so), all relevant environment variables are set to point to these local packages. This way there are no version conflicts between different projects on the same machine.\nBut many problems still remain. Firstly, it doesn’t solve the problem of version conflicts among packages within the same project. Secondly, even with an accompanying requirements.txt, there is no guarantee that someone else will get the exact same libraries as were used in the original project. And then there is the question of interoperablilty between OS. Docker solves that one, but then requires that a Docker or Podman process be running the whole time to manage the container, and each Docker image contains and runs a full blown operating system, with no version guarantees about that.\nNix, uniquely, sidesteps all these problems by abandoning the whole FHS paradigm with shared libraries, installing everything needed by a package, and not relying on anything else being installed elsewhere.\n\n\nNix package management\nWith Nix, a fully provisioned development environment is declared in a text file, which is basically a list of packages you want, although it is actually a functional program(!). It defines a graph of all of the dependencies and installs them in a single /nix/store directory. Nix prepends the hash values of the packages/libraries (actually called derivations) to the to each filename in the store, ensuring no collisions between different versions of any particular library, and that each package has its own requirements satisfied independently.\nSomething else to note: by its nature, all installations are atomic, so either everything is installed or nothing is installed. Under the traditional way of installing elements sequentially through a series of commands, you can end up with partially installed packages. And on the other end, removing packages is a clean procedure. Nix installs packages declaratively, unlike the traditional imperative method.\nThe flake.nix file can create the development environments automatically when you enter a directory with a mechanism called direnv, avoiding the need to invoke the environment at all. Within this directory, and any sub-directories, you can launch your IDE of choice, and have access to all the libraries and tools you have declared. When you leave the directory you leave the shell, and you no longer have access to the packages in the directory. Another directory can have an entirely different set of packages, with potentially entirely different version numbers for packages, and there is no conflict.\nNix does have its challenges, notable around documentation, which is generally poor and inconsistent, but improving all the time.\n\n\nGive it a try\nNixpkgs, the repository for Nix, is actually the largest software repository of all, hosting even more packages than Arch’s AUR. Nix can be installed on Linux, Mac, or Windows WSL. Official installation instructions can be found here, but I recommend using Zero to Nix to install Nix because it will configure the flakes feature for you. Installation boils down to running a simple command. Once Nix is installed, try typing nix shell nixpkgs#floorp to try out a fun web browser. When you are done, exit the shell and program disappears. You can actually use it as the package manager for your system if you want instead of your native package manager, and I know many Mac users prefer it to homebrew.\nTo jump-start your experience, here is a flake for a Python data science environment. By placing this file in a directory and running nix develop, you will have Jupyter Lab, VS Code with relevant extensions, and some basic data science libraries. To “install” more packages, just add them to the list and run nix develop again. Even better, install direnv, and the environment will automatically load when you enter the directory and unload when you leave it. Unfortunately, there are just a few data science packages which are not available on nixpkgs. The flake provides the ability to pip install these, and these packages will be available along with the other packages without a need to activate an additional venv.\nHappy coding!\n```{nix}\n# flake.nix\n{\n  description = \"Python Data Science\";\n  inputs = {\n    nixpkgs = {\n      url = \"github:nixos/nixpkgs/nixos-unstable\";\n    };\n    flake-utils = {\n      url = \"github:numtide/flake-utils\";\n    };\n  };\n  outputs = { nixpkgs, flake-utils, ... }: flake-utils.lib.eachDefaultSystem (system:\n    let\n      pkgs = import nixpkgs {\n        inherit system;\n        config.allowUnfree = true;\n      };\n      myvscode = pkgs.vscode-with-extensions.override {\n        vscodeExtensions = (with pkgs.vscode-extensions; [\n        equinusocio.vsc-material-theme\n        equinusocio.vsc-material-theme-icons\n        vscodevim.vim\n        ms-python.python\n        ms-python.vscode-pylance\n        ms-toolsai.jupyter\n        ms-toolsai.jupyter-renderers\n        ]);\n      };\n    in {\n      devShell = pkgs.mkShell {\n        name = \"python-venv\";\n        venvDir = \"./.venv\";\n        buildInputs = with pkgs; [\n          tmux\n          myvscode\n          (python3.withPackages(ps: with ps; [\n            ipython\n            pip\n            jupyter\n            widgetsnbextension\n            jupyter-nbextensions-configurator\n            jedi-language-server\n            ipywidgets\n            mypy\n            notebook\n            pandas\n            numpy\n            matplotlib\n            seaborn\n          ]))\n        ];\n        postVenvCreation = ''\n          unset SOURCE_DATE_EPOCH\n          pip install -r requirements.txt\n        '';\n\n        shellHook = ''\n            # export BROWSER=brave\n            # Tells pip to put packages into $PIP_PREFIX instead of the usual locations.\n            export PIP_PREFIX=$(pwd)/_build/pip_packages\n            export PYTHONPATH=\"$PIP_PREFIX/${pkgs.python3.sitePackages}:$PYTHONPATH\"\n            export PATH=\"$PIP_PREFIX/bin:$PATH\"\n            unset SOURCE_DATE_EPOCH\n            # jupyter lab  # uncomment to automatically launch jupyter\n        '';\n\n        postShellHook = ''\n          # allow pip to install wheels\n          unset SOURCE_DATE_EPOCH\n        '';\n      };\n    }\n  );\n}\n\n```\n\n\n\n\n Back to top",
    "crumbs": [
      "Linux",
      "Nix for Data Scientists"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-headers/index.html",
    "href": "posts/obsidian/obsidian-headers/index.html",
    "title": "Obsidian Basics - Headers",
    "section": "",
    "text": "Besides the actual content of a note, the most important elements in notes are the headers. More than just enhancing visual presentation of your information, headers give access to a variety of useful functionality. In this article, I will discuss the value of headers as well as the mechanics of using them.",
    "crumbs": [
      "Obsidian",
      "Obsidian Basics - Headers"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-headers/index.html#structure",
    "href": "posts/obsidian/obsidian-headers/index.html#structure",
    "title": "Obsidian Basics - Headers",
    "section": "Structure",
    "text": "Structure\nHeaders allow you to divide your note in logical sections. You can use them like you would an outline, as described above. Every note should, at a minimum, a level one header at the beginning with a title for the note.\nObsidian helps me organize my thoughts. A note often “starts life” as a sentence or two that I “jotted down” when the thought came to me. When I return to the note, if I still find it interesting, the first thing I do is give it a title (level one header). It summarizes the purpose of the note succinctly, and for a simple, atomic note, that may be all I need, although I might want to include some reference information, in which case I create two level-two headers, one for content, one for references. And so on…\nHowever, for more complex notes which are not atomic, developing the note almost always involves breaking down the idea into parts. This is the purpose of headers. If you’re one of those people who like to start with an outline, start a new document with a bunch of headers. Even if you don’t, as you work on documents you will naturally need to break things down into smaller ideas…headers help you structure your thinking.",
    "crumbs": [
      "Obsidian",
      "Obsidian Basics - Headers"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-headers/index.html#navigation",
    "href": "posts/obsidian/obsidian-headers/index.html#navigation",
    "title": "Obsidian Basics - Headers",
    "section": "Navigation",
    "text": "Navigation\nIf you open your right sidebar and click on the Outline icon, you will see the table of contents panel with an outline created by your headers. Clicking on any item will take you to that section of the document. Subsections can be folded in the panel by clicking on the down arrow to the left of the section. This is very useful for long documents.",
    "crumbs": [
      "Obsidian",
      "Obsidian Basics - Headers"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-headers/index.html#folding",
    "href": "posts/obsidian/obsidian-headers/index.html#folding",
    "title": "Obsidian Basics - Headers",
    "section": "Folding",
    "text": "Folding\nIf you move your cursor over a header in your document, you will see a down arrow to the left of the header. This is a toggle which allows you to collapse (hide) or expand (show) a section’s content.\nA section includes all content up to the next Header of the same level or higher. So a level 3 section would include all content up until the next level 3 Header, or a level 2 or 1 Header. This can be confusing to explain, but if you experiment a little it should be clear.\nThere are commands that allow you to work more easily with folds. If you go into the command palette (Ctrl-P) you will see four commands which allow you to expand and collapse folds.\n\nThese commands are really only useful if you assign them hotkeys. If you do so you will find that navigating long documents becomes very quick, assuming that Headers are well-used.",
    "crumbs": [
      "Obsidian",
      "Obsidian Basics - Headers"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-headers/index.html#hooks-for-links",
    "href": "posts/obsidian/obsidian-headers/index.html#hooks-for-links",
    "title": "Obsidian Basics - Headers",
    "section": "Hooks for Links",
    "text": "Hooks for Links\nThe final aspect of headers I’d like to mention is their use as reference hooks for links. When linking to another note, you can directly link to a header and therefore to a section. When creating a link, directly after the name of the note and inside the square brackets, you can type # and you will get a drop-down list of all the headers in the note. You can select one of these headers. Then the embedded document will only display that section in your current note. Similarly, hovering over the link and pressing Ctrl (if using the Hover Editor plugin) will display that section. More advanced plugins like dataview can also reference Headers.\nNB. You can also link directly to paragraphs by using #^ after the note’s name.",
    "crumbs": [
      "Obsidian",
      "Obsidian Basics - Headers"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html",
    "href": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html",
    "title": "Interactive Tables with DataviewJS",
    "section": "",
    "text": "This article is a follow-up to my Gentle Introduction to DataviewJS articles, and assumes that you have read them. It is written for anglers, but if you just want the fish you can Tldr; your way to the bottom an just grab the code. Even if you are here to learn how to fish, depending on your learning style, you may want to take a look at the complete code first before this walk-through.\nWe are now ready to start building a fully dynamic and interactive dashboard to manage the exercise logs. In this case, I’m tracking the following:\n\ntype of activity\ndistance\nduration\ndate of activity\nroute\n\nI have optional fields for notes and, as you can see, for images. Summary tables are generated using calculated values. The entire canvas updates automatically to the current day.\nThis example could easily be extended for tracking anything which includes numerical data and images. Imagine a research project, for example, where observations are taken combining numerical data, textual observations, and photos. Actually creating the log entries, thanks to Metadata Menu and Unique Notes, takes almost no time. You can read about how to do this here.\nIn this article, I’ll focus on creating this dynamic table.\n\nAppropriate widgets are provided allowing for direct editing or the use of popup modals for multi-select fields and dates. The list will display all logs over the last 7 days, so different logs will appear depending on the day.\nTo do this, we will use the excellent JavaScript functionality provided by Metadata Menu. It would be more interesting were you to create a bunch of logs (notes) covering a date range of at least three months and follow along. They should all contain the following fields:\n\ntopic\ntype\nRoute\nActivity\nDuration\nDistance\nNote\nImage\nLink\n\n\n\n\nThroughout this dashboard I work with time. It is natural to want to look at time periods of weeks and months, and to compare current periods to prior periods of the same length. We, just as naturally, compare a week starting Monday or Sunday to a prior week starting on the same day. And we compare February to January. This is often a good thing, because it fits the way people naturally think.\nBut if you are really looking at the numbers, you quickly see the flaw in comparing time periods this way. You can’t compare a complete week to the prior week until the end of the week. To get around this, people use week-to-date, or quite commonly month-to-date. For months the problem is worse, as in the case of Feb/Jan comparison that’s 28 days compared to 31 days. That’s more than a 10% difference in days, and therefor in data!\nA simple way around this, and one which provides better analysis, is to use rolling windows of time. Any seven day period contains every day of the week, and so comparing two of any such periods valid (and complete) week-to-week comparisons. For months, you can compare any 30 (or 28) day period to the prior period of equal length, so you get a full, month-long picture with apples-to-apples data (that’s a technical term😉).\nIn the first article I introduced moment(), an object provided by Dataview (by MomentJS actually). This makes time math easy. moment() itself means today, now, this very second. If you want the date/time two days ago, you can subtract two days with moment().subtract(2, \"days\"). I’ll leave it as an exercise to the reader to figure out how to find the date two months ago.\nIn order to use moment() for any date other than now, you need to write, eg., moment(new Date(\"2023-10-20\"). We will do this so that we can format the date differently using the format() function in moment(). format() wants an argument, a date format string. I don’t need to see the year, and a 2-digit month is fine and shorter. On the other hand, seeing the day of the week would be useful. The format string to produce “10/10 Tue” is “MM/DD ddd”. ### Challenge As an exercise, you might pause here and think about how to display “two days ago” in the format “10/10 Tue”. You have all the knowledge you need. Think about chaining commands together with the period. ### Solution\nHere is a solution:\nconst dateFormat = \"MM/DD ddd\" const twoDaysAgo =\nmoment() .subtract(2, \"days\") .format(dateFormat)\n\ndv.paragraph(twoDaysAgo)\nWhen writing, and especially reading, code, formatting is important. I could have written\nconst twoDaysAgo = moment().subtract(2, \"days\").format(dateFormat)\nbut it is much less clear what is going on. If you are trying to understand a piece of code, start by reformatting. ## Asynchronous Functions\nMost computer programs execute line by line, with each line completing before moving on to the next line. Normally this is what you want. However, as the quantity of data commonly worked with increases geometrically, and the fact that it is often distributed widely across the internet, applications which require fetching data can easily grind to a halt due to a slow internet connection, slow servers, large quantities, etc. To overcome this problems, some functions are executed asynchronously. When an asynchronous function is executed, it doesn’t return the actual data. Instead, it returns what is called a promise. That allows the program to continue executing while the data is being fetched. It will continue execution until the data is actually required, at which point it will wait (not freeze).\nIn practice this is quite easy, involving the use of two new key words: async and await. The first, async, is added to the function definition, and the second, await, is added before any data fetching. That’s all you need to know so that you will understand the words when you see them.\n\n\n\nMetadata Menu provides an asynchronous function for making the tables interactive. To use the function, you must import it with\nconst {fieldModifier: f} = this.app.plugins.plugins[\"metadata-menu\"].api\nUsing the curly braces like this is called deconstructing a variable or function. This means that somewhere there is a function called fieldModifier. The : f is just an alias, so that every time you use the function you don’t need to write out fieldModifier, you can just write f.\nThe other part is interesting. Take it step by step. this is the root directory of your vault. app is the hidden obsidian directory, inside of which there is, you guessed it, a plugins directory with an entry called metadata-menu. The rest, plugins[\"metadata-menu\"].api says to look in the metadata-menu directory for something called api. This object will contain a function called fieldModifier, which we can now refer to simply as f.\nfieldModifier(), which is f() to us now, is used in map() and takes three arguments: the dv object, whatever temporary variable you are using in map() (we have been using b), and the name of the field we want. Were it a normal function you would write\ndataviewjs dv.pages()     \n  .map(b=&gt;[p.ActivityDate, \n  f(dv, b, \"Activity\"),\n  ]\nSince this fetching of date is asynchronous, you have to use the key words async/await here, so\ndataviewjs dv.pages()     \n  .map(async b=&gt;[\n    p.ActivityDate,\n    await f(dv, b, \"Activity\"),\n    ]\nThe entire section containing pages().map needs to be wrapped in an asynchronous function called Promise.all(), which must be (a)waited for and returns, unsurprisingly, a promise.\nawait Promise.all(dv.pages()\n  .map(async b=&gt;[\n    p.ActivityDate,\n    await f(dv, b, \"Activity\"),\n    ])\n\n\n\nNow, lets finally take a look at the entire code that creates the dashboard. Take some time to read through it. Everything should be understandable at this point.\nconst {fieldModifier: f} =\nthis.app.plugins.plugins\\[\"metadata-menu\"\\].api\n\nconst dateFormat = \"MM/DD ddd\"\n\ndv.header(3, \"7 Day Details\")\n\ndv.table(\\[\"🗓️\",\"🚶🚴\", \"📓\", \"⏱️\", \"🗺️\"\\],\n\nawait Promise.all(dv.pages()\n    .where(b =&gt; b.type == \"log\")\n    .where(b =&gt; b.ActivityDate &gt;= moment().subtract(7, \"days\"))\n    .sort(p =&gt; p.ActivityDate, \"desc\")\n    .map(async b=&gt;[\n        moment(new Date(b.ActivityDate)).format(dateFormat),\n        await f(dv, b, \"Activity\"), \n        await f(dv, b, \"Distance\"), \n        await f(dv, b, \"Duration\"), \n        await f(dv, b, \"Route\"),\n    ]\n  )\n)\n\n\n\nIn the next article I will explain how to generate summary information like totals and averages for display on the dashboard. In the mean time I strongly suggest that you do two things:\n\nAdd images to some of your logs and create the table for the Gallery section of the dashboard\nTurn some of your existing Dataview tables into DataviewJS tables. Or, at least, make some new ones from your own information\nSpend some time working with date ranges. In this example, we selected the past seven days. How can you select the seven days prior to that? (Hint: you need to chain together two where() clauses.)\nCan you create a month-to-date view? There are various ways to do it, but it’s helpful to know that you can do moment().startOf('month') to grab the first day of the month. Check out the documentation for more possibilities.\n\nHappy coding!",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Interactive Tables with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#introduction",
    "href": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#introduction",
    "title": "Interactive Tables with DataviewJS",
    "section": "",
    "text": "This article is a follow-up to my Gentle Introduction to DataviewJS articles, and assumes that you have read them. It is written for anglers, but if you just want the fish you can Tldr; your way to the bottom an just grab the code. Even if you are here to learn how to fish, depending on your learning style, you may want to take a look at the complete code first before this walk-through.\nWe are now ready to start building a fully dynamic and interactive dashboard to manage the exercise logs. In this case, I’m tracking the following:\n\ntype of activity\ndistance\nduration\ndate of activity\nroute\n\nI have optional fields for notes and, as you can see, for images. Summary tables are generated using calculated values. The entire canvas updates automatically to the current day.\nThis example could easily be extended for tracking anything which includes numerical data and images. Imagine a research project, for example, where observations are taken combining numerical data, textual observations, and photos. Actually creating the log entries, thanks to Metadata Menu and Unique Notes, takes almost no time. You can read about how to do this here.\nIn this article, I’ll focus on creating this dynamic table.\n\nAppropriate widgets are provided allowing for direct editing or the use of popup modals for multi-select fields and dates. The list will display all logs over the last 7 days, so different logs will appear depending on the day.\nTo do this, we will use the excellent JavaScript functionality provided by Metadata Menu. It would be more interesting were you to create a bunch of logs (notes) covering a date range of at least three months and follow along. They should all contain the following fields:\n\ntopic\ntype\nRoute\nActivity\nDuration\nDistance\nNote\nImage\nLink",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Interactive Tables with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#time-math",
    "href": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#time-math",
    "title": "Interactive Tables with DataviewJS",
    "section": "",
    "text": "Throughout this dashboard I work with time. It is natural to want to look at time periods of weeks and months, and to compare current periods to prior periods of the same length. We, just as naturally, compare a week starting Monday or Sunday to a prior week starting on the same day. And we compare February to January. This is often a good thing, because it fits the way people naturally think.\nBut if you are really looking at the numbers, you quickly see the flaw in comparing time periods this way. You can’t compare a complete week to the prior week until the end of the week. To get around this, people use week-to-date, or quite commonly month-to-date. For months the problem is worse, as in the case of Feb/Jan comparison that’s 28 days compared to 31 days. That’s more than a 10% difference in days, and therefor in data!\nA simple way around this, and one which provides better analysis, is to use rolling windows of time. Any seven day period contains every day of the week, and so comparing two of any such periods valid (and complete) week-to-week comparisons. For months, you can compare any 30 (or 28) day period to the prior period of equal length, so you get a full, month-long picture with apples-to-apples data (that’s a technical term😉).\nIn the first article I introduced moment(), an object provided by Dataview (by MomentJS actually). This makes time math easy. moment() itself means today, now, this very second. If you want the date/time two days ago, you can subtract two days with moment().subtract(2, \"days\"). I’ll leave it as an exercise to the reader to figure out how to find the date two months ago.\nIn order to use moment() for any date other than now, you need to write, eg., moment(new Date(\"2023-10-20\"). We will do this so that we can format the date differently using the format() function in moment(). format() wants an argument, a date format string. I don’t need to see the year, and a 2-digit month is fine and shorter. On the other hand, seeing the day of the week would be useful. The format string to produce “10/10 Tue” is “MM/DD ddd”. ### Challenge As an exercise, you might pause here and think about how to display “two days ago” in the format “10/10 Tue”. You have all the knowledge you need. Think about chaining commands together with the period. ### Solution\nHere is a solution:\nconst dateFormat = \"MM/DD ddd\" const twoDaysAgo =\nmoment() .subtract(2, \"days\") .format(dateFormat)\n\ndv.paragraph(twoDaysAgo)\nWhen writing, and especially reading, code, formatting is important. I could have written\nconst twoDaysAgo = moment().subtract(2, \"days\").format(dateFormat)\nbut it is much less clear what is going on. If you are trying to understand a piece of code, start by reformatting. ## Asynchronous Functions\nMost computer programs execute line by line, with each line completing before moving on to the next line. Normally this is what you want. However, as the quantity of data commonly worked with increases geometrically, and the fact that it is often distributed widely across the internet, applications which require fetching data can easily grind to a halt due to a slow internet connection, slow servers, large quantities, etc. To overcome this problems, some functions are executed asynchronously. When an asynchronous function is executed, it doesn’t return the actual data. Instead, it returns what is called a promise. That allows the program to continue executing while the data is being fetched. It will continue execution until the data is actually required, at which point it will wait (not freeze).\nIn practice this is quite easy, involving the use of two new key words: async and await. The first, async, is added to the function definition, and the second, await, is added before any data fetching. That’s all you need to know so that you will understand the words when you see them.",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Interactive Tables with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#metadata-menu",
    "href": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#metadata-menu",
    "title": "Interactive Tables with DataviewJS",
    "section": "",
    "text": "Metadata Menu provides an asynchronous function for making the tables interactive. To use the function, you must import it with\nconst {fieldModifier: f} = this.app.plugins.plugins[\"metadata-menu\"].api\nUsing the curly braces like this is called deconstructing a variable or function. This means that somewhere there is a function called fieldModifier. The : f is just an alias, so that every time you use the function you don’t need to write out fieldModifier, you can just write f.\nThe other part is interesting. Take it step by step. this is the root directory of your vault. app is the hidden obsidian directory, inside of which there is, you guessed it, a plugins directory with an entry called metadata-menu. The rest, plugins[\"metadata-menu\"].api says to look in the metadata-menu directory for something called api. This object will contain a function called fieldModifier, which we can now refer to simply as f.\nfieldModifier(), which is f() to us now, is used in map() and takes three arguments: the dv object, whatever temporary variable you are using in map() (we have been using b), and the name of the field we want. Were it a normal function you would write\ndataviewjs dv.pages()     \n  .map(b=&gt;[p.ActivityDate, \n  f(dv, b, \"Activity\"),\n  ]\nSince this fetching of date is asynchronous, you have to use the key words async/await here, so\ndataviewjs dv.pages()     \n  .map(async b=&gt;[\n    p.ActivityDate,\n    await f(dv, b, \"Activity\"),\n    ]\nThe entire section containing pages().map needs to be wrapped in an asynchronous function called Promise.all(), which must be (a)waited for and returns, unsurprisingly, a promise.\nawait Promise.all(dv.pages()\n  .map(async b=&gt;[\n    p.ActivityDate,\n    await f(dv, b, \"Activity\"),\n    ])",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Interactive Tables with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#the-table",
    "href": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#the-table",
    "title": "Interactive Tables with DataviewJS",
    "section": "",
    "text": "Now, lets finally take a look at the entire code that creates the dashboard. Take some time to read through it. Everything should be understandable at this point.\nconst {fieldModifier: f} =\nthis.app.plugins.plugins\\[\"metadata-menu\"\\].api\n\nconst dateFormat = \"MM/DD ddd\"\n\ndv.header(3, \"7 Day Details\")\n\ndv.table(\\[\"🗓️\",\"🚶🚴\", \"📓\", \"⏱️\", \"🗺️\"\\],\n\nawait Promise.all(dv.pages()\n    .where(b =&gt; b.type == \"log\")\n    .where(b =&gt; b.ActivityDate &gt;= moment().subtract(7, \"days\"))\n    .sort(p =&gt; p.ActivityDate, \"desc\")\n    .map(async b=&gt;[\n        moment(new Date(b.ActivityDate)).format(dateFormat),\n        await f(dv, b, \"Activity\"), \n        await f(dv, b, \"Distance\"), \n        await f(dv, b, \"Duration\"), \n        await f(dv, b, \"Route\"),\n    ]\n  )\n)",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Interactive Tables with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#next-steps",
    "href": "posts/obsidian/dataview/dataviewjs-summaries-statistics/index.html#next-steps",
    "title": "Interactive Tables with DataviewJS",
    "section": "",
    "text": "In the next article I will explain how to generate summary information like totals and averages for display on the dashboard. In the mean time I strongly suggest that you do two things:\n\nAdd images to some of your logs and create the table for the Gallery section of the dashboard\nTurn some of your existing Dataview tables into DataviewJS tables. Or, at least, make some new ones from your own information\nSpend some time working with date ranges. In this example, we selected the past seven days. How can you select the seven days prior to that? (Hint: you need to chain together two where() clauses.)\nCan you create a month-to-date view? There are various ways to do it, but it’s helpful to know that you can do moment().startOf('month') to grab the first day of the month. Check out the documentation for more possibilities.\n\nHappy coding!",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "Interactive Tables with DataviewJS"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html",
    "href": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html",
    "title": "DataviewJS: A Gentle Introduction",
    "section": "",
    "text": "This article is intended for people who are new to programming, but can serve as a jumping off point for experienced programmers new to JavaScript in general or DataviewJS in particular. The documentation can be found here.\nYou may be asking yourself, “Why learn JavaScript since I have Dataview queries to make lists and tables with my data”? You need JavaScript if, for example, you want interactive tables which allow you to edit data directly from the table without opening the associated note (very important in an information-first, NoSQL style). Or if you want to work with relative dates, eg. “last week”, without needing to update your queries every week. JavaScript allows you to show things like totals, counts and averages together with your tables. And since you are writing JavaScript anyway, you can fully customize the way everything is displayed.\nJavaScript may seem intimidating, but it’s really not too complex or complicated for what we need to do in Obsidian. The basic examples I start with here won’t display any tables, but will get you comfortable with some of the basic concepts and how to write simple JavaScript code. It is very important that you do these examples yourself in your own vault. Like any language, the only way to learn it is to use it (and make mistakes).\n\n\nJavaScript is a general purpose programming language not unlike Python, but quite different from the confusingly-named Java programming language to which it bears no relation. It is responsible for most of the dynamic content on the internet. As happens with human languages spoken widely, JavaScript has evolved a variety of “regional dialects”, sharing a common grammar and basic lexicon, but adding words and idioms to work in specific environments. DataviewJS is one such dialect, one which provides a vocabulary specifically tailored for Obsidian. It does this by giving us a Dataview object, but more on that later.\nThis will be a learn-by-example sort of thing, but it’s worth mentioning a few things up front for those totally new to this. In JavaScript, spaces, tabs and line breaks don’t matter. You could write your script on one line, but that would be silly. I suggest that you use spaces and line breaks liberally, because it will make your code much easier to write, read and quickly understand when you look back at it later. You will also see some people using semi-colons at the end of lines. This, too, is optional. Finally, variables can be named however you like, but the convention for multi-word variables is to capitalize the first letter of every word except the first, like theVariableName.\n(Fun fact: this naming convention is called “camel case”, with the capital letters seen as humps protruding from the back of a camel. Two humps can be seen in one form, CamelCase. The form used in JavaScript, camelCase is specifically called “dromedary case” for apparent reasons.)\nOne thing about JavaScript which took me a while to get used to at first was all of the curly braces ({}). All that they indicate is that there is a block of JavaScript code inside, usually with multiple lines. While we’re on the subject of braces, the square brackets ([]) are used when you want a list of things. Just separate each item by commas, and put quotations around each item, like [\"Item 1\", \"Item2\"]. You often want to go through lists item by item (a process called iteration), and we will see later how to do this. Apostrophes (') and quotations (\") can be used interchangeably to wrap text. Don’t worry, we’ll go over all this later.\n\n\n\nThe first program you traditionally write in any programming language is “Hello World”, which displays, not unsurprisingly, “Hello World” as output. Here is such a one in DataviewJS:\ndv.paragraph(\"Hello World\")\nThe output looks like this:\nHello World\nYeah! You are now a JavaScript programmer! This simple example introduces some important concepts, though.\nFirst, when you want to write some JavaScript, you must use a code block with the key word dataviewjs. While you are writing code, it can be convenient to start the code block with js dataviewjs rather than simply dataviewjs. You will get nice syntax highlighting that way, and can simply remove the first js when you want to run your code.\nAs you might have guessed, paragraph just means to display the text in parentheses as a paragraph. Technically, dv.paragraph() is a command, or function, and the part in parentheses is called the arguments. Note the quotations surrounding the text.\nSo what’s this dv thing? Remember when I said that each flavor of JavaScript provides special vocabulary for it’s context? They do this typically by providing an “object” containing the functionality. dv is what is called the Dataview object, and anytime you want to access the functionality of DataviewJS you indicate that by starting your statement or phrase with dv followed by a period. Since displaying a paragraph on a page of markdown is specific to DataviewJS, we write dv.paragraph()\nIn addition to paragraphs, the dv object allows us to display headers, lists, tables, etc. In fact, the dv object can display any HTML element. You can display multiple elements by putting them one after another in your code.\nLet’s look at another example:\ndv.header(2, \"Introduction to DataviewJS\")\ndv.paragraph('\"Hi, my name is DataviewJS\"')\n\n\n\nHi, my name is DataviewJS\nSince it’s hard to show the result naturally, I’ve included screenshots from my vault with the code on the right and the results on the left.\n\nHere I’m displaying 2 elements. Headers require two arguments, the first indicating the header level. This example is the same as writing ## Introduction to DataviewJS. Note that we don’t use quotations around numbers (usually). The dv object also allows us to display lists, like this:\ndv.list([\"Item 1\", \"Item 2\", \"Item 3\"])\nRemember that, when making lists in JavaScript, you use the square brackets and separate each item with a comma. This is also called an array. This is a pretty simple example, but sometimes things get a lot more complicated, and since spaces and new lines don’t matter, it is good practice to write the same code like this:\ndv.list([\n    \"Item 1\",\n    \"Item 2\",\n    \"Item 3\",\n])\nThis makes it much easier to see what is going on, and also makes copying and pasting of individual items easier, too.\n\n\n\nWell, you may not be too impressed so far. Nothing we’ve done yet has been anything we couldn’t have done by simply writing the markdown. Things get more interesting when we start using variables. These allows us to create and manipulate information before displaying it. Variables are usually made (declared) with either const or let in JavaScript. So const myName = \"Brian\" creates a variable called myName with the value of “Brian”. We can then write this:\nconst myName = \"Brian\"\ndv.paragraph(\"Hi, my name is \" + myName + \".\")\nwhich produces\nHi, my name is Brian.\nWhen used with text, called strings in coding lingo, the + puts together (concatenates) multiple strings. You can see that a long string with multiple variables could start to look messy. A more convenient way to combine text and variables is to use back ticks instead of quotes, and put the variable names in curly braces preceded with the dollar sign. So instead we can write:\nconst myName = \"Brian\"\ndv.paragraph(`Hi, my name is ${myName}.`)\nto get the same result.\nYou could still object, no pun intended, that we haven’t done anything we couldn’t have done just by writing out the sentence. We’ll get to that in the next article, but before closing this one, I’ll show you something you can only do with JavaScript. DataviewJS comes with another JavaScript flavor baked in called MomentJS. Like DataviewJS provides a dv object, MomentJS provides a moment object which makes it easy to work with dates and times. We’ll talk a lot more about this later, but for now you can use it like this:\nconst today = moment(Date.now()).format(\"MMMM DD, YYYY\")\ndv.paragraph(`Today is ${today}`)\nToday is October 10, 2023\nThis display’s today’s date in a format specified by argument to the format() function. Every time you open a note or canvas, the current day will be displayed.\nPutting it all together, we can write\nconst myName = \"Brian\"\nconst today = moment(Date.now()).format(\"MMMM DD, YYYY\")\nconst learnings = [\n    \"JavaScript basics\",\n    \"The dataview object\",\n    \"Displaying elements with JavaScript\",\n    \"JavaScript variables\",\n]\ndv.header(3, `Things ${myName} learned on ${today}`)\ndv.list(learnings)\nto produce\n\n\n\nJavaScript basics\nThe dataview object\nDisplaying elements with JavaScript\nJavaScript variables\n\n\n\n\n\n\nIn the next article I’ll start looking at using DataviewJS to make lists and tables and otherwise use the information in your vault. Meanwhile, if you want to learn more about working with dates, you can visit the Moment.js website. And the DataviewJS documentation can be found here. Happy coding!",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#javascript",
    "href": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#javascript",
    "title": "DataviewJS: A Gentle Introduction",
    "section": "",
    "text": "JavaScript is a general purpose programming language not unlike Python, but quite different from the confusingly-named Java programming language to which it bears no relation. It is responsible for most of the dynamic content on the internet. As happens with human languages spoken widely, JavaScript has evolved a variety of “regional dialects”, sharing a common grammar and basic lexicon, but adding words and idioms to work in specific environments. DataviewJS is one such dialect, one which provides a vocabulary specifically tailored for Obsidian. It does this by giving us a Dataview object, but more on that later.\nThis will be a learn-by-example sort of thing, but it’s worth mentioning a few things up front for those totally new to this. In JavaScript, spaces, tabs and line breaks don’t matter. You could write your script on one line, but that would be silly. I suggest that you use spaces and line breaks liberally, because it will make your code much easier to write, read and quickly understand when you look back at it later. You will also see some people using semi-colons at the end of lines. This, too, is optional. Finally, variables can be named however you like, but the convention for multi-word variables is to capitalize the first letter of every word except the first, like theVariableName.\n(Fun fact: this naming convention is called “camel case”, with the capital letters seen as humps protruding from the back of a camel. Two humps can be seen in one form, CamelCase. The form used in JavaScript, camelCase is specifically called “dromedary case” for apparent reasons.)\nOne thing about JavaScript which took me a while to get used to at first was all of the curly braces ({}). All that they indicate is that there is a block of JavaScript code inside, usually with multiple lines. While we’re on the subject of braces, the square brackets ([]) are used when you want a list of things. Just separate each item by commas, and put quotations around each item, like [\"Item 1\", \"Item2\"]. You often want to go through lists item by item (a process called iteration), and we will see later how to do this. Apostrophes (') and quotations (\") can be used interchangeably to wrap text. Don’t worry, we’ll go over all this later.",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#hello-world",
    "href": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#hello-world",
    "title": "DataviewJS: A Gentle Introduction",
    "section": "",
    "text": "The first program you traditionally write in any programming language is “Hello World”, which displays, not unsurprisingly, “Hello World” as output. Here is such a one in DataviewJS:\ndv.paragraph(\"Hello World\")\nThe output looks like this:\nHello World\nYeah! You are now a JavaScript programmer! This simple example introduces some important concepts, though.\nFirst, when you want to write some JavaScript, you must use a code block with the key word dataviewjs. While you are writing code, it can be convenient to start the code block with js dataviewjs rather than simply dataviewjs. You will get nice syntax highlighting that way, and can simply remove the first js when you want to run your code.\nAs you might have guessed, paragraph just means to display the text in parentheses as a paragraph. Technically, dv.paragraph() is a command, or function, and the part in parentheses is called the arguments. Note the quotations surrounding the text.\nSo what’s this dv thing? Remember when I said that each flavor of JavaScript provides special vocabulary for it’s context? They do this typically by providing an “object” containing the functionality. dv is what is called the Dataview object, and anytime you want to access the functionality of DataviewJS you indicate that by starting your statement or phrase with dv followed by a period. Since displaying a paragraph on a page of markdown is specific to DataviewJS, we write dv.paragraph()\nIn addition to paragraphs, the dv object allows us to display headers, lists, tables, etc. In fact, the dv object can display any HTML element. You can display multiple elements by putting them one after another in your code.\nLet’s look at another example:\ndv.header(2, \"Introduction to DataviewJS\")\ndv.paragraph('\"Hi, my name is DataviewJS\"')",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#introduction-to-dataviewjs",
    "href": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#introduction-to-dataviewjs",
    "title": "DataviewJS: A Gentle Introduction",
    "section": "",
    "text": "Hi, my name is DataviewJS\nSince it’s hard to show the result naturally, I’ve included screenshots from my vault with the code on the right and the results on the left.\n\nHere I’m displaying 2 elements. Headers require two arguments, the first indicating the header level. This example is the same as writing ## Introduction to DataviewJS. Note that we don’t use quotations around numbers (usually). The dv object also allows us to display lists, like this:\ndv.list([\"Item 1\", \"Item 2\", \"Item 3\"])\nRemember that, when making lists in JavaScript, you use the square brackets and separate each item with a comma. This is also called an array. This is a pretty simple example, but sometimes things get a lot more complicated, and since spaces and new lines don’t matter, it is good practice to write the same code like this:\ndv.list([\n    \"Item 1\",\n    \"Item 2\",\n    \"Item 3\",\n])\nThis makes it much easier to see what is going on, and also makes copying and pasting of individual items easier, too.",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#variables",
    "href": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#variables",
    "title": "DataviewJS: A Gentle Introduction",
    "section": "",
    "text": "Well, you may not be too impressed so far. Nothing we’ve done yet has been anything we couldn’t have done by simply writing the markdown. Things get more interesting when we start using variables. These allows us to create and manipulate information before displaying it. Variables are usually made (declared) with either const or let in JavaScript. So const myName = \"Brian\" creates a variable called myName with the value of “Brian”. We can then write this:\nconst myName = \"Brian\"\ndv.paragraph(\"Hi, my name is \" + myName + \".\")\nwhich produces\nHi, my name is Brian.\nWhen used with text, called strings in coding lingo, the + puts together (concatenates) multiple strings. You can see that a long string with multiple variables could start to look messy. A more convenient way to combine text and variables is to use back ticks instead of quotes, and put the variable names in curly braces preceded with the dollar sign. So instead we can write:\nconst myName = \"Brian\"\ndv.paragraph(`Hi, my name is ${myName}.`)\nto get the same result.\nYou could still object, no pun intended, that we haven’t done anything we couldn’t have done just by writing out the sentence. We’ll get to that in the next article, but before closing this one, I’ll show you something you can only do with JavaScript. DataviewJS comes with another JavaScript flavor baked in called MomentJS. Like DataviewJS provides a dv object, MomentJS provides a moment object which makes it easy to work with dates and times. We’ll talk a lot more about this later, but for now you can use it like this:\nconst today = moment(Date.now()).format(\"MMMM DD, YYYY\")\ndv.paragraph(`Today is ${today}`)\nToday is October 10, 2023\nThis display’s today’s date in a format specified by argument to the format() function. Every time you open a note or canvas, the current day will be displayed.\nPutting it all together, we can write\nconst myName = \"Brian\"\nconst today = moment(Date.now()).format(\"MMMM DD, YYYY\")\nconst learnings = [\n    \"JavaScript basics\",\n    \"The dataview object\",\n    \"Displaying elements with JavaScript\",\n    \"JavaScript variables\",\n]\ndv.header(3, `Things ${myName} learned on ${today}`)\ndv.list(learnings)\nto produce\n\n\n\nJavaScript basics\nThe dataview object\nDisplaying elements with JavaScript\nJavaScript variables",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#next-steps",
    "href": "posts/obsidian/dataview/gentle-introduction-to-dataviewjs/index.html#next-steps",
    "title": "DataviewJS: A Gentle Introduction",
    "section": "",
    "text": "In the next article I’ll start looking at using DataviewJS to make lists and tables and otherwise use the information in your vault. Meanwhile, if you want to learn more about working with dates, you can visit the Moment.js website. And the DataviewJS documentation can be found here. Happy coding!",
    "crumbs": [
      "Obsidian",
      "Dataview",
      "DataviewJS: A Gentle Introduction"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-whiteboards/index.html",
    "href": "posts/obsidian/obsidian-whiteboards/index.html",
    "title": "Obsidian Canvas Work Spaces",
    "section": "",
    "text": "Visual MOCery \n\n\n\nMy Freeing Your Thinking series of articles has had a primary theme: focus on information, not files. My first articles explained how time spent organizing files into folders and naming fleeting notes is essentially wasted time. Worse, maintaining such a system is a daily drain on productivity. Even worse, you will probably decide one day that a different structure is better and, to re-arrange everything, you need to invest yet more time. And what if you want a different structure, but still maintain the current structure at the same time? You can’t do that with folders, but you can with bookmarks. With bookmarks you can build a tree of the information, regardless of files and file locations. Specific information is not limited to one location.\nThe point of this system is to be as productive as possible. Organization is a tool, not a goal, and should only be used if it adds value (increases productivity). I want to avoid the loss of focus and efficiency entailed in opening files and switching between notes. Time spent organizing and working with files is inefficient given the many options available in Obsidian. Time spent organizing and working with information, on the other hand, is what it’s all about.\nIn this article I’ll look at the popular Map of Content and consider it’s virtues with respect to productivity. At least as I have seen it described, I find it an unwieldy tool for working with the information in my vault. It is inefficient, steals focus, and does not promote creativity. I will propose an approach based on Canvas. With Canvas, I can construct a rich work environment for developing my knowledge on a subject, with information displayed and structured visually. From my canvas I have direct access to all of the files implicated, and don’t need to leave the canvas to review or edit them. Tasks, goals, questions, even the narrative, are on the canvas. I will post a link at the end to my repository with this example vault.\n\n\n\nThe map of content is a common way of organizing notes in an Obsidian vault. The concept is flexible, and can be applied to many sorts of notes, the simplest being a note with an index-like list of links to all of the notes relating to a topic of interest. If you have been working with Freeing Your Thinking, you might have noticed the red flag in this concept: the note itself. Creating a document consisting solely or primarily of links to other documents is file-based thinking. In fact, in this system, you get such a list for free, without effort, as you will see below. Actually, there are three ways to find and access all information on the topic of interest, without creating such a list. And I do not mean with a Dataview query in a note. So making such a note is a waste of time.\nAll of my notes have a topic property. This takes one or more links. The topic does not need to exist when assigned to a note. I have notes with a type of moc, but they are empty of content, only containing metadata, a topic and a type. In a NoSQL database, such documents containing only metadata are merely nodes which tie the data together. Their purpose is not to hold information per se. As a result, when I finally create the actual file, I get an index for free (three actually), with the ability to inspect all of the information without opening the files.\nSpecifically, in the right sidebar I have all of the links, as well as a graph. On the left, a simple filter gives me the same information. And I rarely need to open any of these files, since I can view and edit them directly with Hover Editor.\n\n\nFrom a simple index, the next step is often to give structure to the notes, typically in a table of contents format. This process usually involves opening and reading, perhaps editing, the notes and placing them in an appropriate “location” in the MOC note. Again, this is file-based thinking. As explained before, bookmarks provide this type of organization more effectively, flexibly, and efficiently.\n\n\n\nCreating a traditional Map of Content does not really get you much closer to your information. The notes may be well organized, but they remain essentially closed books. And, in the format of a note, assimilation of the larger picture of a topic is difficult, and relationships are hard to visualize. Using Canvas, on the other hand, I can create a work space for each topic I’m interested in. As mentioned, the work space contains all of the reference material side by side with questions, ideas, tasks and a working document. And the canvas contains a white board where all of the information is exposed and organized.\nThis canvas-based approach to content mapping well satisfies my two criteria for evaluating a process. It maximizes my ability to focus on the information without distraction, and it is efficient as I rarely if ever, need to leave the canvas. No opening and closing of files, or remembering file names and locations.\n\n\nAt the top of the work space I have my desk area. Here I put my goals for the topic and guiding questions, as well as any tasks related to the topic. This is also where I have my developing narrative on the topic. This long form document describes the topic and may result in an essay or article…the result of my efforts pursuing the subject and a statement of my understanding.\nThe tasks are all in a file accessible in The Stacks, which I will show in a minute. Questions and goals are actually tasks, too. At the top are queries of the form\nfilename includes Occitan Tasks\nfilter by function task.status.symbol === '!'\nTasks may be edited directly from the queries without opening the containing file.\nRemoving extraneous information allows the eyes to focus with less effort on the meaningful information, so I have used CSS to hide the links that would normally be displayed next to the task description. This brief video shows the features of the desk area.\n\n\n\n\n\nOn the left is my reference area. Here I have various queries allowing me to process files directly from the workspace. Canvas is nice, because I can “roll up” these views and only expand them when I need them. All notes on the topic of Occitan are grouped into fleeting notes, processed notes, and actionable notes (I have an action property.) They are primarily used to discover information to add to the canvas.\nThis, for example, is the “Fleeting Notes” query:\nlist\nwhere contains(topic, [[Occitan]]) & contains(type, \"fleeting\")\nHere is also found the Task Master, which is the note containing the tasks related to the topic. The questions, goals and tasks displayed at the top come from this note. Once a task has been added, any further editing is done directly from the views at the top of the canvas.\nIn addition, I have queries which list all of the content of all of the notes on the Occitan topic which I can scroll through. This is a very fast way to browse all my information. Most interesting, perhaps, is the view of the content of all notes which mention Occitan but are NOT linked to the MOC.\nThe query to produce this is more involved than the others. However, if you have followed along with the DataviewJS articles, most of this should be familiar. We do have a few new functions. dv.io.load() returns the full contents of each file. split(\\n\\n) then divides the content into blocks. (\\n refers to a new line, and since paragraphs are separated by blank lines that gives two \\ns.) Then filter() returns only the paragraphs containing the word Occitan. Taken step by step the code should be clear.\nconst pagesWithTopic = await Promise.all(\n    dv\n    .pages(\"-[[Occitan]]\")\n    .where(t =&gt; t.file.name != \"Occitan Unlinked References\")\n    .where(t =&gt; t.file.name != \"Occitan Tasks\")\n        .map(n =&gt; new Promise(async (resolve, reject) =&gt; {\n        const content = await dv.io.load(n.file.path);\n        resolve({\n            link: n.file.link,\n            content\n        });\n    }))\n);\n\nconst blocksWithTopic = pagesWithTopic.map(({\n    link,\n    content\n}) =&gt; ({\n    link,\n    content: content\n        .split('\\n\\n')\n        .filter(content =&gt; content.includes('Occitan'))\n}));\n\nblocksWithTopic.forEach(\n    page =&gt;\n    page.content.forEach(\n        n =&gt; dv.paragraph(`(${page.link})\\n\\n ${n} `)\n    )\n);\n\n\n\n\n\nAt some point in every crime procedural television show they wheel out the white boards and start pinning up information, making annotations and drawing links. Items of interest are removed from their file and put up in plain site, with the information clearly exposed. There are all of the photographs, news clippings, maps and highlighted sections of documents, supplemented with commentary and lines connecting the information. The important thing here is that relevant information is clearly displayed, and no effort is needed to view it. Information is grouped visually with connections between specific information indicated by the lines.\nThe investigators make these boards because they provide a useful way to explore a subject, ask and answer questions, identify gaps in knowledge, and ultimately construct a narrative around the information. Obsidian’s Canvas allows me to do exactly the same thing, exposing my information and allowing me to arrange and re-arrange it as I work through it in my mind. As I add to the canvas, I can easily add the views to my information tree as well, giving logical structure to the visual structure.\nHere is an overview of my workspace for the topic of the Occitan language, one I am only starting to explore.\n\n\n\n\n\n\nObsidian is a powerful tool, but the functionality it offers is often overlooked and/or underused. Rather than treating it as essentially a file editor and approaching it the way one would a word processor, think of it as both an information repository and a collection of tools to access and develop the information. Views on a canvas are much more intellectually stimulating than links, or even embeds, in a file.\nThe contents used for this article are available for download from my GitHub. If you find this content useful, please consider making a donation to support future work.",
    "crumbs": [
      "Obsidian",
      "Obsidian Canvas Work Spaces"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-whiteboards/index.html#organization-and-productivity",
    "href": "posts/obsidian/obsidian-whiteboards/index.html#organization-and-productivity",
    "title": "Obsidian Canvas Work Spaces",
    "section": "",
    "text": "My Freeing Your Thinking series of articles has had a primary theme: focus on information, not files. My first articles explained how time spent organizing files into folders and naming fleeting notes is essentially wasted time. Worse, maintaining such a system is a daily drain on productivity. Even worse, you will probably decide one day that a different structure is better and, to re-arrange everything, you need to invest yet more time. And what if you want a different structure, but still maintain the current structure at the same time? You can’t do that with folders, but you can with bookmarks. With bookmarks you can build a tree of the information, regardless of files and file locations. Specific information is not limited to one location.\nThe point of this system is to be as productive as possible. Organization is a tool, not a goal, and should only be used if it adds value (increases productivity). I want to avoid the loss of focus and efficiency entailed in opening files and switching between notes. Time spent organizing and working with files is inefficient given the many options available in Obsidian. Time spent organizing and working with information, on the other hand, is what it’s all about.\nIn this article I’ll look at the popular Map of Content and consider it’s virtues with respect to productivity. At least as I have seen it described, I find it an unwieldy tool for working with the information in my vault. It is inefficient, steals focus, and does not promote creativity. I will propose an approach based on Canvas. With Canvas, I can construct a rich work environment for developing my knowledge on a subject, with information displayed and structured visually. From my canvas I have direct access to all of the files implicated, and don’t need to leave the canvas to review or edit them. Tasks, goals, questions, even the narrative, are on the canvas. I will post a link at the end to my repository with this example vault.",
    "crumbs": [
      "Obsidian",
      "Obsidian Canvas Work Spaces"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-whiteboards/index.html#maps-of-content",
    "href": "posts/obsidian/obsidian-whiteboards/index.html#maps-of-content",
    "title": "Obsidian Canvas Work Spaces",
    "section": "",
    "text": "The map of content is a common way of organizing notes in an Obsidian vault. The concept is flexible, and can be applied to many sorts of notes, the simplest being a note with an index-like list of links to all of the notes relating to a topic of interest. If you have been working with Freeing Your Thinking, you might have noticed the red flag in this concept: the note itself. Creating a document consisting solely or primarily of links to other documents is file-based thinking. In fact, in this system, you get such a list for free, without effort, as you will see below. Actually, there are three ways to find and access all information on the topic of interest, without creating such a list. And I do not mean with a Dataview query in a note. So making such a note is a waste of time.\nAll of my notes have a topic property. This takes one or more links. The topic does not need to exist when assigned to a note. I have notes with a type of moc, but they are empty of content, only containing metadata, a topic and a type. In a NoSQL database, such documents containing only metadata are merely nodes which tie the data together. Their purpose is not to hold information per se. As a result, when I finally create the actual file, I get an index for free (three actually), with the ability to inspect all of the information without opening the files.\nSpecifically, in the right sidebar I have all of the links, as well as a graph. On the left, a simple filter gives me the same information. And I rarely need to open any of these files, since I can view and edit them directly with Hover Editor.\n\n\nFrom a simple index, the next step is often to give structure to the notes, typically in a table of contents format. This process usually involves opening and reading, perhaps editing, the notes and placing them in an appropriate “location” in the MOC note. Again, this is file-based thinking. As explained before, bookmarks provide this type of organization more effectively, flexibly, and efficiently.",
    "crumbs": [
      "Obsidian",
      "Obsidian Canvas Work Spaces"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-whiteboards/index.html#work-spaces",
    "href": "posts/obsidian/obsidian-whiteboards/index.html#work-spaces",
    "title": "Obsidian Canvas Work Spaces",
    "section": "",
    "text": "Creating a traditional Map of Content does not really get you much closer to your information. The notes may be well organized, but they remain essentially closed books. And, in the format of a note, assimilation of the larger picture of a topic is difficult, and relationships are hard to visualize. Using Canvas, on the other hand, I can create a work space for each topic I’m interested in. As mentioned, the work space contains all of the reference material side by side with questions, ideas, tasks and a working document. And the canvas contains a white board where all of the information is exposed and organized.\nThis canvas-based approach to content mapping well satisfies my two criteria for evaluating a process. It maximizes my ability to focus on the information without distraction, and it is efficient as I rarely if ever, need to leave the canvas. No opening and closing of files, or remembering file names and locations.\n\n\nAt the top of the work space I have my desk area. Here I put my goals for the topic and guiding questions, as well as any tasks related to the topic. This is also where I have my developing narrative on the topic. This long form document describes the topic and may result in an essay or article…the result of my efforts pursuing the subject and a statement of my understanding.\nThe tasks are all in a file accessible in The Stacks, which I will show in a minute. Questions and goals are actually tasks, too. At the top are queries of the form\nfilename includes Occitan Tasks\nfilter by function task.status.symbol === '!'\nTasks may be edited directly from the queries without opening the containing file.\nRemoving extraneous information allows the eyes to focus with less effort on the meaningful information, so I have used CSS to hide the links that would normally be displayed next to the task description. This brief video shows the features of the desk area.\n\n\n\n\n\nOn the left is my reference area. Here I have various queries allowing me to process files directly from the workspace. Canvas is nice, because I can “roll up” these views and only expand them when I need them. All notes on the topic of Occitan are grouped into fleeting notes, processed notes, and actionable notes (I have an action property.) They are primarily used to discover information to add to the canvas.\nThis, for example, is the “Fleeting Notes” query:\nlist\nwhere contains(topic, [[Occitan]]) & contains(type, \"fleeting\")\nHere is also found the Task Master, which is the note containing the tasks related to the topic. The questions, goals and tasks displayed at the top come from this note. Once a task has been added, any further editing is done directly from the views at the top of the canvas.\nIn addition, I have queries which list all of the content of all of the notes on the Occitan topic which I can scroll through. This is a very fast way to browse all my information. Most interesting, perhaps, is the view of the content of all notes which mention Occitan but are NOT linked to the MOC.\nThe query to produce this is more involved than the others. However, if you have followed along with the DataviewJS articles, most of this should be familiar. We do have a few new functions. dv.io.load() returns the full contents of each file. split(\\n\\n) then divides the content into blocks. (\\n refers to a new line, and since paragraphs are separated by blank lines that gives two \\ns.) Then filter() returns only the paragraphs containing the word Occitan. Taken step by step the code should be clear.\nconst pagesWithTopic = await Promise.all(\n    dv\n    .pages(\"-[[Occitan]]\")\n    .where(t =&gt; t.file.name != \"Occitan Unlinked References\")\n    .where(t =&gt; t.file.name != \"Occitan Tasks\")\n        .map(n =&gt; new Promise(async (resolve, reject) =&gt; {\n        const content = await dv.io.load(n.file.path);\n        resolve({\n            link: n.file.link,\n            content\n        });\n    }))\n);\n\nconst blocksWithTopic = pagesWithTopic.map(({\n    link,\n    content\n}) =&gt; ({\n    link,\n    content: content\n        .split('\\n\\n')\n        .filter(content =&gt; content.includes('Occitan'))\n}));\n\nblocksWithTopic.forEach(\n    page =&gt;\n    page.content.forEach(\n        n =&gt; dv.paragraph(`(${page.link})\\n\\n ${n} `)\n    )\n);\n\n\n\n\n\nAt some point in every crime procedural television show they wheel out the white boards and start pinning up information, making annotations and drawing links. Items of interest are removed from their file and put up in plain site, with the information clearly exposed. There are all of the photographs, news clippings, maps and highlighted sections of documents, supplemented with commentary and lines connecting the information. The important thing here is that relevant information is clearly displayed, and no effort is needed to view it. Information is grouped visually with connections between specific information indicated by the lines.\nThe investigators make these boards because they provide a useful way to explore a subject, ask and answer questions, identify gaps in knowledge, and ultimately construct a narrative around the information. Obsidian’s Canvas allows me to do exactly the same thing, exposing my information and allowing me to arrange and re-arrange it as I work through it in my mind. As I add to the canvas, I can easily add the views to my information tree as well, giving logical structure to the visual structure.\nHere is an overview of my workspace for the topic of the Occitan language, one I am only starting to explore.",
    "crumbs": [
      "Obsidian",
      "Obsidian Canvas Work Spaces"
    ]
  },
  {
    "objectID": "posts/obsidian/obsidian-whiteboards/index.html#final-thoughts",
    "href": "posts/obsidian/obsidian-whiteboards/index.html#final-thoughts",
    "title": "Obsidian Canvas Work Spaces",
    "section": "",
    "text": "Obsidian is a powerful tool, but the functionality it offers is often overlooked and/or underused. Rather than treating it as essentially a file editor and approaching it the way one would a word processor, think of it as both an information repository and a collection of tools to access and develop the information. Views on a canvas are much more intellectually stimulating than links, or even embeds, in a file.\nThe contents used for this article are available for download from my GitHub. If you find this content useful, please consider making a donation to support future work.",
    "crumbs": [
      "Obsidian",
      "Obsidian Canvas Work Spaces"
    ]
  },
  {
    "objectID": "posts/obsidian/metadata-menu/index.html",
    "href": "posts/obsidian/metadata-menu/index.html",
    "title": "Obsidian Metadata Menu Plugin",
    "section": "",
    "text": "Metadata Menu\nWith the release of Obsidian 1.4.5, many people’s focus has been on metadata, which Obsidian calls Properties. I have written a series of articles (see Freeing Your Thinking) on using Obsidian’s natural database features, and my entire workflow depends heavily on metadata. Naturally, I was anticipating this release with some excitement, having seen some previews on YouTube.\nI was frankly disappointed. It is movement in a positive direction, and has some very nice features for managing metadata. However, its insistence on YAML for properties when many users such as myself prefer in-line metadata for its flexibility, makes some of its features useless to me.\nAnother reason for my disappointment, or feeling underwhelmed, was that I had already discovered a truly amazing plugin for managing metadata which did everything I need and more. Metadata Menu, a Community Plugin by mathieu, provides a full set of features for managing properties. It has a modal, available in multiple places, which allows you to manage literally all aspects of a file’s metadata without opening the file, perfect for my workflow. It even has a file class template system which supports nesting groups of metadata fields. Fields in tables can have actions, allowing for direct editing of metadata in the table itself using standard widgets. And, it is visually clean and attractive\nThis video isn’t a guide to using Metadata Menu. Rather, it’s to show off some of its features. If metadata is important to you, you must check it out.\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Obsidian",
      "Obsidian Metadata Menu Plugin"
    ]
  },
  {
    "objectID": "posts/obsidian/graph-view-deets/index.html",
    "href": "posts/obsidian/graph-view-deets/index.html",
    "title": "Obsidian: The mechanics of Graph View",
    "section": "",
    "text": "Core functionality in Obsidian seems to be somewhat overlooked. I have written in other articles about using Search, Bookmarks, and Unique Notes for Freeing Your Thinking. In these videos, I demonstrate the functionality of another core component, Graph View.\nA graph is a specific data structure consisting of nodes and edges. Obsidian consists of notes which are linked together. This type of data can be stored in a non-relational database. With links and metadata, Obsidian is a non-relational database.\nFor these reasons, graph view is a powerful way to visualize your data. Graph view is a force directed graph drawing. Where Obsidian search is the best way to find information, graph view is the best way to visualize information and discovering relationships which have not been defined, such as orphans.\nThese videos go through the details of the graph view functionality in the hope that you will begin to use it to visualize data.",
    "crumbs": [
      "Obsidian",
      "Obsidian: The mechanics of Graph View"
    ]
  },
  {
    "objectID": "posts/obsidian/graph-view-deets/index.html#part-1",
    "href": "posts/obsidian/graph-view-deets/index.html#part-1",
    "title": "Obsidian: The mechanics of Graph View",
    "section": "Part 1",
    "text": "Part 1",
    "crumbs": [
      "Obsidian",
      "Obsidian: The mechanics of Graph View"
    ]
  },
  {
    "objectID": "posts/obsidian/graph-view-deets/index.html#part-2",
    "href": "posts/obsidian/graph-view-deets/index.html#part-2",
    "title": "Obsidian: The mechanics of Graph View",
    "section": "Part 2",
    "text": "Part 2",
    "crumbs": [
      "Obsidian",
      "Obsidian: The mechanics of Graph View"
    ]
  },
  {
    "objectID": "posts/obsidian/git-github-obsidian/index.html",
    "href": "posts/obsidian/git-github-obsidian/index.html",
    "title": "Git and GitHub for Obsidian Users",
    "section": "",
    "text": "I’ve been doing some repair work around my house recently and have been reflecting on how important the choice of tools are in making a project go quickly and smoothly. More specifically, I’ve been thinking about how important it is to choose a tool designed for a specific purpose rather than a general purpose one. For example, while you can screw a nut on a bolt with vise grips, you’re much better off using ratchet wrench which is designed for that specific purpose. It is faster, easier, and less likely to do damage to the nut.\nI’ve been seeing quite a few articles about git and GitHub in the context of backups and synchronization. In this article I would like to explain exactly what they are and why they are not good tools to use for backing up or synchronizing an Obsidian vault. I will then propose best-of-class solutions for these needs. At the end, I will show how git and GitHub can, in fact, be useful for some special purposes. ## Git\nGit is a version control system designed for collaborative software development. Using git, developers are able to manage a project’s code base throughout development cycles, allowing multiple authors to contribute code to a single project, everyone keeping up to date with the latest version. It allows for maintaining separate branches for production and development, branches for features, and branches for each developer. The various branches can be merged as they are completed, and thereby update the production or main development branch.\nIf you are wondering how any of this relates to Obsidian, which is not a software development project and does not typically have multiple contributors, well, it doesn’t, which is kind of my point. It’s a vice grips solution for Obsidian.\nIts design is clever, though, and worth taking a moment to understand. The information generated by git is stored in a repository. A repository is simply a hidden directory created in the main directory of your project, or in this case, your vault. The repository itself keeps track of commits. A commit is a file which describes exactly what files have been added, removed, or edited since the last commit. In the case of edits, it keeps track of the specific changes made to each file by tracking changes to each line. The changes are combined with information such as the author of the change and a description of what was changed, and this becomes a new commit. The granularity allows for identifying specific lines which introduced bugs. They can then “re-set” the project to the prior, bug-free state while someone fixes the bug, or to a point where a deleted file still existed.\nBefore moving on, I will just point out that the git repository has nothing to do with GitHub. It’s just a hidden directory on your local file system.\n\n\nAt it’s core, GitHub is service like Dropbox which provides cloud storage. But there are important differences. It was specifically created to promote sharing of code and collaboration among software developers. As such, you can have as many free public repositories as you want, but need to pay for private ones. A limitation of GitHub is file size. There is a maximum size of 100 MB per file. This is not a problem for most people, but should you have any videos in your vault, for example, you will not be able to use GitHub at all. In any case, unless you need tools provided by GitHub, you might just as well copy your directory, or your local git repository, to Dropbox instead of pushing to GitHub.\nIn addition to storage, GitHub provides a whole suite of tools which developers can use to design automated workflows and even deploy projects directly from GitHub. None of this really applies to Obsidian either, with potential exceptions which I’ll describe below.\n\n\n\nAs we have seen, GitHub is designed for keeping multiple developers and project branches in sync, so it might seem to appropriate for keeping Obsidian vaults in sync. But, since there is only one author and only one branch, it is overkill at the very least. A developer pulls to see what others have done and pushes to share their work…neither relevant for Obsidian.\nBut my main objection is practical. Using GitHub requires manual interactions be performed every time you switch devices, namely a pull and a push to GitHub. Ideally, synchronization across devices should be automatic, instantaneous and real time. I wrote an article on synchronizing your vault across different devices using a tool called Syncthing. It is a free, fit for purpose tool which uses direct, device-to-device synchronization, requiring no intervention, and changes are immediately reflected across all devices. ## Backups Be honest: do you back up your computer regularly? If you are like the majority of people the answer is no. Given the number of articles I’ve seen on solutions for backing up vaults I can only conclude that many Obsidian users, like others, don’t have a regular backup system, because, if you are backing up your computer, you are backing up your vault. Obsidian is just another directory, and doesn’t need anything special. If you already perform regular backups, and have tested file recovery, then you don’t need to read the next bit…unless you are using GitHub for your backups.\nTo understand why GitHub should not be used to back up your vault, consider the main features required for a good backup system:\n\nEase of recovery - even people who actually do backups often do not test how quickly and easily it is to restore files from the backups. Ideally, you should be able to navigate through backed up files and directories just like ordinary ones, and then simply copy what you want. Recovering lost or old versions of files is possible using git, but the process is much more cumbersome.\nRotation of daily, weekly, monthly backups - a good backup system will automatically rotate your backups, removing unnecessary versions as they age and new ones are made. That way you can find something from two days, two weeks, two months or two years ago. Git provide no such functionality.\nSpeed and Space efficiency - over time, the amount of data you need to back up can be many gigabytes, especially if you have videos or many images. When you do a backup, the tool must check for changes across the entire directory, so it needs to be fast. At any point, you will have dozens of backups each representing a snapshot in time. In order to maintain so many “copies”, the system must be efficient in compressing the information. With git and GitHub there is no compression except for during file transfer.\nOff-site backups - Best practice for backups means storing at least one copy of your backups in a different physical location. This could be a cloud server. A good tool should make off-site backups just as easy as on-site backups. GitHub does fulfill this requirement.\n\nThe best backup system around is called Restic. It is free, open source, cross-platform, and can be easily managed with a few simple commands. Most importantly, it is blazing fast, and creates surprisingly small repositories. This is basically because it breaks up your files into variable-length blobs, or chunks of bytes. I will explain the details in a subsequent article. ## Sharing - use cases for git and GitHub\nI have found some very good use cases for git and GitHub. They involve sharing content of my vault. Before describing them, let me point out that sharing of vault content should generally only be one way. Obsidian is not meant to be a collaborative tool. So you, and only you, control what goes into your vault.\n\n\nGit, and especially GitHub itself, are convenient for creating live, interactive presentations from content in my vault. I can create a formal presentation using the Advanced Slides plugin, or just make a section of my vault with specific content and put that part on GitHub. GitHub allows others to browse the vault or view the presentation on-line. Alternatively, they can download the content, open it locally with Obsidian, or simply copy it into their own vault.\nIn the context of a presentation, git itself is useful, because a presentation is a product. Like any product, version control is useful. A presentation can change and evolve. Sometimes one wants to see something from a prior iteration of a project, and git makes this simple. In other words, presentations have versions.\nI’ll give some tips on the mechanics of how to do this at the end, but it’s simply a matter of collecting all the necessary files, including attachments, in a sub-directory of my vault. I copy this directory to a different location, open it as a vault and enable necessary plugins. GitHub can directly serve HTML files, so I convert the entire new vault to HTML with the Webpage HTML Export plugin. I create a README.md with some sort of linked table of contents or at least a link to enter the html files. With that done, I can just push to GitHub, and everything goes live in minutes on a url GitHub creates for me.\nAt this point I can make the presentation and the the audience can follow along on their own computers and engage with the content, either on-line or locally by downloading the vault from GitHub. If they happen to be Obsidian users, they could also copy the files into their own vault to further interact with the material.\n\n\n\nAs a teacher, it didn’t take long to consider how Obsidian might be used in the classroom. The idea of making and distributing course content which is easily navigable, visually interesting, and incorporates multi-media, graphs and charts, and external resources is very attractive. Sharing of content can be done the same way as the presentation above, and students could get it either “live” or by making their own copy.\nBut, as I do go on about, Obsidian is a database too, so why not push this a step further, and run the whole course with Obsidian, including design, distribution of materials, receiving assignments from students, applying rubrics if appropriate, grading and evaluations. This sounds like a project, and git and GitHub are perfect for this use. In addition to facilitating the distribution of the course materials, you can update the materials from time to time, and students will always have access to the latest version. I segregate the course into a public/ and private/ directory, and only push public/ to GitHub. All completed work, grades, evaluations, and any identifying material is kept confidential. Students can submit their responses by emailing the single note, which I simply place in the private/ directory. The Properties take care of everything else (except the actual grading).\nThis is obviously a more complex example, involving metadata (properties), Dataview, and various templates to provide the metadata and compile grades. A full description would take too long for this article, but I intend to write a detailed article with a sample vault in the near future.\n\n\n\n\nThis article is already long, so I can’t go into details about using git and GitHub, but I want to show how simple it is for this purpose. You need to install git itself, and I use another program called gh (the GitHub Client), which allows me to manage everything from the command line. With these installed and a free GitHub account, all I need to do is create the repository locally with the command git init. I then create the repository on GitHub itself with\ngh repo create my-vault-name --public --source=. --remote=upstream`.\nAfter that, whenever I add, delete or change content, I just do\ngit add .\ngit commit -m \"Some message\"\ngit push\nOn the GitHub website, under the Settings menu there is a Pages option. Simply go there and you can deploy your vault (with HTML rendered) with a couple of clicks. It will provide you with a live URL, where the content will be kept up to date every time you push.\nThat’s all there is to it.",
    "crumbs": [
      "Obsidian",
      "Git and GitHub for Obsidian Users"
    ]
  },
  {
    "objectID": "posts/obsidian/git-github-obsidian/index.html#github",
    "href": "posts/obsidian/git-github-obsidian/index.html#github",
    "title": "Git and GitHub for Obsidian Users",
    "section": "",
    "text": "At it’s core, GitHub is service like Dropbox which provides cloud storage. But there are important differences. It was specifically created to promote sharing of code and collaboration among software developers. As such, you can have as many free public repositories as you want, but need to pay for private ones. A limitation of GitHub is file size. There is a maximum size of 100 MB per file. This is not a problem for most people, but should you have any videos in your vault, for example, you will not be able to use GitHub at all. In any case, unless you need tools provided by GitHub, you might just as well copy your directory, or your local git repository, to Dropbox instead of pushing to GitHub.\nIn addition to storage, GitHub provides a whole suite of tools which developers can use to design automated workflows and even deploy projects directly from GitHub. None of this really applies to Obsidian either, with potential exceptions which I’ll describe below.",
    "crumbs": [
      "Obsidian",
      "Git and GitHub for Obsidian Users"
    ]
  },
  {
    "objectID": "posts/obsidian/git-github-obsidian/index.html#synchronization",
    "href": "posts/obsidian/git-github-obsidian/index.html#synchronization",
    "title": "Git and GitHub for Obsidian Users",
    "section": "",
    "text": "As we have seen, GitHub is designed for keeping multiple developers and project branches in sync, so it might seem to appropriate for keeping Obsidian vaults in sync. But, since there is only one author and only one branch, it is overkill at the very least. A developer pulls to see what others have done and pushes to share their work…neither relevant for Obsidian.\nBut my main objection is practical. Using GitHub requires manual interactions be performed every time you switch devices, namely a pull and a push to GitHub. Ideally, synchronization across devices should be automatic, instantaneous and real time. I wrote an article on synchronizing your vault across different devices using a tool called Syncthing. It is a free, fit for purpose tool which uses direct, device-to-device synchronization, requiring no intervention, and changes are immediately reflected across all devices. ## Backups Be honest: do you back up your computer regularly? If you are like the majority of people the answer is no. Given the number of articles I’ve seen on solutions for backing up vaults I can only conclude that many Obsidian users, like others, don’t have a regular backup system, because, if you are backing up your computer, you are backing up your vault. Obsidian is just another directory, and doesn’t need anything special. If you already perform regular backups, and have tested file recovery, then you don’t need to read the next bit…unless you are using GitHub for your backups.\nTo understand why GitHub should not be used to back up your vault, consider the main features required for a good backup system:\n\nEase of recovery - even people who actually do backups often do not test how quickly and easily it is to restore files from the backups. Ideally, you should be able to navigate through backed up files and directories just like ordinary ones, and then simply copy what you want. Recovering lost or old versions of files is possible using git, but the process is much more cumbersome.\nRotation of daily, weekly, monthly backups - a good backup system will automatically rotate your backups, removing unnecessary versions as they age and new ones are made. That way you can find something from two days, two weeks, two months or two years ago. Git provide no such functionality.\nSpeed and Space efficiency - over time, the amount of data you need to back up can be many gigabytes, especially if you have videos or many images. When you do a backup, the tool must check for changes across the entire directory, so it needs to be fast. At any point, you will have dozens of backups each representing a snapshot in time. In order to maintain so many “copies”, the system must be efficient in compressing the information. With git and GitHub there is no compression except for during file transfer.\nOff-site backups - Best practice for backups means storing at least one copy of your backups in a different physical location. This could be a cloud server. A good tool should make off-site backups just as easy as on-site backups. GitHub does fulfill this requirement.\n\nThe best backup system around is called Restic. It is free, open source, cross-platform, and can be easily managed with a few simple commands. Most importantly, it is blazing fast, and creates surprisingly small repositories. This is basically because it breaks up your files into variable-length blobs, or chunks of bytes. I will explain the details in a subsequent article. ## Sharing - use cases for git and GitHub\nI have found some very good use cases for git and GitHub. They involve sharing content of my vault. Before describing them, let me point out that sharing of vault content should generally only be one way. Obsidian is not meant to be a collaborative tool. So you, and only you, control what goes into your vault.\n\n\nGit, and especially GitHub itself, are convenient for creating live, interactive presentations from content in my vault. I can create a formal presentation using the Advanced Slides plugin, or just make a section of my vault with specific content and put that part on GitHub. GitHub allows others to browse the vault or view the presentation on-line. Alternatively, they can download the content, open it locally with Obsidian, or simply copy it into their own vault.\nIn the context of a presentation, git itself is useful, because a presentation is a product. Like any product, version control is useful. A presentation can change and evolve. Sometimes one wants to see something from a prior iteration of a project, and git makes this simple. In other words, presentations have versions.\nI’ll give some tips on the mechanics of how to do this at the end, but it’s simply a matter of collecting all the necessary files, including attachments, in a sub-directory of my vault. I copy this directory to a different location, open it as a vault and enable necessary plugins. GitHub can directly serve HTML files, so I convert the entire new vault to HTML with the Webpage HTML Export plugin. I create a README.md with some sort of linked table of contents or at least a link to enter the html files. With that done, I can just push to GitHub, and everything goes live in minutes on a url GitHub creates for me.\nAt this point I can make the presentation and the the audience can follow along on their own computers and engage with the content, either on-line or locally by downloading the vault from GitHub. If they happen to be Obsidian users, they could also copy the files into their own vault to further interact with the material.\n\n\n\nAs a teacher, it didn’t take long to consider how Obsidian might be used in the classroom. The idea of making and distributing course content which is easily navigable, visually interesting, and incorporates multi-media, graphs and charts, and external resources is very attractive. Sharing of content can be done the same way as the presentation above, and students could get it either “live” or by making their own copy.\nBut, as I do go on about, Obsidian is a database too, so why not push this a step further, and run the whole course with Obsidian, including design, distribution of materials, receiving assignments from students, applying rubrics if appropriate, grading and evaluations. This sounds like a project, and git and GitHub are perfect for this use. In addition to facilitating the distribution of the course materials, you can update the materials from time to time, and students will always have access to the latest version. I segregate the course into a public/ and private/ directory, and only push public/ to GitHub. All completed work, grades, evaluations, and any identifying material is kept confidential. Students can submit their responses by emailing the single note, which I simply place in the private/ directory. The Properties take care of everything else (except the actual grading).\nThis is obviously a more complex example, involving metadata (properties), Dataview, and various templates to provide the metadata and compile grades. A full description would take too long for this article, but I intend to write a detailed article with a sample vault in the near future.",
    "crumbs": [
      "Obsidian",
      "Git and GitHub for Obsidian Users"
    ]
  },
  {
    "objectID": "posts/obsidian/git-github-obsidian/index.html#usage-of-git-and-github",
    "href": "posts/obsidian/git-github-obsidian/index.html#usage-of-git-and-github",
    "title": "Git and GitHub for Obsidian Users",
    "section": "",
    "text": "This article is already long, so I can’t go into details about using git and GitHub, but I want to show how simple it is for this purpose. You need to install git itself, and I use another program called gh (the GitHub Client), which allows me to manage everything from the command line. With these installed and a free GitHub account, all I need to do is create the repository locally with the command git init. I then create the repository on GitHub itself with\ngh repo create my-vault-name --public --source=. --remote=upstream`.\nAfter that, whenever I add, delete or change content, I just do\ngit add .\ngit commit -m \"Some message\"\ngit push\nOn the GitHub website, under the Settings menu there is a Pages option. Simply go there and you can deploy your vault (with HTML rendered) with a couple of clicks. It will provide you with a live URL, where the content will be kept up to date every time you push.\nThat’s all there is to it.",
    "crumbs": [
      "Obsidian",
      "Git and GitHub for Obsidian Users"
    ]
  },
  {
    "objectID": "posts/obsidian/pretty-canvas/index.html",
    "href": "posts/obsidian/pretty-canvas/index.html",
    "title": "Obsidian: Pretty Canvas",
    "section": "",
    "text": "Going visual has really helped free my thinking, and pretty much all of my time in Obsidian is now spent on canvases. Other than when I am making fleeting notes on an article, book or video, I do pretty much everything from a canvas. I process and edit my notes, list questions which guide my research, maintain tasks associated with the effort, and perhaps develop an article on the topic, all from the canvas.\nNaturally I want my canvases to be pretty. It’s not just a question of aesthetics either. Changing simple things like the color or size of elements makes the canvas much more use-able, drawing your eyes quickly to more important information and removing extraneous stuff. Obsidian fortunately allows us to use CSS to do this.\nHere’s an example of a Visual Dashboard I’m just getting started with for a new article. The first image is with default styling. It is usable, and has some pretty colors, but it’s visually confusing.\n\nThere are too many colors for one. The title of the canvas doesn’t stand out, and the group headings don’t either. There is also a bunch of stuff that I don’t need to see, like the links after every task.\nHere is the same with CSS applied. Notice how some useless clutter has been removed, task descriptions have a standardized color which is easy to read, group headings stand out better to guide your eyes, and titles look good.",
    "crumbs": [
      "Obsidian",
      "Obsidian: Pretty Canvas"
    ]
  },
  {
    "objectID": "posts/obsidian/pretty-canvas/index.html#getting-started",
    "href": "posts/obsidian/pretty-canvas/index.html#getting-started",
    "title": "Obsidian: Pretty Canvas",
    "section": "Getting Started",
    "text": "Getting Started\nUsing CSS is pretty straight-forward in principle, but figuring out how to apply it can take a bit of work, as there seems to be no good documentation for automatically generated classes. Also, CSS is simply unfamiliar to many people. Never the less, a little can go a long way. So I wanted to share some of the styling I’m using for my canvases as a jumping-off point for you to personalize your canvases and be more efficient in working with them.\nTo get started, you need to create a snippets directory in the hidden obsidian directory of your vault. Any files placed here with the extension css will be available to Obsidian. You can name the files anything you want, so choose a descriptive name. With obvious irony, these files can only be edited with an external editor, but any editor will do, like vim, nano or Notepad. I suggest you create a file called canvas-fyt.css and copy the contents below into the file. In Obsidian, you will be able to “activate” whichever files you want to use, so you can have multiple styles of canvases.\nOnce you have created a css file in the correct directory you can go to the Appearance section and toggle on the new file to make it active. You may need to press the reload button to re-scan the directory.",
    "crumbs": [
      "Obsidian",
      "Obsidian: Pretty Canvas"
    ]
  },
  {
    "objectID": "posts/obsidian/pretty-canvas/index.html#background",
    "href": "posts/obsidian/pretty-canvas/index.html#background",
    "title": "Obsidian: Pretty Canvas",
    "section": "Background",
    "text": "Background\nWith that done, let’s start with the background. Add the following to the file to change the color and remove the dots:\nsvg.canvas-background {\n    background: linear-gradient(80deg, #253B0B, #2c0149, #390B1C);\n}\n\nsvg.canvas-background circle {\n    display: none;\n}\nThe background is actually an image of the type svg, which stands for Scalable Vector Graphic (as compared to png or jpg). The svg is automatically assigned the class canvas-background. By writing svg.canvas-background we select only the background image and not any other svg files which might be on the canvas. As you can see, styles use key/value pairs followed by a semi-colon. I’ve used a gradient here, but if you only want a single color, you could simply write background-color: tan; for example. There are over 150 named colors which you can use. If you have an RGB value from a color picker, you can write background-color: rgb(10,160,245);.\ndisplay: none; says not to display a particular element, in this case the grid dots (circles). We’ll use this again later.",
    "crumbs": [
      "Obsidian",
      "Obsidian: Pretty Canvas"
    ]
  },
  {
    "objectID": "posts/obsidian/pretty-canvas/index.html#tasks",
    "href": "posts/obsidian/pretty-canvas/index.html#tasks",
    "title": "Obsidian: Pretty Canvas",
    "section": "Tasks",
    "text": "Tasks\nNext we can style the tasks. Adding\n.task-description {\n    color: cyan;\n}\n\n.tasks-backlink {\n    display: none;\n}\nwill change all of the task descriptions to a consistent and easily readable color. It also removes the backlink text which points to the file containing the task. I’m not interested in files, so this is useless to me but takes up a lot of visual space.",
    "crumbs": [
      "Obsidian",
      "Obsidian: Pretty Canvas"
    ]
  },
  {
    "objectID": "posts/obsidian/pretty-canvas/index.html#titlesheaders",
    "href": "posts/obsidian/pretty-canvas/index.html#titlesheaders",
    "title": "Obsidian: Pretty Canvas",
    "section": "Titles/Headers",
    "text": "Titles/Headers\nWith that done we can turn to the textual elements. For these, you must specify on the canvas which elements you want to apply the styles to. For this you need to use HTML instead of markdown, allowing you to assign a class to an element. Let’s style the headings which I will use for titles so they stand out.\n.markdown-rendered .title-tag-1 {\n    text-align: center;\n    font-size: 4em;\n}\n.markdown-rendered .title-tag-2 {\n    text-align: center;\n    font-size: 3em;\n}\n.markdown-rendered .title-tag-3 {\n    text-align: center;\n    font-size: 2em;\n}\nOn the canvas, in order to use the styles, you can’t use the standard hash tags (#, ##, ###) for headers. Instead you need to use HTML and specify the class. For example, instead of writing # Header you write &lt;h1 class=\"title-tag-1\"&gt;Header&lt;/h1&gt;. Then you will see the styles applied to the class.\nNotice the units used for the font size. You can use different units for font size such as pixels (font-size: 24px;). em is convenient though, because it is a relative size. 2 em is like saying “display this twice as large as it would normally be displayed”. The actual size will change based on the zoom level.",
    "crumbs": [
      "Obsidian",
      "Obsidian: Pretty Canvas"
    ]
  },
  {
    "objectID": "posts/obsidian/pretty-canvas/index.html#other-textual-elements",
    "href": "posts/obsidian/pretty-canvas/index.html#other-textual-elements",
    "title": "Obsidian: Pretty Canvas",
    "section": "Other textual elements",
    "text": "Other textual elements\nI also like to be able to have certain words jump out on the page, more than can be achieved with bold or italics. So I have a class\n.markdown-rendered .my-emphasis {\n    color: #b7db6f;\n    font-size: 1.2em;\n}\nI can use this class by wrapping text in a &lt;span&gt; element like this:\nIn this paragraph &lt;span class=\"my-emphasis\"&gt;these words&lt;/span&gt; will stand out.\nIn this paragraph these words will stand out.\nTo change the font, add\n.markdown-rendered {\n    font-family: \"Quicksand\", sans-serif;\n}\n.canvas-group-label {\n    font-size: 2em;\n    font-family: \"Quicksand\";\n    font-weight: bold;\n    color: black;\n}\nI like the Quicksand font. It is a free font from Google, which provides a huge variety of fonts. In order to display a font you must download it and install it to your system. If it is not on your system, the default sans-serif font will be used instead. As you can see, I’ve also changed the size, color and weight of the labels for the groups on the canvas.\nFinally, I would like to have the filenames of links on the canvas to be a different color, so\n.markdown-rendered .internal-link {\n    color: red;\n}\nAnd that’s it. For convenience I provide the complete file below. I hope adding some style will enrich your experience working with your Visual Dashboards.",
    "crumbs": [
      "Obsidian",
      "Obsidian: Pretty Canvas"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-workflow/index.html",
    "href": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-workflow/index.html",
    "title": "Obsidian: Freeing Your Thinking Workflow",
    "section": "",
    "text": "I recently published several articles on using Obsidian’s native database functionality to increase focus and efficiency when working with the information in our vaults. Rather than using Obsidian as a souped-up word processor, we can abandon the file paradigm altogether and use Bookmark and Canvas for accessing, editing, analyzing and organizing our information.\nGiven the interest that the articles have generated, and the fact that the concepts may be unfamiliar to some, I thought it would be helpful to make a video demonstrating how it works in practice. It ended up being somewhat long, so this is the second part.\nIf you haven’t read the articles, this may be confusing as I don’t go into depth on the concepts. This isn’t really a how-to video, so it assumes you know your way around Obsidian.\n\n\nPart 1\n\n\n\n\n\n\nPart 2\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Obsidian",
      "Freeing Your Thinking",
      "Obsidian: Freeing Your Thinking Workflow"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html",
    "href": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html",
    "title": "Obsidian: Meaningless Names, No Directories, Now What?",
    "section": "",
    "text": "Let me state my goal plainly: I want to be able to find all of my relevant information on a given topic and synthesize my understanding of that topic in a document or drawing, maybe for personal use, maybe for sharing or publishing.\nFor this purpose, a primary tool I use is Graph View. Yes, Graph View. And, in the process, I rehabilitate file names as a Valuable Thing.\nTo explain why Graph View is the best tool for this type of work, I need to re-visit the idea of Obsidian as a database.\n\n\n\nThere are two different types of databases. Most common are relational databases, or SQL databases. These are table-based (think spreadsheets) and depend on pre-defining relationships and hierarchies between tables. But Obsidian is clearly not an SQL database, since it has no underlying table structure. It is, in fact, a NoSQL database, aka non-relational database.\nThe Substack Helper thought my detailed explanation too long for this article, so I moved most of the discussion to Part 3. I encourage you to read that before continuing, but it is not actually necessary\nTLDR: Since Obsidian is a non-relational database, I use it as such.\nDatabases are queried for information. We have search filters that we can use for that, and which can be applied to both graphs and searches. We don’t get information from a database by opening a table or navigating to a record in the database. That is effectively the approach you take if you open a note to get its information.\n\n\n\nIn the first part, I talked about a workflow which created files with unique, meaningless names. If you have tried the proposed workflow, however, you will already have a number of meaningfully named notes in your vault. Every note has a topic:: field, which is a link or links, and those linked files have the name of the topic. The file may or may not actually exist. For Graph View, it doesn’t matter. You may have added links to the related:: field as well, thereby creating more meaningfully name notes. In the course of processing your atomic notes, you have given them meaningful names. Now that we are getting visual, meaningful names are useful. All your MOCs, existing or not, are already meaningfully named.\n\n\n\nThe most frequent type of comment I see about Graph View is along the lines of “it’s pretty to look at, but when you have a lot of notes it just becomes a useless, chaotic mess”. That’s like complaining that a dataview query listing every note in your vault is just too overwhelming to deal with. Well, yeah! Other than when you are enjoying the eye candy, you would never look at an unfiltered graph any more than use an unfiltered query.\nAs described in part one, filters can be just as sophisticated as dataview queries, and graph filters works exactly like filters do in Search. So, if I want to explore the information I have relating to a topic, I start by typing the topic in the graph filter. Now I can see all the files containing the words of the topic, any of which are potentially related. Some may already be linked, but if I show orphans, I can see more files which could or should be linked to the topic. With Hover Editor, it’s easy to look at each document’s contents and decided whether they should be related or not without opening the file. And if they should be, I can add the link directly in Hover Editor. I never need to actually open any files. And I can add or remove words to my filter to find information that is related but perhaps worded differently. Finally, I can bookmark the graph and easily return to it later to discover new, related information using the same filters.\nAssuming I have already created notes on a topic, I will already have a map of content named after the topic. If the file doesn’t yet exist, I can click on the node to create it and manually add the standard template. This Map of Content will already have a list of related files and a graph view which shows them, both accessible in the right sidebar. Now, as I look at potentially relevant notes in Graph View and add links to the topic:: field, my MOC is accumulating the links, and a neural network is developing.\nThe right sidebar version of the graph provides an additional feature, the ability to adjust “levels”. This allows you to not only see notes linked to the current note, but also notes that are linked to those notes. In a family tree this is kind of like cousins, cousins once-removed, and so forth. Some of these more distant relations should/could be directly linked to the main topic.\nAnd remember, thanks to Hover Editor, I never have to open a file except when I create one.\n\n\n\nHonestly, sometimes my brain just wants a list of things without extra visual stimulation. With Bookmarks, I can build knowledge trees for a topic, basically like a rich outline.\nI create a virtual (bookmark) directory for the topic. As I work with my information, I bookmark interesting blocks with information (I do not bookmark files, just information). These could be sections, paragraphs, images, embedded PDFs…anything really. Bookmarks can be named as well. For complex topics, I can create subdirectories as appropriate.\nThis results in what is essentially a detailed outline of all my information on a topic. A virtual Map of Content, if you will.",
    "crumbs": [
      "Obsidian",
      "Freeing Your Thinking",
      "Obsidian: Meaningless Names, No Directories, Now What?"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#analyzing-and-synthesizing-information",
    "href": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#analyzing-and-synthesizing-information",
    "title": "Obsidian: Meaningless Names, No Directories, Now What?",
    "section": "",
    "text": "Let me state my goal plainly: I want to be able to find all of my relevant information on a given topic and synthesize my understanding of that topic in a document or drawing, maybe for personal use, maybe for sharing or publishing.\nFor this purpose, a primary tool I use is Graph View. Yes, Graph View. And, in the process, I rehabilitate file names as a Valuable Thing.\nTo explain why Graph View is the best tool for this type of work, I need to re-visit the idea of Obsidian as a database.",
    "crumbs": [
      "Obsidian",
      "Freeing Your Thinking",
      "Obsidian: Meaningless Names, No Directories, Now What?"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#sql-and-nosql-databases",
    "href": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#sql-and-nosql-databases",
    "title": "Obsidian: Meaningless Names, No Directories, Now What?",
    "section": "",
    "text": "There are two different types of databases. Most common are relational databases, or SQL databases. These are table-based (think spreadsheets) and depend on pre-defining relationships and hierarchies between tables. But Obsidian is clearly not an SQL database, since it has no underlying table structure. It is, in fact, a NoSQL database, aka non-relational database.\nThe Substack Helper thought my detailed explanation too long for this article, so I moved most of the discussion to Part 3. I encourage you to read that before continuing, but it is not actually necessary\nTLDR: Since Obsidian is a non-relational database, I use it as such.\nDatabases are queried for information. We have search filters that we can use for that, and which can be applied to both graphs and searches. We don’t get information from a database by opening a table or navigating to a record in the database. That is effectively the approach you take if you open a note to get its information.",
    "crumbs": [
      "Obsidian",
      "Freeing Your Thinking",
      "Obsidian: Meaningless Names, No Directories, Now What?"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#filenames-revisited",
    "href": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#filenames-revisited",
    "title": "Obsidian: Meaningless Names, No Directories, Now What?",
    "section": "",
    "text": "In the first part, I talked about a workflow which created files with unique, meaningless names. If you have tried the proposed workflow, however, you will already have a number of meaningfully named notes in your vault. Every note has a topic:: field, which is a link or links, and those linked files have the name of the topic. The file may or may not actually exist. For Graph View, it doesn’t matter. You may have added links to the related:: field as well, thereby creating more meaningfully name notes. In the course of processing your atomic notes, you have given them meaningful names. Now that we are getting visual, meaningful names are useful. All your MOCs, existing or not, are already meaningfully named.",
    "crumbs": [
      "Obsidian",
      "Freeing Your Thinking",
      "Obsidian: Meaningless Names, No Directories, Now What?"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#graph-view",
    "href": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#graph-view",
    "title": "Obsidian: Meaningless Names, No Directories, Now What?",
    "section": "",
    "text": "The most frequent type of comment I see about Graph View is along the lines of “it’s pretty to look at, but when you have a lot of notes it just becomes a useless, chaotic mess”. That’s like complaining that a dataview query listing every note in your vault is just too overwhelming to deal with. Well, yeah! Other than when you are enjoying the eye candy, you would never look at an unfiltered graph any more than use an unfiltered query.\nAs described in part one, filters can be just as sophisticated as dataview queries, and graph filters works exactly like filters do in Search. So, if I want to explore the information I have relating to a topic, I start by typing the topic in the graph filter. Now I can see all the files containing the words of the topic, any of which are potentially related. Some may already be linked, but if I show orphans, I can see more files which could or should be linked to the topic. With Hover Editor, it’s easy to look at each document’s contents and decided whether they should be related or not without opening the file. And if they should be, I can add the link directly in Hover Editor. I never need to actually open any files. And I can add or remove words to my filter to find information that is related but perhaps worded differently. Finally, I can bookmark the graph and easily return to it later to discover new, related information using the same filters.\nAssuming I have already created notes on a topic, I will already have a map of content named after the topic. If the file doesn’t yet exist, I can click on the node to create it and manually add the standard template. This Map of Content will already have a list of related files and a graph view which shows them, both accessible in the right sidebar. Now, as I look at potentially relevant notes in Graph View and add links to the topic:: field, my MOC is accumulating the links, and a neural network is developing.\nThe right sidebar version of the graph provides an additional feature, the ability to adjust “levels”. This allows you to not only see notes linked to the current note, but also notes that are linked to those notes. In a family tree this is kind of like cousins, cousins once-removed, and so forth. Some of these more distant relations should/could be directly linked to the main topic.\nAnd remember, thanks to Hover Editor, I never have to open a file except when I create one.",
    "crumbs": [
      "Obsidian",
      "Freeing Your Thinking",
      "Obsidian: Meaningless Names, No Directories, Now What?"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#knowledge-trees",
    "href": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#knowledge-trees",
    "title": "Obsidian: Meaningless Names, No Directories, Now What?",
    "section": "",
    "text": "Honestly, sometimes my brain just wants a list of things without extra visual stimulation. With Bookmarks, I can build knowledge trees for a topic, basically like a rich outline.\nI create a virtual (bookmark) directory for the topic. As I work with my information, I bookmark interesting blocks with information (I do not bookmark files, just information). These could be sections, paragraphs, images, embedded PDFs…anything really. Bookmarks can be named as well. For complex topics, I can create subdirectories as appropriate.\nThis results in what is essentially a detailed outline of all my information on a topic. A virtual Map of Content, if you will.",
    "crumbs": [
      "Obsidian",
      "Freeing Your Thinking",
      "Obsidian: Meaningless Names, No Directories, Now What?"
    ]
  },
  {
    "objectID": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#the-elephant-in-the-room",
    "href": "posts/obsidian/freeing-your-thinking/freeing-your-thinking-part-2/index.html#the-elephant-in-the-room",
    "title": "Obsidian: Meaningless Names, No Directories, Now What?",
    "section": "The Elephant in the Room",
    "text": "The Elephant in the Room\nMore advanced users often use the Excalibrain plugin to visualize their vault. It is useful if you are thinking in terms of files and ontological relationships between them. I love the Excalidraw plugin and use it a lot for visual mocs. I had really hoped that I would like Excalibrain, too. But, for my process, it is inferior in a number of ways to Graph View.\nGraph View allows me to easily discover and interact with notes that are not yet linked to my topic. I can see orphans and adjust my filter, asking my questions in different ways, adding and removing search terms to reveal more potential relevant information.\nVisually, Excalibrain is blocky and static. Graph view is multi-form and dynamic. From a visual perspective, this is obviously personal preference. But the dynamism of Graph View is very useful and for me makes it superior.\nAs new notes are linked, all the nodes adjust to fit them in place, kind of like in your brain, in which new information shifts your thought patterns. Dragging nodes and changing force levels can emphasize relationships, and I can achieve elegant visual representations of my information this way. Notes can be visually grouped using colors, which is useful for complex topics with multiple sub-topics. The relative size of nodes indicates the information density of a note, suggesting other topics or sub-topics which could be developed. I find all of this useful.\nIn short, Graph View is fluid and flexible. Just the way I like my thinking.",
    "crumbs": [
      "Obsidian",
      "Freeing Your Thinking",
      "Obsidian: Meaningless Names, No Directories, Now What?"
    ]
  },
  {
    "objectID": "posts/obsidian/regex-search-in-line-metadata/index.html",
    "href": "posts/obsidian/regex-search-in-line-metadata/index.html",
    "title": "Regular Expressions for In-Line Fields",
    "section": "",
    "text": "If you are like me and use in-line (double colon) fields more than YAML, the improvements to metadata management (Properties) wasn’t much of a help. The new filter syntax, [\"key\":value], and the glob version, [\"key\":], don’t search in-line fields.\nOf course, this isn’t unexpected. Obsidian, for what I’m sure are good reasons, resists embracing in-line metadata, even though it is very widely used. Oh well, hope springs eternal… Nevertheless, though I’ve tried many times, I can’t give up on the convenience of in-lines.\nBecause in-line fields use two colons, which do not appear together in ordinary writing, it is easy to filter for them. For a glob filter like [\"key\":] in the new Properties, you can simply use \"key:: \" to achieve the same effect, querying in-line metadata instead of YAML.\nFiltering for specific values is a little trickier, because we must take into account multi-value fields. Fortunately, Obsidian allows us to use regular expressions in filters, and a simple one will achieve our goal. To use regular expressions, use the slash, /, as a delimiter instead of \". (In the example above, we actually could have used /key:: / instead of \"key:: \".)\nIn regular expressions, a period, ., matches any character, and an asterisk, *, indicates that there can be 0 or more of a character. By putting the combination .* before the value that we are filtering, we are saying that there may or may not be other characters between the field name and the value being searched. In other words, there may be multiple values.\nSo, even if the value we are looking for is one among many, to search for a specific key/value, you can use /key:: .*value/, which does the same as [\"key\":value] does for YAML.\n\n\n\n Back to top",
    "crumbs": [
      "Obsidian",
      "Regular Expressions for In-Line Fields"
    ]
  },
  {
    "objectID": "posts/obsidian/dataview.html",
    "href": "posts/obsidian/dataview.html",
    "title": "Dataview",
    "section": "",
    "text": "DataviewJS: A Gentle Introduction Part 2\n\n\n\nJavascript\n\nObsidian\n\nDataviewJS\n\n\n\nSelecting Files, Working with Dates\n\n\n\n\n\nFeb 28, 2023\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nDataviewJS: A Gentle Introduction\n\n\n\nJavascript\n\nObsidian\n\nDataviewJS\n\n\n\nFor Obsidian users who are not programmers\n\n\n\n\n\nOct 11, 2023\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Tables with DataviewJS\n\n\n\nJavascript\n\nObsidian\n\nDataviewJS\n\n\n\nSelecting Files, Working with Dates\n\n\n\n\n\nOct 14, 2023\n\n\nBrian Carey\n\n\n\n\n\n\n\n\n\n\n\n\nSummarizing Information with DataviewJS\n\n\n\nJavascript\n\nObsidian\n\nPKM\n\nDataviewJS\n\n\n\nPresenting and analyzing information in Obsidian\n\n\n\n\n\nOct 16, 2023\n\n\nBrian Carey\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "Dataview"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/shiso-fine/index.html",
    "href": "posts/chinese-medicine/shiso-fine/index.html",
    "title": "Shiso Fine",
    "section": "",
    "text": "Shiso with salmon1\nIf you’ve ever been to a good Japanese or Korean shusi restaurant in the United States, you know that the raw fish is inevitably served with sliced ginger and whole green, or sometimes purple, leaves.2 These are shiso leaves. They have a strong and complex flavor, and together with ginger allow you to “cleanse your palette” between slices of fish. But, did you ever imagine that the ginger and the shiso provide more than just flavor, and are actually medicinal substances in the Chinese pharmacopia, with medical properties specifically related to protecting your body from fish and shellfish related food poisoning? In otherwords, their presence on the sushi plate is about more than just flavor.\nPerilla, the English name for shiso, has several fascinating and even unique functions in Chinese Medicine (CM). This article is part of an occasional series exploring plants we encounter in our daily lives which are also important medicinal substances in CM, and which can illustrate some of the more interesting aspects of this medical system.",
    "crumbs": [
      "Chinese Medicine",
      "Shiso Fine"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/shiso-fine/index.html#seafood-toxicity",
    "href": "posts/chinese-medicine/shiso-fine/index.html#seafood-toxicity",
    "title": "Shiso Fine",
    "section": "Seafood Toxicity",
    "text": "Seafood Toxicity\nPerilla leaves have a unique function. It is the only herb in the Materia Medica that has the specific function to relieve seafood toxicity. There are quite a few herbs which address food poisoning, or toxins in general, but none other is specific to seafood. On the other hand, the most important herb to relieve toxicity in general is fresh ginger (shēng jiāng 生薑), which is commonly used to process toxic medicinal substances prior to administration. Clinically, the two are almost always used together to treat patients with fish or shellfish poisoning.\nUsing them together follows the herbologic principle that one plus one sometimes equals three. Certain herbal combinations are recognized as not only complementing each others functions, but potentiating them, making their effects stronger than they would be if used alone. Culinarily these two are also frequently used together, and good cooks recognize that this strange mathematics applies in their domain as well.",
    "crumbs": [
      "Chinese Medicine",
      "Shiso Fine"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/shiso-fine/index.html#perilla",
    "href": "posts/chinese-medicine/shiso-fine/index.html#perilla",
    "title": "Shiso Fine",
    "section": "Perilla",
    "text": "Perilla\n\n\n\n\n\n\nzǐ sū 紫蘇 Perilla frutescens\n\n\n\n\n\n\n\nPerilla plant3\n\n\n\nPerilla is zǐ sū (紫蘇) in Chinese. Zĭ (紫) means purple, the original color of the plant, green varieties being more recently developed for culinary purposes. Sū (蘇) means “revive”. This has a specific medical meaning beyond the generic one, and often refers to restoring consciousness to a patient. It also refers to “waking up” the digestive system, as we’ll see.\nThe plant is generally warming and pungent, and affects the respiratory and digestive systems. In CM, this implicates the Lung and Spleen Systems4, with their paired organs, the Large Intestines and Stomach. These are the organs which are responsible for providing the body with qì (氣), blood (xuè 血), and the other physical resources required for daily living. Food is processed in the Spleen/Stomach with oxygen provided by the Lungs, which also spreads the qì (氣) throughout the body, waste materials being eliminated by the Large Intestine.\n\n\n\n\n\n\nzǐ sū yè 紫蘇葉 Perillae Follium\n\n\n\n\n\n\n\nPurple perilla leaves5\n\n\n\nThe earliest references to zĭ sū yè (紫蘇葉) date back to the 5th century C.E. The leaves have a strong and complex, minty, peppery flavor, mixed with hints of cinnamon. It is a member of the mint family. In addition to being pungent, the leaves are aromatic, with many volatile, essential oils. In the Materia Medica, they are grouped with the many herbs used to treat colds and flu in patients with chills and no sweating. Internally, it focuses on the abdomen and digestion. It is the other functions and indications which make the herb so interesting, though. In addition to relieving seafood toxicity, which we already discussesd, the leaves can calm the fetus. I look at this fascinating concept below.\n\n\n\n\n\n\nsū gĕng 蘇梗 Perillae Caulis\n\n\n\nPerilla stems are less commonly used clinically. They do share some of the properties and functions of the leaves, albeit with weaker effect. Unlike the leaves, they help to relieve lung congestion.\n\n\n\n\n\n\nsū zĭ 蘇子 Perillae Fructus\n\n\n\n\n\n\n\nPurple seeds6\n\n\n\nThe seeds are focused on the lungs and large intestines. The close connection between these two organs is rather mysterious at first glance, but fundamental to physiology in CM, as I will explain later. The seeds, as is true with many seeds, are considered to have a downward movement. (Directionality is another of the many ways herbs are classified.) This downward movement means that it can drain the congestion from the lungs and/or alleviate constipation.",
    "crumbs": [
      "Chinese Medicine",
      "Shiso Fine"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/shiso-fine/index.html#calming-fetuses",
    "href": "posts/chinese-medicine/shiso-fine/index.html#calming-fetuses",
    "title": "Shiso Fine",
    "section": "Calming fetuses",
    "text": "Calming fetuses\nAmong the many functions attributed to herbs in Chinese Medicine, one that seems somewhat peculiar is that of herbs that calm the fetus. Herbs with this function have two main uses: treating pregnancy-related concerns in the mother, and protecting the developing fetus from the negative impacts of herbs intended to treat the mother.\nIn the latter case, it is common practice among gynecologists, or any doctors treating pregnant women, to include a small amount of such an herb to protect the fetus from any side effects of formulas administered for the benefit of the mother. The idea of calming the fetus in this context is, in a sense, letting the fetus know that the other herbs are not intended for itself, and to just ignore them.\nThe former case, that of treating pregnancy-related conditions, is somewhat more interesting. From the perspective of the mother’s body, the fetus is a source of stagnation in the body, specifically stagnation of phlegm, qì (氣), and blood (xuè 血). In an uncomplicated pregnancy, especially if the woman has no pre-existing health conditions, a woman’s body is able to respond and “work through this”, keeping everything flowing as normally as possible. But pregnancy is hard on the body.\nMorning sickness is a common example of the stagnation of phlegm caused by the fetus manifesting in the stomach, with symptoms of nausea, vomiting, and poor/unusual appetite. The qì (氣) of the stomach is unable to descend normally into the intestines. This results in a condition with the colorful name of rebellious qì, meaning nausea and vomiting in this context. It is this condition that zǐ sū yè (紫蘇葉) is particularly effective in treating, combined, of course, with other herbs, such as fresh ginger (shēng jiāng 生薑).",
    "crumbs": [
      "Chinese Medicine",
      "Shiso Fine"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/shiso-fine/index.html#wake-up-spleen",
    "href": "posts/chinese-medicine/shiso-fine/index.html#wake-up-spleen",
    "title": "Shiso Fine",
    "section": "“Wake up, Spleen!”",
    "text": "“Wake up, Spleen!”\n\nHarnessing the sense of smell\n\nPhlegm is one of the pathogens identified in Chinese medicine. It is probably the most difficult to understand for students and the most difficult to treat for the practitioners. Phlegm includes the stuff you cough out of your lungs, which is called visible phlegm, but also includes invisible phlegm, which can bring internal systems to a halt. Phlegm is closely related to another pathogen, dampness, but many doctors consider phlegm to be a component of all chronic disease. In extreme cases, when phlegm affects the Heart System it can lead to loss of consciousness. When if affects the Lung and Spleen, it can cause severe chest and abdominal congestion and pain, and disorders such as COPD7.\nThe Spleen and Stomach are the center of the body, sometimes referred to as the “pivot”, and are together responsible for the upward and downward motion of substances in the body. Downward movement ensures that food and drink move smoothly through the GI tract to the point of elimination, while upward movement sends the nourishment, qì (氣) and xué (學) to the Lungs and Heart respectively, whence they are distributed throughout the body. The Spleen is, in fact, the primary organ orchestrating all of this while also being responsible for processing phlegm in the body.\nPhlegm, especially severe cases, is treated with aromatic substances. This is a special flavor and is not one of the standard five.8 Aromatic herbs act through the sense of smell, the sense associated with the Lungs, rather than the sense of taste, which is associated with the Spleen. These herbs typically contain large amounts of volatile, essential oils. Most people are familiar with using smelling salts to revive consciousness. Smelling salts are ammonium compounds. In CM, we use musk and several plant resins for the same purposes. In treating phlegm in the Lungs and Spleen, many other herbs are available, such as zǐ sū yè (紫蘇葉).",
    "crumbs": [
      "Chinese Medicine",
      "Shiso Fine"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/shiso-fine/index.html#lungs-and-intestines",
    "href": "posts/chinese-medicine/shiso-fine/index.html#lungs-and-intestines",
    "title": "Shiso Fine",
    "section": "Lungs and Intestines",
    "text": "Lungs and Intestines\n\nThe end of the tale\n\nThe seeds of the perilla are used for two conditions, with a connection that illustrates a fascinating aspect of the traditional Chinese understanding of human physiology. Zǐ sū zĭ (紫蘇子) treats both lung congestion and constipation. This seems somewhat random or unconnected, but from a CM perspective, the conjunction of these two functions in a single substance is very natural.\nFirst, regarding the Lungs, the seeds are used to treat such conditions as asthma, COPD, or certain types of flu. Chinese herbology is all about combinations. Herbs are not used singly, but together with others, often in specific pairs or triplets of herbs. To treat lung conditions, perilla seed is combined with radish seeds (lái fú zĭ 萊菔子) and mustard seeds (bái jiè zǐ 白芥子). It is noteable that many culinary spices are actually recognized in CM as medical substances used to treat digestive disorders.\nZǐ sū zĭ (紫蘇子) is also used to treat constipation. Its high oil content makes it useful to treat constipation with dry stool, a condition common in the elderly. It is usually combined with other high-fat seeds for this purpose, in particular cannabis seeds (huǒ má rén 火麻仁), the principle medicinal substance in the Materia Medica for this condition.\nIn Chinese Medicine, there is a concept of paired organs. In Chinese philosphy, all things ineveitably have yin aspects and yang aspects, and the organ system is no exception. The five organ systems in CM are actually composed of pairs of organs. Some are obvious, like the Liver (yin) being paired with the Gall Bladder (yang), or the Kidney (yin) paired with the Urinary Bladder (yang). Others are strange, such as the pairing of the Lung, a yin organ, with the Large Intestines, a yang organ.\nNow that we understand that the Lungs play an important part in the digestive process, it makes some sense that the large intestine would be paired with the Lung, thereby incorporating elimiation in the overall process. But there is also a spiritual aspect to the Lung/Large Intestine relationship, one which recalls the calm the fetus function of the leaves. According to the Taoist philosophy underlying CM, when one is born, and takes the first breath, the mortal spirit (pò 魄) enters the body. When one dies, the spirit leaves the body through the large intestine. Sit with that one for a while ;-).",
    "crumbs": [
      "Chinese Medicine",
      "Shiso Fine"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/shiso-fine/index.html#conclusion",
    "href": "posts/chinese-medicine/shiso-fine/index.html#conclusion",
    "title": "Shiso Fine",
    "section": "Conclusion",
    "text": "Conclusion\nPerilla is a great example of medicine hiding in plain sight, as it were. It is also an example of the culinary history of a national cuisine being influenced by traditional medical practices.",
    "crumbs": [
      "Chinese Medicine",
      "Shiso Fine"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/shiso-fine/index.html#footnotes",
    "href": "posts/chinese-medicine/shiso-fine/index.html#footnotes",
    "title": "Shiso Fine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAppetizer Stock photos by Vecteezy↩︎\nCheap restaurants will often put a little piece of jagged green plastic, resembling a leaf, kind of, to stand in for the shiso leaf.↩︎\nOriginal photo from https://pxhere.com/en/photo/853091↩︎\nThe Spleen in Chinese medicine corresponds more closely to the Pancreas in terms of physiological function↩︎\nImage from Sushi University↩︎\nImage from Sushi University↩︎\nChronic Obstructive Pulmonary Disease↩︎\nSweet, sour, salty, bitter and pungent↩︎",
    "crumbs": [
      "Chinese Medicine",
      "Shiso Fine"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/eating-in-spring/index.html",
    "href": "posts/chinese-medicine/eating-in-spring/index.html",
    "title": "Eating in spring",
    "section": "",
    "text": "The Huáng dì nèi jīng (黃帝內經) is the oldest extant Chinese medical text, dated to around the 2nd century B.C.E. It lays out the foundational concepts on which all Chinese medicine is based. Before addressing disease, pathology and treatment, it discusses disease prevention: how to stay healthy. In fact, it posits that those who maintain an appropriate diet and activity level, together with rest and relaxation, with moderation in all things, should rarely fall ill, and quickly recover when they do. The elements of this advice are hardly surprising, but the nèi jīng does add, and emphasizes, an element, which may seem strange to Westerners: everything should be done in accordance with the season.\nIn the clinic, attention to the season is important in diagnosing and treating patients. A fever in summer, for example, is, in nearly all respects, different from an fever in winter, with regard to etiology, pathology, treatment and prognosis. In day-to-day living, it is equally important to pay attention to the season, especially with regard to diet, if one is to maintain optimal health.\nIt should be recognized that humans today have much less regular exposure to the progression of seasons than in the vast history of our evolution. Air conditioning and cars allow people to avoid being outside almost entirely, thus limiting their exposure to environmental conditions. Those who do not spend time outside, or otherwise expose themselves to seasonal changes, tend to be more subject to seasonal illnesses in my experience.\nSpring is upon us, and in this article, I will explain how to maintain an optimal diet for the spring season, foods to seek out and to avoid, and give some spring meal ideas.",
    "crumbs": [
      "Chinese Medicine",
      "Eating in spring"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/eating-in-spring/index.html#properties-of-foods",
    "href": "posts/chinese-medicine/eating-in-spring/index.html#properties-of-foods",
    "title": "Eating in spring",
    "section": "Properties of foods",
    "text": "Properties of foods\nFoods are classified in many ways, the most basic being flavor (sweet, sour, bitter, pungent, salty, bland)2 and nature (cold, cool, neutral, warm, hot). Other properties include direction of action and organ systems most affected. Like herbs, each food has specific functions, and these are taken into account in the clinic. In terms of maintaining health, we need not be so concerned about the details, however.\nThe nèi jīng gives some guidance on supporting the Liver system in the spring:\n\n“In the east, wind arises, there is movement that promotes green woody grouwth, and when unripe, gives off a sour taste. The sour taset stimulates the liver when ingested and nourishes the tendons and ligaments. When the wind is gentle it harmonizes all, but when it is extreme it can be destructive, just as in people, emotion turns into rage when the liver is out of control. Metal is the control element, so therefore grief counters anger while dryness lessens the wind and pungent neutralizes sour.” (Ni 1995, 243)\n\nOne of the first pleasures of spring is the arrival of the many herbs that flourish at that time. The rapid, upward growth, pungent flavor, and bright green color are perfect for the Liver System. These can be easily incorporated in salads and soups, or be made into pesto for a light pasta dish.\nI should note that Chinese medicine, and Chinese cuisine, strongly frown on raw food3. In my clinic, I have certainly seen patients whose health issues were tied to over-consumption of raw food. However, at least in patients without a compromised digestive system, eating raw salads as part of a balanced diet seems not to have a deleterious affect.\nSour foods in moderation are excellent, and so some fruit should be eaten regularly, as most are considered sour. Apricots and cherries are especially useful, but don’t worry too much and buy what is freshest. Herbed vinaigrettes are perfect.\nAs regards animal protein, shellfish is useful, such as crab, clams, oysters, and eel (unagi).\nOn the other hand, sweet foods should be minimized (so no lemonade). This includes not only obviously sweet foods, but also starchy foods and root vegetables. Sweet foods can impede the “free flow” of the Liver. This means eating relatively smaller portions of the staple grains such as rice and pasta, and more vegetables. In theory, you have built up and conserved energy stores through the winter…now it’s time to use that energy and get moving.",
    "crumbs": [
      "Chinese Medicine",
      "Eating in spring"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/eating-in-spring/index.html#goji-berries",
    "href": "posts/chinese-medicine/eating-in-spring/index.html#goji-berries",
    "title": "Eating in spring",
    "section": "Goji berries",
    "text": "Goji berries\nA specific food related to the Liver that I want to mention is goji berries since they have become common as a “superfood”. This is, in fact, one of the most important herbs used in the clinic to treat Liver System disorders. In Chinese, it is gǒu qǐ zǐ 枸杞子, and it is used to nourish the Liver, more specifically blood and yin. There are numerous folk stories about the regular consumption of gǒu qǐ zǐ leading to unusually long lives. It is tasty, widely available, and a handful can be easily added to morning cereal, cold or cooked, or to salads, or can be mixed with nuts and dried apricots4 for a snack mix.",
    "crumbs": [
      "Chinese Medicine",
      "Eating in spring"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/eating-in-spring/index.html#soup-and-salad",
    "href": "posts/chinese-medicine/eating-in-spring/index.html#soup-and-salad",
    "title": "Eating in spring",
    "section": "Soup and Salad",
    "text": "Soup and Salad\n\n\n\n\n\n\nBeetroot and fennel salad with peppermint vinegrette\n\n\n\n\n\nBoth beets and fennel are specific to the Liver. The fennel used is the vegetable, not the spice. It is a crisp and crunchy bulb with a mild licorice flavor.\nBoil a large beet, skin on, until it can be pierced with a fork and the skin slides off easily. After removing the skin, cut into bite-size cubes. Similarly, cut a fennel bulb, a bunch of spring onions, a red or yellow bell pepper, and a few celery stalks into pieces and add to the beets. Add a grated carrot, walnuts or peanuts, dried apricots and goji berries. If you like prunes, they are perfect for the Liver.\nMix vinegar and oil together, roughly half-half, although different vinegars are different, so you’ll need to taste and adjust. I like to use balsamic vinegar, but apple or wine vinegar is fine, too. Add fresh peppermint and a mix of whatever fresh herbs you have available, such as: parsley, cilantro, chives, tarragon, thyme, basil, oregano, etc. It is important that the herbs be fresh, not dried. Crush them with a mortar and pestle before adding, and don’t forget the salt and pepper.\nMix the dressing with the salad right away. Prepare it in advance, as it will taste better the longer it sits. It keeps well in the refrigerator for several days.\n\n\n\n\n\n\n\n\n\nLeek soup with white beans and chestnuts\n\n\n\n\n\nLeeks, along with their relatives from the onion/garlic family, must be cooked slowly to bring out the natural sweetness and eliminate the bitterness. They should be rinsed thoroughly after chopping because they can be dirty. The entire leek is edible, but avoid the ends if they are too tough.\nGently heat some oil or butter with fennel seeds and a couple bay leaves until you start to smell the fennel, then add the leeks and some salt, stir to thoroughly coat the leeks in oil, adding a small amount if necessary. Keep them on low heat, stirring occasionally, until they are soft, translucent, and sweet to the taste.\nCombine the leeks with chestnut or walnut pieces and a couple cups of cooked white beans (canned is fine). These are the large, flat beans, or butter beans. If you don’t have them, any bean will do, or even a couple of cubed potatoes. Add enough vegetable broth to cover, and let it cook for 20 minutes or so. Personally, I don’t have time to make vegetable broth, so I add a couple boullion cubes9 and boiling water. Do not add cold water or broth, or it will stop the cooking process.\nIt should be done in 20 minutes or so, and near the end add some fresh herbs like parsley, cilantro, thyme, rosemary…whatever is available and strikes your fancy. Add a splash of balsamic and/or apple vinegar, and additional salt if necessary. Garnish with some chopped chives. Avoid eating this with bread, especially if you used potatoes.",
    "crumbs": [
      "Chinese Medicine",
      "Eating in spring"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/eating-in-spring/index.html#mains",
    "href": "posts/chinese-medicine/eating-in-spring/index.html#mains",
    "title": "Eating in spring",
    "section": "Mains",
    "text": "Mains\n\n\n\n\n\n\nTarragon pesto pasta\n\n\n\n\n\nTake a large bunch of tarragon and remove any woody or tough stems. Place in a food processor or blender with a little salt, olive oil, and a couple handfuls of walnuts. If you eat cheese, add a small piece of parmesan or pecorino romano, if not you could add a tablespoon or so of nutritional yeast if you like. Both of these add considerable umami to the dish. Just remember to start with a little of everything but the tarragon. As you blend, add more olive oil until the pesto looks and tastes moist, being careful not to make it “oily”. You should adjust the seasoning as you go, and add more cheese or nuts if desired.\nThis is best served with smaller pasta with lots of nooks and crannies for the pesto to get stuck in. Sometimes I’ll add a small squirt of lemon which gives it punch (and a little sour benefits the Liver System).\n\n\n\n\n\n\n\n\n\nStir-fry with tofu, snow peas, peanuts\n\n\n\n\n\nThe technique here is more stir-steaming than stir-frying. Wok cooking is done at very high heat, with a small amount of liquid in the bottom which must be continually replenished so as to never dry out. The goal is to steam the vegetables, not boil them. Liquid should be added from the side of the wok, and there should always be visible steam. Vegetable broth is fine, although I like to use a mix of mirin or rice cooking wine, mushroom sauce, and fish sauce.\nFor best results, cut a firm block of tofu into bite-size pieces, place on a tray with pieces not touching, and freeze for at least four hours. For vegetables, I suggest snow peas, carrots, and red bell pepper. These should all be cut into bite size pieces.\nGrate an inch or so of fresh ginger and fry in a little oil at medium heat until it softens and smells good. Add the tofu, some liquid, and some soy sauce. The best is Thai mushroom sauce, but any light soy sauce is ok. Add some fresh mushrooms, ideally shiitake.\nWhen the tofu is softened, add a little oyster sauce, mix, and move the tofu to the side of the wok or remove it to a bowl. Make sure there is liquid in the bottom of the hot wok, and one by one cook the carrots, snow peas, and bell pepper for about one minute each, moving them to the side of the wok after they are slightly cooked.\nMix all the ingredients together, add additional soy sauce or oyster sauce as needed, and cook for another minute. Turn off the heat, and mix in some unsalted peanuts and chopped scallions. This is best served with Thai jasmine rice, but any will do. Remember to keep the rice portion small in the spring. I like spicy foods, so I add some Sriracha.\n\n\n\n\n\n\n\n\n\nLime-cilantro shrimp\n\n\n\n\n\nChop some onions and garlic and cook them on low-medium heat with salt in some olive oil until they start to brown. Raise the heat a little and add a chopped tomato. Let the water cook off the tomato, then add a chopped carrot, celery stalk and red bell pepper. Add water as necessary to keep everything moist but not soupy. Once the vegetables are softened but still firm, add the shrimp. As soon as they start to curl and change color, remove from the heat as they will continue to cook and will become tough if overcooked.\nStir in a generous amount of lime juice from fresh limes. Taste and juiciness of limes varies widely, but I find the small limes have the best flavor. Add a bunch of chopped cilantro and green onions. Serve with tortillas or rice.",
    "crumbs": [
      "Chinese Medicine",
      "Eating in spring"
    ]
  },
  {
    "objectID": "posts/chinese-medicine/eating-in-spring/index.html#footnotes",
    "href": "posts/chinese-medicine/eating-in-spring/index.html#footnotes",
    "title": "Eating in spring",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLiver, Kidney, Spleen, Lung, Heart↩︎\n“In general, foods that are pungent have dispersing qualities, those that are sour have astringent qualities, sweet foods have harmonizing and decelerating qualities, bitter foods have a dispensing and drying effect, and salty foods have a softening effect.” (Ni 1995, 94)↩︎\nChina was the first place I ever ate cooked lettuce↩︎\nApricots are specific to the Liver↩︎\nvinegar, citrus↩︎\nhoney, palm sugar, maple syrup, cane sugar↩︎\nIncludes green onions, chives, parsley, coriander, but includes most every herb which thrives in the spring and all the onion family.(Lu 1994)↩︎\nSuch foods include apricots, beets, mushrooms, cabbage, carrot, celery, eggs, grapes, honey, kidney beans, milk, olives, peanuts, oysters, string beans, sweet potatoes, kohlrabi, figs.(Lu 1994)↩︎\nThere are many high quality boullion cubes on the market these days, with many vegetarian and vegan choices. Experiment and find ones that you like.↩︎",
    "crumbs": [
      "Chinese Medicine",
      "Eating in spring"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Back to topReferences\n\nBensky, Dan, ed. 2004. Chinese Herbal Medicine: Materia Medica. 3. ed. Seattle, WA: Eastland Press.\n\n\nChen, John K., Tina T. Chen, and Laraine Crampton. 2004. Chinese Medical Herbology and Pharmacology. City of Industry, Calif: Art of Medicine Press.\n\n\nLu, Henry C. 1994. Chinese Natural Cures. Black Dog & Leventhal Publishers, Inc.\n\n\nNi, Maoshing. 1995. The Yellow Emperor’s Classic of Medicine: A New Translation of the Neijing Suwen with Commentary. Shambhala.\n\n\nRapp, Albert. 2022. “The Ultimate Guide to Starting a Quarto Blog – Albert Rapp.” July 2022. https://albert-rapp.de/posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#closing.\n\n\nShanny-Csik, Sam. 2022. “Creating Your Personal Website Using Quarto.” August 2022. https://ucsb-meds.github.io/creating-quarto-websites/.\n\n\nShanny-Csik, Samantha. 2022. “Adding a Blog to Your Existing Quarto Website.” October 24, 2022. https://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/."
  }
]